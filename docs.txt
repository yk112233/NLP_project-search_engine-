0			['AI新基建如何构建？浪潮给出了一个答案_AI科技大本营-CSDN博客']			作者 | Just出品 | AI科技大本营（ID:rgznai100）伴随生产力升级，社会基础设施也正在发生变化。而智慧时代的新型基础设施，要能够对外提供各种算力服务、数据服务和AI服务。浪潮认为，其核心是计算力的生产中心。因此，浪潮提出了“智算中心”的概念：智算中心是智慧时代最主要的计算力生产中心和供应中心，它以融合架构计算系统为平台，以数据为资源，能够以强大算力驱动AI模型来对数据进行深度加工，源源不断产生各种智慧计算服务，并通过网络以云服务形式向组织及个人进行供应。它正在成为经济社会运行的重要基础设施。在4月7日举办的IPF 2020浪潮云数据中心合作伙伴大会上，浪潮集团执行总裁、首席科学家王恩东称，智算中心要成为新基建，必须满足三个基本要求：开放标准，要求智算中心从硬件到软件、从芯片到架构、从建设模式到应用服务都应该是开放的、标准的；集约高效，要求智算中心的建设要有超大规模，要采用领先技术，保证自身的先进性；普适普惠，要求智算中心发挥基础设施的社会价值，服务大众。人工智能和开放计算是浪潮智算中心的两大发展方向，他们将在AI的基础上，全力发展领先的计算力。从基础设施角度看待AI算力，王恩东在会上介绍了服务闭环：第一，生产算力，浪潮将为智算中心建设供给算力机组。王恩东列举了数个重量级产品，浪潮拥有业内最强最全的AI计算产品阵列。浪潮AGX-5是目前全球最高性能的AI计算主机之一；AGX-2是目前单位空间内GPU计算密度最高的服务器；FP5295可支持CPU和GPU间数据同步传输。浪潮超大规模AI计算框架LMS，可实现单GPU超大算力支撑，支持70亿参数的NLP智能语言模型训练，相比主流Bert模型参数量提升20倍。第二，聚合算力。在训练方面，浪潮优化了TensorFLow框架，在全球首次实现在512个GPU卡上90%的扩展效率，打破Imagenet训练集训练时间全球最快纪录。在推理方面，针对高并发推理集群进行架构优化，构建了高性能的NVMe存储池，深度优化了软件栈，性能提升了3.5倍以上。第三，调度算力。浪潮AI Station计算资源平台支持AI训练和推理，可以提供AI模型开发和部署一站式交付，赋力更多创新可能。第四，释放算力。浪潮自动机器学习平台AutoML Suite ，可自动建模、自动模型压缩、自动超参调整，降低AI应用门槛，快速落地进化AI。同时它还支持本地化和云端部署，已应用在智慧城市、高铁等行业。总结来看，浪潮AI&HPC产品线总经理刘军在接受媒体采访时表示，AI算力要成为新基建的核心，实现高效的提升，一定要去打通算力作业环节，只有这样AI算力才真正可能行之有效，承担社会新基建的重任。此外，浪潮将引领开放计算体系，打造智算中心基石，构建从模式开放到技术开放，从产品开放到服务开放的开放计算体系。AI的发展也离不开开放生态。为了连接传统合作伙伴和新兴AI企业，推动AI产业化进程，浪潮在去年推出了“元脑”生态计划，以聚合具备AI开发核心能力的左手伙伴和具备行业整体方案交付能力的右手伙伴能力。据称，元脑生态聚合了AI最强算力平台、最优质的算法模型开发能力和最优质的集成、部署和服务能力，将支撑和加速各行业、各产业与人工智能的融合，让各个行业、产业具备可感知、自学习、可进化的能力，最终帮助用户完成业务智能转型升级，以生态之力成就行业、产业AI大脑。通过元脑生态，浪潮加快智算中心的建设和应用服务落地。王恩东举例，在金融行业，浪潮在国有六大行的服务器占有率超过50%，在银行业，浪潮支撑每日数亿笔金融交易，每日TB级账务更改，每日数千亿元金融业务IT服务。浪潮与声扬科技、赞华一起推出的 “智慧声纹识别”解决方案在某大型银行落地，该方案可实现2秒语音通话即可确认客户身份，1:1声纹确认准确率达99.7%。“元脑生态”推出一年，已经拓展了50个左手伙伴和120个右手伙伴。刘军对AI科技大本营坦陈，在“元脑”生态推出前对AI生态到底如何建设并没有特别全面的感知和认识，一开始加入比较多的实际是左手伙伴，但是慢慢发现右手伙伴从元脑生态中感知到了业务洞察能获取的价值。“浪潮本质上不是一家AI算法公司，我们是一家AI系统公司”，刘军表示，浪潮会更多的从AI计算的角度上为社区做更多的贡献，他希望，生态各方能开放、协作，各取所需，提供相应的价值，让整个生态会变得更加繁荣、健康。欢迎所有开发者扫描下方二维码填写《开发者与AI大调研》，只需2分钟，便可收获价值299元的「AI开发者万人大会」在线直播门票！推荐阅读前百度主任架构师创业，两年融资千万美元，他说AI新药研发将迎来黄金十年北京四环堵车引发的智能交通大构想400 多行代码！超详细中文聊天机器人开发指南 | 原力计划一站式杀手级AI开发平台来袭！告别切换零散建模工具你知道吗？其实&nbsp;Oracle&nbsp;直方图自动统计算法存在这些缺陷！（附验证步骤）你公司的虚拟机还闲着？基于 Jenkins 和 Kubernetes 的持续集成测试实践了解一下！从 Web 1.0到Web 3.0：详析这些年互联网的发展及未来方向你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/105548042
1			['IDEA中Maven依赖下载失败解决方案_ThinkWon的博客-CSDN博客']			使用IDEA进行Maven项目开发时，时不时会遇到pom.xml报错的情况，其中很大概率是因为Maven依赖的jar包下载失败，找来找去也没有找到是什么问题，困扰了很多程序猿，这里给出IDEA中Maven依赖下载失败解决方案，给大家参考，实测有用。文章目录首先检查网络有没有问题，确定网络没有问题，请看下一步多次点击重新导入Maven依赖的按钮设置自动导入Maven依赖在IDEA中找到Maven的配置文件的地址，然后检查配置的远程仓库或者镜像有没有问题如果上面几步都没有解决问题，可以使用以下脚本删除Mvaen中的lastUpdated文件Maven仓库依赖存在依旧报错首先检查网络有没有问题，确定网络没有问题，请看下一步多次点击重新导入Maven依赖的按钮重新导入Maven依赖有两种方式，如上图所示。如果多次点击重新导入依赖按钮依然报错，请看下一步设置自动导入Maven依赖Settings -> Build,Execution,Deployment -> Build Tools -> Maven -> Importing，如下图这样设置后，如果Maven仓库存在依赖的话，IDEA会自动导入到项目中，如果没有用，看下一步在IDEA中找到Maven的配置文件的地址，然后检查配置的远程仓库或者镜像有没有问题如上图所示，我的配置在C:\develop\Maven\apache-maven-3.5.3\conf\settings.xml我配置的是阿里云仓库，没有什么问题，如果配置的是Maven私服Nexus的话，需要检查配置的路径和私服网络有没有问题<!-- 配置阿里云仓库 --><mirror><id>nexus-aliyun</id><mirrorOf>*</mirrorOf><name>Nexus aliyun</name><url>http://maven.aliyun.com/nexus/content/groups/public</url></mirror>如果上面几步都没有解决问题，可以使用以下脚本删除Mvaen中的lastUpdated文件如果你的电脑是Windows系统，新建cleanLastUpdated.bat文件注意：记得将脚本cleanLastUpdated.bat文件的仓库路径改为自己Maven仓库的路径@echo off  rem 这里写你的仓库路径setREPOSITORY_PATH=C:\develop\Maven\apache-maven-3.5.3\respositoryrem 正在搜索...for/f"delims="%%iin('dir /b /s "%REPOSITORY_PATH%\*lastUpdated*"')do(del /s /q %%i)rem 搜索完毕pause保存，然后双击执行脚本就可以删除lastUpdated文件，然后点击重新导入Maven依赖的按钮Maven仓库依赖存在依旧报错我的依赖问题到这一步才得到解决，好累，不过问题终于解决了！有两种解决方式：把pom.xml中对应的依赖先删除，然后刷新右侧，之后再把依赖粘贴到pom.xml中，再次刷新右侧就好了从本地仓库将对应的包删除掉，然后让maven重新下载比如你要删除spring-boot-starter-web-2.1.8.RELEASE.jar，你要进入Maven仓库路径C:\develop\Maven\apache-maven-3.5.3\respository\org\springframework\boot\spring-boot-starter-web\2.1.8.RELEASE\spring-boot-starter-web-2.1.8.RELEASE.jar，然后进行删除			https://blog.csdn.net/ThinkWon/article/details/101312918
2			['5年Python功力，总结了10个开发技巧_AI科技大本营-CSDN博客']			作者 | 写代码的明哥来源 |Python编程时光（ID: Cool-Python）如何在运行状态查看源代码？查看函数的源代码，我们通常会使用 IDE 来完成。比如在 PyCharm 中，你可以 Ctrl + 鼠标点击 进入函数的源代码。那如果没有 IDE 呢？当我们想使用一个函数时，如何知道这个函数需要接收哪些参数呢？当我们在使用函数时出现问题的时候，如何通过阅读源代码来排查问题所在呢？这时候，我们可以使用 inspect 来代替 IDE 帮助你完成这些事。# demo.pyimport inspectdef add(x, y):    return x + yprint("===================")print(inspect.getsource(add))运行结果如下$ python demo.py===================def add(x, y):    return x + y如何关闭异常自动关联上下文？当你在处理异常时，由于处理不当或者其他问题，再次抛出另一个异常时，往外抛出的异常也会携带原始的异常信息。就像这样子。try:    print(1 / 0)except Exception as exc:    raise RuntimeError("Something bad happened")从输出可以看到两个异常信息Traceback (most recent call last):  File "demo.py", line 2, in <module>    print(1 / 0)ZeroDivisionError: division by zeroDuring handling of the above exception, another exception occurred:Traceback (most recent call last):  File "demo.py", line 4, in <module>    raise RuntimeError("Something bad happened")RuntimeError: Something bad happened如果在异常处理程序或 finally 块中引发异常，默认情况下，异常机制会隐式工作会将先前的异常附加为新异常的__context__属性。这就是 Python 默认开启的自动关联异常上下文。如果你想自己控制这个上下文，可以加个 from 关键字（from语法会有个限制，就是第二个表达式必须是另一个异常类或实例。），来表明你的新异常是直接由哪个异常引起的。try:    print(1 / 0)except Exception as exc:    raise RuntimeError("Something bad happened") from exc输出如下Traceback (most recent call last):  File "demo.py", line 2, in <module>    print(1 / 0)ZeroDivisionError: division by zeroThe above exception was the direct cause of the following exception:Traceback (most recent call last):  File "demo.py", line 4, in <module>    raise RuntimeError("Something bad happened") from excRuntimeError: Something bad happened当然，你也可以通过with_traceback()方法为异常设置上下文__context__属性，这也能在traceback更好的显示异常信息。try:    print(1 / 0)except Exception as exc:    raise RuntimeError("bad thing").with_traceback(exc)最后，如果我想彻底关闭这个自动关联异常上下文的机制？有什么办法呢？可以使用raise...from None，从下面的例子上看，已经没有了原始异常$ cat demo.pytry:    print(1 / 0)except Exception as exc:    raise RuntimeError("Something bad happened") from None$$ python demo.pyTraceback (most recent call last):  File "demo.py", line 4, in <module>    raise RuntimeError("Something bad happened") from NoneRuntimeError: Something bad happened(PythonCodingTime)最快查看包搜索路径的方式当你使用 import 导入一个包或模块时，Python 会去一些目录下查找，而这些目录是有优先级顺序的，正常人会使用 sys.path 查看。>>> import sys>>> from pprint import pprint   >>> pprint(sys.path)['', '/usr/local/Python3.7/lib/python37.zip', '/usr/local/Python3.7/lib/python3.7', '/usr/local/Python3.7/lib/python3.7/lib-dynload', '/home/wangbm/.local/lib/python3.7/site-packages', '/usr/local/Python3.7/lib/python3.7/site-packages']>>>那有没有更快的方式呢？我这有一种连 console 模式都不用进入的方法呢？你可能会想到这种，但这本质上与上面并无区别[wangbm@localhost ~]$ python -c "print('\n'.join(__import__('sys').path))"/usr/lib/python2.7/site-packages/pip-18.1-py2.7.egg/usr/lib/python2.7/site-packages/redis-3.0.1-py2.7.egg/usr/lib64/python27.zip/usr/lib64/python2.7/usr/lib64/python2.7/plat-linux2/usr/lib64/python2.7/lib-tk/usr/lib64/python2.7/lib-old/usr/lib64/python2.7/lib-dynload/home/wangbm/.local/lib/python2.7/site-packages/usr/lib64/python2.7/site-packages/usr/lib64/python2.7/site-packages/gtk-2.0/usr/lib/python2.7/site-packages这里我要介绍的是比上面两种都方便的多的方法，一行命令即可解决[wangbm@localhost ~]$ python3 -m sitesys.path = [    '/home/wangbm',    '/usr/local/Python3.7/lib/python37.zip',    '/usr/local/Python3.7/lib/python3.7',    '/usr/local/Python3.7/lib/python3.7/lib-dynload',    '/home/wangbm/.local/lib/python3.7/site-packages',    '/usr/local/Python3.7/lib/python3.7/site-packages',]USER_BASE: '/home/wangbm/.local' (exists)USER_SITE: '/home/wangbm/.local/lib/python3.7/site-packages' (exists)ENABLE_USER_SITE: True从输出你可以发现，这个列的路径会比 sys.path 更全，它包含了用户环境的目录。将嵌套 for 循环写成单行我们经常会如下这种嵌套的 for 循环代码list1 = range(1,3)list2 = range(4,6)list3 = range(7,9)for item1 in list1:    for item2 in list2:       for item3 in list3:           print(item1+item2+item3)这里仅仅是三个 for 循环，在实际编码中，有可能会有更层。这样的代码，可读性非常的差，很多人不想这么写，可又没有更好的写法。这里介绍一种我常用的写法，使用 itertools 这个库来实现更优雅易读的代码。from itertools import productlist1 = range(1,3)list2 = range(4,6)list3 = range(7,9)for item1,item2,item3 in product(list1, list2, list3):    print(item1+item2+item3)输出如下$ python demo.py1213131413141415如何使用 print 输出日志初学者喜欢使用 print 来调试代码，并记录程序运行过程。但是 print 只会将内容输出到终端上，不能持久化到日志文件中，并不利于问题的排查。如果你热衷于使用 print 来调试代码（虽然这并不是最佳做法），记录程序运行过程，那么下面介绍的这个 print 用法，可能会对你有用。Python 3 中的 print 作为一个函数，由于可以接收更多的参数，所以功能变为更加强大，指定一些参数可以将 print 的内容输出到日志文件中代码如下：>>> with open('test.log', mode='w') as f:...     print('hello, python', file=f, flush=True)>>> exit()$ cat test.loghello, python如何快速计算函数运行时间计算一个函数的运行时间，你可能会这样子做import timestart = time.time()# run the functionend = time.time()print(end-start)你看看你为了计算函数运行时间，写了几行代码了。有没有一种方法可以更方便的计算这个运行时间呢？有。有一个内置模块叫 timeit使用它，只用一行代码即可import timeimport timeitdef run_sleep(second):    print(second)    time.sleep(second)# 只用这一行print(timeit.timeit(lambda :run_sleep(2), number=5))运行结果如下2222210.020059824利用自带的缓存机制提高效率缓存是一种将定量数据加以保存，以备迎合后续获取需求的处理方式，旨在加快数据获取的速度。数据的生成过程可能需要经过计算，规整，远程获取等操作，如果是同一份数据需要多次使用，每次都重新生成会大大浪费时间。所以，如果将计算或者远程请求等操作获得的数据缓存下来，会加快后续的数据获取需求。为了实现这个需求，Python 3.2 + 中给我们提供了一个机制，可以很方便的实现，而不需要你去写这样的逻辑代码。这个机制实现于 functool 模块中的 lru_cache 装饰器。@functools.lru_cache(maxsize=None, typed=False)参数解读：maxsize：最多可以缓存多少个此函数的调用结果，如果为None，则无限制，设置为 2 的幂时，性能最佳typed：若为 True，则不同参数类型的调用将分别缓存。举个例子from functools import lru_cache@lru_cache(None)def add(x, y):    print("calculating: %s + %s" % (x, y))    return x + yprint(add(1, 2))print(add(1, 2))print(add(2, 3))输出如下，可以看到第二次调用并没有真正的执行函数体，而是直接返回缓存里的结果calculating: 1 + 233calculating: 2 + 35下面这个是经典的斐波那契数列，当你指定的 n 较大时，会存在大量的重复计算def fib(n):    if n < 2:        return n    return fib(n - 2) + fib(n - 1)第六点介绍的 timeit，现在可以用它来测试一下到底可以提高多少的效率。不使用 lru_cache 的情况下，运行时间 31 秒import timeitdef fib(n):    if n < 2:        return n    return fib(n - 2) + fib(n - 1)print(timeit.timeit(lambda :fib(40), number=1))# output: 31.2725698948由于使用了 lru_cache 后，运行速度实在太快了，所以我将 n 值由 30 调到 500，可即使是这样，运行时间也才 0.0004 秒。提高速度非常显著。import timeitfrom functools import lru_cache@lru_cache(None)def fib(n):    if n < 2:        return n    return fib(n - 2) + fib(n - 1)print(timeit.timeit(lambda :fib(500), number=1))# output: 0.0004921059880871326在程序退出前执行代码的技巧使用 atexit 这个内置模块，可以很方便的注册退出函数。不管你在哪个地方导致程序崩溃，都会执行那些你注册过的函数。示例如下如果clean()函数有参数，那么你可以不用装饰器，而是直接调用atexit.register(clean_1, 参数1, 参数2, 参数3='xxx')。可能你有其他方法可以处理这种需求，但肯定比上不使用 atexit 来得优雅，来得方便，并且它很容易扩展。但是使用 atexit 仍然有一些局限性，比如：如果程序是被你没有处理过的系统信号杀死的，那么注册的函数无法正常执行。如果发生了严重的 Python 内部错误，你注册的函数无法正常执行。如果你手动调用了os._exit()，你注册的函数无法正常执行。实现类似 defer 的延迟调用在 Golang 中有一种延迟调用的机制，关键字是 defer，例如下面的示例import "fmt"func myfunc() {    fmt.Println("B")}func main() {    defer myfunc()    fmt.Println("A")}输出如下，myfunc 的调用会在函数返回前一步完成，即使你将 myfunc 的调用写在函数的第一行，这就是延迟调用。AB那么在 Python 中否有这种机制呢？当然也有，只不过并没有 Golang 这种简便。在 Python 可以使用上下文管理器达到这种效果import contextlibdef callback():    print('B')with contextlib.ExitStack() as stack:    stack.callback(callback)    print('A')输出如下AB如何流式读取数G超大文件使用 with...open... 可以从一个文件中读取数据，这是所有 Python 开发者都非常熟悉的操作。但是如果你使用不当，也会带来很大的麻烦。比如当你使用了 read 函数，其实 Python 会将文件的内容一次性的全部载入内存中，如果文件有 10 个G甚至更多，那么你的电脑就要消耗的内存非常巨大。# 一次性读取with open("big_file.txt", "r") as fp:    content = fp.read()对于这个问题，你也许会想到使用 readline 去做一个生成器来逐行返回。def read_from_file(filename):    with open(filename, "r") as fp:        yield fp.readline()可如果这个文件内容就一行呢，一行就 10个G，其实你还是会一次性读取全部内容。最优雅的解决方法是，在使用 read 方法时，指定每次只读取固定大小的内容，比如下面的代码中，每次只读取 8kb 返回。def read_from_file(filename, block_size = 1024 * 8):    with open(filename, "r") as fp:        while True:            chunk = fp.read(block_size)            if not chunk:                break            yield chunk上面的代码，功能上已经没有问题了，但是代码看起来代码还是有些臃肿。借助偏函数 和 iter 函数可以优化一下代码from functools import partialdef read_from_file(filename, block_size = 1024 * 8):    with open(filename, "r") as fp:        for chunk in iter(partial(fp.read, block_size), ""):            yield chunk明哥原创文都已传至 Github：https://github.com/iswbm/PythonCodingTime博客链接：http://python.iswbm.com/en/latest/c03/c03_07.html推荐阅读全网唯一秃头数据集：20万张人像，网罗各类秃头阿里云科学家入选计算机顶会HPCA名人堂，他是什么来头？干货 | 基于SRS直播平台的监控系统之实现思路与过程程序员在这些地方敲代码，普通笔记本根本扛不住又一国产数据库诞生！腾讯发布 TGDB，实时查询比 Neo4j 快 20-150 倍力挺比特币的世界第2交易员：仅次于索罗斯，连续25年无亏损你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106631665
3			['干货 | 基于SRS直播平台的监控系统之实现思路与过程_AI科技大本营-CSDN博客']			作者 | 中国农业银行艾明浩、矫宏鹤责编 | Carol封图 |  CSDN 下载自视觉中国当前市面直播平台百家争鸣，直播监控系统是判断一个直播平台是否完善的必要条件。文章简要介绍了笔者搭建的一套基于SRS的直播平台，并从设计思路、实现方法与实现过程等方面重点介绍了针对此直播平台建设的监控系统，对相关直播平台监控系统的建设具有一定的参考价值。直播平台与其监控系统简介国内直播行业自2016年“井喷”式发展以来，在各行各业遍地开花，秀场、娱乐、游戏、教育、电商等行业应用尤为突出。笔者与时俱进，准备研究一下企业级视频直播技术应用，目前已经搭建了一套直播平台。在技术选型上，出于性能、文档完善性等方面考虑，我们选择了开源直播框架SRS作为直播服务器。SRS是由国人Winlin大神在Github上开源的一款流媒体服务器软件，定位于运营级互联网直播服务器集群，目前已经更新至V4版本，V3版是其稳定版本。我们在SRS已实现的基本RTMP等协议推拉流的基础上，开发了用户系统、直播房间管理、防盗链功能、弹幕系统、点赞系统、礼物系统等功能，并开发了Android、iOS、PC、Web直播客户端，实现了一个大致完善的视频直播平台。随着功能的增多，在开发调试及测试的过程中，我们发现，必须拥有一套针对直播平台的监控管理平台，以增加平台的健壮性、可用性与易用性。该监控管理平台用来对SRS集群进行统一配置管理，对服务器资源、网络流量、多媒体流、房间信息等进行统一监控，以方便后台人员对整个平台进行把控；同时，拥有一系列预警机制以应对大流量、网络中断、服务器断电等突发事件。监控数据的获取要进行平台的监控，需考虑需要监控哪些指标，能够从平台获得什么数据，采用什么方式从平台获取数据等问题。针对基于SRS开发的直播服务平台，我们采集获取监控数据的方式主要有3种：基于SRS提供的HTTP Api获取、从数据库及Redis获取、程序中埋点输出获取。1. 基于SRS提供的HTTP Api获取监控数据。SRS提供了一套HTTP Api，供外部程序获取数据及管理服务器，支持跨域访问。以下是SRS官方提出的HTTP Api设计原则：只提供json数据格式接口，要求请求和响应的数据全都是json。不提供html数据，譬如运行SRS后，浏览器打开HTTP接口或HTTP服务地址，看到的是json，不是html。发生错误时，支持HTTP错误码，或者json中的code错误码。通过查阅SRS官方文档及源码，SRS HTTP Api可获取的数据如下表：调用SRS提供的HTTP Api，需要在SRS服务器配置中增加http_api相关配置，下图是一个参考配置：首先，将enable设置为on，表示开启http api；listen设置为1985，表示其监听端口，假定SRS服务器地址为127.0.0.1，则可以通过http://127.0.0.1:1985/api/v1访问其http接口；raw_api相关配置表示开启服务器配置及重载等写入型接口。完成配置并重新加载服务后，即可访问SRS服务器的http api。以summaries api为例，在浏览器中输入http://[srs ip]:1985/api/v1/summaries，返回结果形式如下：可以看出，返回的结果是标准json形式，需要进行json解析并筛选出所需字段进行展示。2. 从服务端数据库及Redis中获取监控数据。除了SRS服务器资源相关数据之外，直播间信息、用户信息、直播统计信息、主播及观众等业务相关数据存储在数据库及缓存中。监控系统采用SpringBoot框架开发，导入相关驱动后，映射数据对象即可直接访问数据源获取所需数据。3. 程序中埋点输出监控所需数据。一些中间过程数据可以体现程序处理状态或者系统走势，这类数据既不能在SRS的HTTP接口中获取，也不会在数据库或缓存中存储，此类数据的获取需要在程序中埋点输出。所谓埋点就是在程序生成该数据处添加监控语句，将数据以指定方式输出，如输出到内存、文件、监控缓存或数据库。如果是带时间序列的数据，则会增加时间戳，展示时可展示成历史曲线，用以预测系统走势。监控指标及数据展示有了数据获取的途径，接下来需要考虑具体展示哪些监控指标及如何将指标展示的问题。1. 监控指标监控指标大致可分为三类：集群性能数据、多媒体流数据、业务统计数据。通过集群性能数据，用户可直观了解直播集群当前CPU使用率、内存大小、网络流量、负载等情况，实现系统层面的指标监控。通过多媒体流数据，用户可获取集群中每台机器的推拉流信息及当前直播的每路推流端的具体信息，实现媒体层面的指标监控。通过业务统计数据，用户可了解当前直播房间数、房间观众数等业务数据，实现业务层面的指标监控。2. 监控数据展示通过调用接口、查询数据库及埋点等方式获取的监控数据，经过服务端解析处理后，展示至前端监控平台。以多媒体流数据为例，用户通过管理端页面选择浏览当前推流数据信息，前端页面将请求传至后台，后台调用流信息查询接口向SRS请求流信息数据，SRS收到数据请求后将对应服务器上的流信息以json格式返回，监控平台对json格式报文进行解析，并以约定格式提供给前端以供展示，效果如下：也可通过定时模块将指定间隔采集的数据记录到数据库中（时间间隔可配置）。当前端发起历史数据查看时，监控平台依据请求从数据库中获取指定时间段内的监控数据，经过处理返回给前端，前端采用echart等插件完成从数据到图形的映射，生成折线图等形式的图表，大幅度提升可视化应用的性能。平台参数配置参数配置功能提供服务配置重新加载、服务器全局和Vhost配置等功能。配置重载简化了系统重新加载配置的流程，配合服务器参数配置功能，实现参数实时修改，实时生效。参数配置功能通过管理端将参数值由前端传至服务端，通过调用系统更新数据接口方式改变所选系统参数。该功能主要基于SRS提供的HTTP RAW API。首先开启raw_api相关设置，允许通过api进行配置修改（若不开启会返回错误码1061）。以配置重载为例，通过直播监控平台前端选择需要重载配置的机器ip，并将请求发至后台，后台收到请求后调用reload接口，SRS接收请求并进行配置修改，若修改成功，则返回成功（code为0）。直播监控平台将SRS的返回处理后发送至前端，前端对返回码进行判断，更新执行结果信息，效果如下：参数配置功能和数据展示功能相辅相成，用户可通过数据展示了解当前系统运行情况，并以此为据，通过参数配置功能调节系统运行参数，调整后再观察相应指标的数据展示，实现动态调整配置，确保直播系统平稳运行。总结文章介绍了一个基于SRS构建的直播平台的监控系统的搭建思路与实现方法，重点从监控数据的获取方法、监控指标需求、数据展示方法与过程、平台参数配置过程等方面介绍监控系统实现过程，为如火如荼的直播平台提供了监控领域的参考。艾明浩，互联网金融研发工程师，就职于中国农业银行研发中心，从事即时通讯、音视频等领域研发工作；矫宏鹤，互联网金融研发工程师，就职于中国农业银行研发中心，从事即时通讯、音视频等领域研发工作。推荐阅读全网唯一秃头数据集：20万张人像，网罗各类秃头Uber 前无人驾驶工程师告诉你，国内无人驾驶之路还要走多久？如何用NLP辅助投资分析？三大海外机构落地案例详解AI 终极问题：我们的大脑是一台超级计算机吗？借助大数据进行社交媒体营销，企业们得这么玩！力挺比特币的世界第2交易员：仅次于索罗斯，连续25年无亏损你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106595812
4			['深挖谷歌 DeepMind 和它背后的技术_AI科技大本营-CSDN博客']			作者 | James Murphy译者 | 天道酬勤 责编 | Carol出品 | AI科技大本营（ID:rgznai100）人工智能(AI)的子集已经成倍增长，并完成了只有人类才能完成的各种任务。像机器学习这样的技术可以执行管理任务、人脸识别、下棋，甚至翻译语言。毫无疑问，人工智能到来的十年带来了许多发展。此外，深度学习从非结构化数据中学习来编写分析报告或执行无人监督的任务。所有这些发展都为不同的公司发挥作用并证明他们的价值奠定了基础。因此，很多像DeepMind这样的公司成立了，来继续发展这一领域。你对其有哪些了解？今天就来看一下有关Google DeepMind的一切吧！Google DeepMind的历史DeepMind Technologies于2010年在伦敦成立，但是4年后，Google收购了这家公司。它的所有权在2015年也发生了变化，因为它后来被Alphabet，Inc收购，从那以后，它一直是该公司的子公司。DeepMind最初是由Demis Hassabis，Mustafa Suleyman和Shane Legg创立的，他们都是人工智能的爱好者，有些人将他们视为深度学习的先驱者。自成立以来，DeepMind Technologies已经在美国、加拿大和法国开设了研究中心。自从2016年AlphaGo打败世界围棋冠军Lee Sedol之后，开始得到许多人的认可。游戏被记录下来，在人们看到这些之后，他们开始信任这家公司。除此之外，他们还开发了另一个名为AlphaZero的程序，可以下国际象棋、日本象棋并取得了最佳成绩。由于斯科特•班尼斯特(Scott Banister)和埃隆•马斯克(Elon Musk)等人的加入，DeepMind得到了相当大的资金支持。这是他们从风险投资公司、维港投资(Horizons Ventures)和Founders Fund获得的额外资本。DeepMind的创始人向这些实体进行了可靠的介绍，这就是他们获得资金的原因。如果你也有一个创新而有前途的想法，明智的做法是查询最佳的音高板，创建一个高度专业且有效的演示文稿。通用学习算法DeepMind在通用学习算法方面非常有趣，它不仅可以改善这一领域，还将帮助人们更好地理解人类大脑。该公司已经开始通过开发能够玩各种不同游戏的系统来实现这一目标。其中一位创始人提到，他们相信，当一个程序可以玩各种不同的游戏时，就可以达到人类水平的人工智能。他们的策略得到科学研究的支持，这些科学研究证明，像象棋这样的游戏可以提高战略思维能力。通过学习如何玩这些复杂的游戏，机器将获得思考和采取战略行动的能力。DeepMind的通用学习算法让机器可以通过游戏化学习，尝试获得类人的智力和行为。尽管该公司对实现人类智能的机器学习非常感兴趣，但它对使用这些技术的安全性也有客观的看法。为了避免机器灾难，DeepMind开发了一个开源测试平台，以确定在存在不良行为时，算法是否具有终止开关。这个开源测试平台称为GridWorld，它可确保AI对自身、开发人员和其他接触到它的人都是安全无害的。DeepMind 的深度强化学习DeepMind通过实现一个完全不同的技术系统，将深度学习提升到了一个全新的水平。该系统称为深度强化学习，与常规的人工智能系统不同，它是完全独立的。例如，IBM Watson或Deep Blue是出于特定目的而开发的，并且被编程为仅以所需容量运行。DeepMind的深度强化学习不是预先编程的，而是像人类一样通过经验学习。从本质上讲，它基于卷积神经网络进行深度学习，并将其与Q-learning进行匹配。随后他们的系统在各种电子游戏上进行了测试，而不需要编写关于如何玩这个游戏的指令。每件事都由系统独立完成的，它学习如何玩电子游戏，经过多次尝试，它的玩法比任何人都要好。这个系统已经玩了各种游戏，并且比最擅长玩游戏的人掌握的还要好。深度强化学习消除了任何可能干扰游戏效率的人为错误。它不仅被用于游戏中，还被用于对医疗保健行业产生影响的各种不同的有用系统中。WaveNet 协作：为语言障碍者重新发声WaveNet协作是DeepMind促成的最卓越的医疗发展之一。有数百万的人患有语言障碍，无法恢复原来的声音。文本-语音转换系统通常会产生机械或听起来不自然的声音。DeepMind与Google以及患有肌萎缩性脊髓侧索硬化症(ALS)的蒂姆·肖(Tim Shaw)等语言障碍人士合作。目的是开发一种听起来像患者自然声音的系统，乍一看似乎是不可能完成任务。再现声音需要个人阅读特定脚本的数小时音频记录。不幸的是，有语言障碍的人可能没有这种奢侈，因为他们甚至不能轻易地组成一个句子。DeepMind研究了一种算法，该算法只需要少量的录音即可重现声音。6个月后，WaveNet合作已经在Tim的声音上发挥作用，并将其呈现给他和他的家人。结果使他们感到惊讶，因为这听起来像是在ALS开始影响Tim的语音能力之前的声音。你可以在YouTube上看到人们的反应，因为整个过程都被拍下来并上传了。对 Google 的其他贡献DeepMind已经参与了很多开发工作，其中很多是针对Google人工智能部门的。个性化的应用建议是绝大多数人每天使用的最受欢迎的应用之一。DeepMind的人工智能系统会收集你的偏好数据，然后推荐与你之前下载的应用程序类似的应用。他们进行的一个更复杂的项目是创建算法，用来冷却数据中心中Google服务器的温度。DeepMind系统已经提高了这些冷却系统的效率，而Google为该公司准备了更大的计划。很快，使用Android Pie设备的用户将拥有自适应亮度和电池等功能。机器学习将通过调整亮度来适应当前的照明条件，从而帮助这些设备实现节能。此外，它将使操作系统更容易使用，从而改善用户体验。由于该项目的规模较小，所以创建这些系统会有些复杂。这种机器学习系统通常需要更大的计算能力才能成功运行。重点DeepMind在人工智能领域取得了巨大的进步，推出了许多有用的创新系统。它为谷歌的人工智能部门所做的贡献是非常宝贵的，并且已经在全球范围内得到了应用。另一方面，DeepMind还与WaveNet等公司合作，为人们的生活增加价值。由于他们所使用的人工智能系统的特殊性，深度强化学习让他们成为Google的首选公司。你期待 DeepMind 还能再做些什么？或看完这篇文章后，想再挖掘人工智能的哪些可能性？可以在评论区和我们一起讨论哦~原文：https://hackernoon.com/all-we-need-to-know-about-googles-deepmind-0u6532r9本文为CSDN 翻译，转载请经授权推荐阅读前百度主任架构师创业，两年融资千万美元，他说AI新药研发将迎来黄金十年北京四环堵车引发的智能交通大构想400 多行代码！超详细中文聊天机器人开发指南 | 原力计划一站式杀手级AI开发平台来袭！告别切换零散建模工具你知道吗？其实&nbsp;Oracle&nbsp;直方图自动统计算法存在这些缺陷！（附验证步骤）你公司的虚拟机还闲着？基于 Jenkins 和 Kubernetes 的持续集成测试实践了解一下！从 Web 1.0到Web 3.0：详析这些年互联网的发展及未来方向你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/105570507
5			['传百度要与阿里、腾讯争夺在线办公市场？“百度Hi”开放520人同时在线音视频会议 \xa0_AI科技大本营-CSDN博客']			在线办公市场持续火热。4月20日，百度旗下在线办公平台“百度Hi”再升级，正式发布业内大规模的520人音视频会议，并支持多入口快速入会，加码在线办公。另有消息称，4月底，百度在线办公平台将发布重磅升级，加入知识管理功能。百度Hi于2008年推出，此前一直作为百度公司的内部办公协同软件，服务百度公司数万员工。疫情期间，企业远程办公需求骤增，百度于2月宣布开放百度Hi，并对湖北等疫区企业免费提供高清音视频会议、企业云盘、企业IM和应用中心平台等多项服务，以支持企业快速恢复生产能力，减少疫情对于企业和社会等经济影响。此次，百度Hi再次升级，将多人音视频会议这一刚需的同时在线人数提升至520人。这一功能解决了企业、学校、政府对于大型在线会议的需求，适用于企业培训、大型线上公开课等多种场景。同时，百度Hi推出多入口快速入会功能，为办公场景下的线上通讯带来“新玩法”。升级后的百度Hi支持电话专项功能入口、日程详情入口、会议邀请卡片入口、会议链接等多个入口，一键快速入会。其中，会议链接入口源自于百度Hi在疫情期间升级的免费web视频会议系统，会议组织者在百度Hi上创建会议链接后，其他参会人员无需安装百度Hi客户端，点击链接就可以加入视频会议。这一功能规避了同类产品进行多人视频会议时，需要提前安装APP、注册、加好友等繁琐步骤，并且没有时间地点、电脑手机设备切换的限制。除超大规模音视频会议和快速入会功能之外，百度Hi基于百度在语音、视觉、自然语言处理等AI领域的优势，支持网络电话、视频电话、企业通讯录、企业群聊等诸多通讯需求，也支持创建日程、企业云盘等协同办公功能，还可以同步支持移动签到、企业通知、企业后台等功能，便于企业管理员工的考勤和外勤等。据悉，4月底，百度在线办公平台还将有大动作，并将进行全新品牌升级及更名，向知识管理方向发力。当下，在线办公市场已经跨过市场教育阶段进入增量期，在普适的基础功能上进行差异化竞争，将成为行业突破的重要路径。百度Hi在音视频会议方面的持续提升，以及后续在知识管理方面的拓展，将为企业提供更加多元化、智能化的选择。			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/105643289
6			['来了来了！趋势预测算法大PK！_AI科技大本营-CSDN博客']			作者 | 王哲责编 | Carol头图 | CSDN 付费下载自视觉中国趋势预测在很多应用场景中都会起到至关重要的作用，比如淘宝商家会考虑库存量应该保持在多少才能够满足客户需求，商场希望得知假期会迎来多大的客流量以安排系列活动，机场想要预测五一黄金周会有多大的客运量来做相应的应急部署等。在智能运维领域，趋势预测同样具有一定的理论意义和实际应用价值。趋势预测在运维场景中的应用背景在实时监控系统中会采集到大量的数据，有些数据具有周期性等时间特征，也称之为时间序列。如果能挖掘出时间序列中所蕴含的信息，实现辅助人工决策，甚至是自动决策，都会为运维工作带来事半功倍的效果。比如KPI异常检测可以衡量服务的健康程度，分析出CPU、交易量、响应时间等指标的历史规律后，设置动态阈值，得到更加准确的异常报警，减少漏报误报情况的发生，提高应急响应效率；通过对历史事件单的分析，预测出下次系统告警可能发生的时间或者指定时间内可能发生告警的系统等，就可以根据分析结果做出相应的响应措施，为后续运维工作带来极大的指导意义。类似这样的应用场景还有很多，如何充分挖掘出时间序列中所蕴含的信息，成为智能运维领域研究中的一大热点。那么时间序列一般会有哪些比较明显的特征呢？一般来讲具有趋势性、季节性、周期性和随机性四种特征。但是对于时间序列预测，想要找到一个适用所有场景的通用模型几乎是不可能的，因为现实中每个预测问题的背景不同，影响预测值的因素与程度也往往不同，针对不同的问题就要采用不同的方法和模型进行统计分析，这都会给建模人员和数据分析师带来极大的难度，也使得时间序列预测问题变得复杂。不同的模型会有各自的优势和劣势，本文将对传统时间序列预测模型ARIMA、经典神经网络模型LSTM以及Prophet模型展开具体介绍，并在事件单数据集上做了初步的探索。常用的趋势预测算法2.1、ARIMA模型ARIMA模型，全称为自回归积分滑动平均模型（Autoregressive Integrated Moving Average Model），是由博克思(Box)和詹金斯(Jenkins)于20世纪70年代初提出的一种时间序列预测方法。ARIMA模型并不是一个特定的模型，而是一类模型的总称。通常用p，d，q值来确定，记做ARIMA（p，d，q）。其中p代表自回归模型阶数，d代表差分阶数，q代表移动平均阶数。ARIMA模型的建模步骤如下：首先对时间序列数据进行平稳性检测，若不通过，则采取对数、差分等相应的变换将其变为平稳序列。通过平稳性检测之后，进行白噪声检测，当序列不是白噪声序列时，即可选择合适的ARIMA模型进行拟合。如果误差值通过白噪声检测，就可以采用拟合出的模型对时序数据进行预测了。虽然ARIMA模型已经在很多个场景中得以应用，但是它存在的缺陷是不可忽视的：要求时序数据具有稳定性，或者通过差分化后是稳定的；对于数据中存在缺失值的情况，需要先进行缺失值填补，这很大程度上损害了数据的可靠性。2.2 神经网络模型2.2.1 网络模型初探在人工智能领域，DNN（Deep Neural Networks，深度神经网络）的应用极其广泛，在图像分类、目标检测等领域已经取得了优异的成绩。但是在DNN中，当前网络层只和上一层神经网络有连接，没有考虑样本出现的时间顺序，只能进行逐层进行训练。为了将时间因素包含在内，出现了RNN（循环神经网络模型），可以将神经元的输出在下一个时间戳直接作用到自身。具体来讲，就是在t时刻接收到输入之后，隐藏层的值是，输出值是。关键点在于，的值不仅仅取决于，还取决于，也就是说当前层的输出值不仅与当前时间点的输入有关，还要结合上一个时间点的输出值共同参与模型的训练。这样RNN就成为了一个在时间上传递的神经网络了。但RNN也存在一定的弊端，RNN只能够接受上一个节点的输出，随着网络层次的加深，可能会发生梯度消失或者梯度爆炸，通俗来讲，就是当前节点无法对距离自己较远节点的信息进行“记忆”。为了解决该问题，研究人员提出了很多解决办法，其中最为经典的一个网络模型是长短时间记忆模型（Long Short-Term Memory，LSTM）。LSTM与RNN主要的区别在于，它在模型中加入了一个判断信息是否有用的“处理器”，这个处理器被称为“记忆单元”（Memory Cell）。在一个Cell中放置了三扇门，分别是输入门、遗忘门和输出门。也就是说，当一个信息进入到LSTM中后，可以根据规则来判断其是否有用，只有符合要求的信息才会被留下，不符合的信息会被直接“遗忘”。这样序列数据在训练过程中的“记忆”问题就迎刃而解了。采用神经网络的方法虽然能够达到较好的效果，但是模型不够灵活，很难让使用者引入问题的背景知识，或者一些有用的假设；训练模型还需要大量的数据，数量不够多很可能会产生过拟合，影响训练效果；除此之外，LSTM是单步预测，只能预测出下一个时间点的值，对于未来任意时间段的预测很不友好。2.3 Prophet模型facebook发布了prophet（“先知”）项目，它以更简单、灵活的预测方式获得与经验丰富的分析师相媲美的预测结果。Prophet是Facebook发布的基于可分解（趋势+季节+节假日）模型的开源库。它让我们可以用更加简单、直观的参数进行高精度的时间序列预测，并且支持自定义季节和节假日因素的影响。prophet的整体框架分为四部分：Modeling、Forecast Evaluation、Surface Problems以及Visually Inspect Forecasts。从整体上看，这是一个循环结构，而这个结构又可以根据虚线分为分析师操纵部分与自动化部分。因此，整个过程就是分析师与自动化过程相结合的循环体系，也是一种将问题背景知识与统计分析融合起来的过程，这种结合大大的增加了模型的适用范围，提高了模型的准确性。首先Modeling：建立时间序列模型；然后进行Forecast Evaluation，也就时模型评估，对参数进行多种尝试，根据仿真效果评估出更加合适的模型；接着是Surface Problems：呈现问题，将误差较大的潜在原因呈现给分析师进行人工干预；最后一部分是Visually Inspect Forecasts：以可视化的方式反馈整个预测结果，将问题反馈给分析师后，由分析师考虑是否进一步调整和构建模型。针对不同的应用场景，Prophet也有相应的模型：增长趋势的模型：有几个月（最好是一年）的每小时、每天或每周观察的历史数据；季节趋势的模型（seasonality模型）：有较强的季节性趋势；有事先知道的以不定期的间隔发生的重要节假日（holiday模型），比如国庆节等。整体来讲，Prophet模型可以根据前一段时间的序列数据，结合专家的经验，预测出期望时间段的输出值，以较小的模型训练成本获得较好的训练结果。趋势预测算法小试牛刀3.1 数据采集本文采集了一些系统工单数据，通过LSTM和prophet模型分别对系统产生的工单数量的趋势进行预测。首先对对提取到的数据进行预处理，提取出时间（ds）和事件单数（y）两列数据，部分数据可视化结果如下图所示：import numpy as npimport pandas as pd#读入数据sales_df = pd.read_csv('false.csv')sales_df.head()3.2 数据实验1）LSTM模型构建与实验本文采用keras框架，构建有一个LSTM和一个全连接层的网络，采用MSE损失函数，用adam来优化损失函数，并将数据以7:3的比例划分为训练集和测试集。具体代码如下：# create and fit the LSTM networkmodel = Sequential()model.add(LSTM(4, input_shape=(1, look_back)))model.add(Dense(1))model.compile(loss='mean_squared_error', optimizer='adam')model.fit(trainX, trainY, epochs=240, batch_size=1, verbose=2)在数据集上的实验结果如下图所示，左图展现了LSTM模型在数据集上的拟合情况，横轴为时间，纵轴为事件单数，黄色线为真实值，绿色实现代表训练集上的预测结果，绿色虚线表示测试集上的结果。可以看到模型基本可以拟合出变化趋势，但是在具体的数量预测还有改进的空间。右图展现了在验证集和测试集上的loss趋势图。2）Prophet模型构建与实验from fbprophet import Prophet## 拟合模型m = Prophet()m.fit(sales_df)# # 构建待预测日期数据框，periods = 10 代表除历史数据的日期外再往后推 10 天future = m.make_future_dataframe(periods=10)# 预测数据集forecast = m.predict(future)forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()通过fit()方法拟合Prophet()模型，make_future_dataframe()可以对未来日期的数据趋势进行预测，periods = 10 代表预测10天的结果，并计算出预测值（yhat），预测最小值（yhat_lower），预测最大值（yhat_upper）。具体趋势预测图如曲线图所示，黑点代表真实值，浅蓝色部分代表预测区间，可以看到模型基本可以拟合出较好的时间预测效果。还能够通过plot_components(forecast)方法预测出数据的整体趋势和周效应：趋势--整体趋势：趋势--周效应：除了预测每天产生的工单数量，本文还对某些系统每天发生的工单数进行了分析和预测，希望能够在预测的基础上，我们可以根据工单的预测数辅助运维工作的安排和开展。如某系统产生的工单数如下图：小结趋势预测算法在众多场景中都有重要的应用价值。本文对比较主流的ARIMA模型、LSTM神经网络模型和facebook发布的Prophet模型进行介绍，并在系统工单数据集上进行了初步探索。虽然由于数据量的原因以及模型调参上还没有达到最佳的训练结果，但是模型对于趋势预测的有效性已经初步展现。之后，还会对趋势预测算法作进一步的探索和更深层次的研究，相信趋势预测算法在智能运维领域的应用也会更加广泛和可靠。作者简介：王哲，中国农业银行软件研发中心，从事运维支持工作的90后mm。目前承担新一代运维工具的研发，致力于研究人工智能算法，为实现智能运维、提升应用支持效率做出不懈努力。希望寻找业界同仁，共同对智能运维展开探讨：583283841@qq.com。推荐阅读实操来了！一文告诉你如何用 Streamlit 和 Heroku 开发 Web避坑！使用 Kubernetes 最易犯的 10 个错误雷军：4G 手机已清仓，全力转 5G；QQ音乐播放中途插语音广告引热议；Wine 5.9 发布 | 极客头条15 岁黑进系统，发挑衅邮件意外获 Offer，不惑之年捐出全部财产，Twitter CEO 太牛了！必读！53个Python经典面试题详解赠书 | 1月以来 Tether 增发47亿 USDT，美元都去哪儿了？你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106368395
7			['Python 炫技操作：合并字典的七种方法_AI科技大本营-CSDN博客']			来源 | Python编程时光（ID: Cool-Python）Python 语言里有许多（而且是越来越多）的高级特性，是 Python 发烧友们非常喜欢的。在这些人的眼里，能够写出那些一般开发者看不懂的高级特性，就是高手，就是大神。但你要知道，在团队合作里，炫技是大忌。为什么这么说呢？我说下自己的看法：越简洁的代码，越清晰的逻辑，就越不容易出错；在团队合作中，你的代码不只有你在维护，降低别人的阅读/理解代码逻辑的成本是一个良好的品德简单的代码，只会用到最基本的语法糖，复杂的高级特性，会有更多的依赖（如语言的版本）该篇是「炫技系列」的第二篇内容（第一篇：Python 炫技操作（01）：条件语句的七种写法），在这个系列里，我将总结盘点一下，我所见过的那些炫技操作。在这里，如果你是 Python 发烧友，你可以学到一些写出超酷的代码书写技巧。同时，看了这些内容，对你在阅读别人的代码时，也许会有些帮助。最简单的原地更新字典对象内置了一个 update 方法，用于把另一个字典更新到自己身上。>>> profile = {"name": "xiaoming", "age": 27}>>> ext_info = {"gender": "male"}>>>>>> profile.update(ext_info)>>> print(profile){'name': 'xiaoming', 'age': 27, 'gender': 'male'}如果想使用 update 这种最简单、最地道原生的方法，但又不想更新到自己身上，而是生成一个新的对象，那请使用深拷贝。>>> profile = {"name": "xiaoming", "age": 27}>>> ext_info = {"gender": "male"}>>>>>> from copy import deepcopy>>>>>> full_profile = deepcopy(profile)>>> full_profile.update(ext_info)>>>>>> print(full_profile){'name': 'xiaoming', 'age': 27, 'gender': 'male'}>>> print(profile){"name": "xiaoming", "age": 27}先解包再合并字典使用**可以解包字典，解包完后再使用 dict 或者{}就可以合并。>>> profile = {"name": "xiaoming", "age": 27}>>> ext_info = {"gender": "male"}>>>>>> full_profile01 = {**profile, **ext_info}>>> print(full_profile01){'name': 'xiaoming', 'age': 27, 'gender': 'male'}>>>>>> full_profile02 = dict(**profile, **ext_info)>>> print(full_profile02){'name': 'xiaoming', 'age': 27, 'gender': 'male'}若你不知道dict(**profile, **ext_info)做了啥，你可以将它等价于>>> dict((("name", "xiaoming"), ("age", 27), ("gender", "male"))){'name': 'xiaoming', 'age': 27, 'gender': 'male'}借助 itertools在 Python 里有一个非常强大的内置模块，它专门用于操作可迭代对象。正好我们字典也是可迭代对象，自然就可以想到，可以使用itertools.chain()函数先将多个字典（可迭代对象）串联起来，组成一个更大的可迭代对象，然后再使用 dict 转成字典。>>> import itertools>>>>>> profile = {"name": "xiaoming", "age": 27}>>> ext_info = {"gender": "male"}>>>>>>>>> dict(itertools.chain(profile.items(), ext_info.items())){'name': 'xiaoming', 'age': 27, 'gender': 'male'}借助 ChainMap如果可以引入一个辅助包，那我就再提一个，ChainMap也可以达到和itertools同样的效果。>>> from collections import ChainMap>>>>>> profile = {"name": "xiaoming", "age": 27}>>> ext_info = {"gender": "male"}>>>>>> dict(ChainMap(profile, ext_info)){'name': 'xiaoming', 'age': 27, 'gender': 'male'}使用 ChainMap 有一点需要注意，当字典间有重复的键时，只会取第一个值，排在后面的键值并不会更新掉前面的（使用 itertools 就不会有这个问题）。>>> from collections import ChainMap>>>>>> profile = {"name": "xiaoming", "age": 27}>>> ext_info={"age": 30}>>> dict(ChainMap(profile, ext_info)){'name': 'xiaoming', 'age': 27}使用dict.items() 合并在 Python 3.9 之前，其实就已经有|操作符了，只不过它通常用于对集合（set）取并集。利用这一点，也可以将它用于字典的合并，只不过得绕个弯子，有点不好理解。你得先利用items方法将 dict 转成 dict_items，再对这两个 dict_items 取并集，最后利用 dict 函数，转成字典。>>> profile = {"name": "xiaoming", "age": 27}>>> ext_info = {"gender": "male"}>>>>>> full_profile = dict(profile.items() | ext_info.items())>>> full_profile{'gender': 'male', 'age': 27, 'name': 'xiaoming'}当然了，你如果嫌这样太麻烦，也可以简单点，直接使用 list 函数再合并（示例为 Python 3.x ）>>> profile = {"name": "xiaoming", "age": 27}>>> ext_info = {"gender": "male"}>>>>>> dict(list(profile.items()) + list(ext_info.items())){'name': 'xiaoming', 'age': 27, 'gender': 'male'}若你在 Python 2.x 下，可以直接省去 list 函数。>>> profile = {"name": "xiaoming", "age": 27}>>> ext_info = {"gender": "male"}>>>>>> dict(profile.items() + ext_info.items()){'name': 'xiaoming', 'age': 27, 'gender': 'male'}最酷炫的字典解析式Python 里对于生成列表、集合、字典，有一套非常 Pythonnic 的写法。那就是列表解析式，集合解析式和字典解析式，通常是 Python 发烧友的最爱，那么今天的主题：字典合并，字典解析式还能否胜任呢？当然可以，具体示例代码如下：>>> profile = {"name": "xiaoming", "age": 27}>>> ext_info = {"gender": "male"}>>>>>> {k:v for d in [profile, ext_info] for k,v in d.items()}{'name': 'xiaoming', 'age': 27, 'gender': 'male'}Python 3.9 新特性在 2 月份发布的 Python 3.9.04a 版本中，新增了一个抓眼球的新操作符操作符：|， PEP584 将它称之为合并操作符（Union Operator），用它可以很直观地合并多个字典。>>> profile = {"name": "xiaoming", "age": 27}>>> ext_info = {"gender": "male"}>>>>>> profile | ext_info{'name': 'xiaoming', 'age': 27, 'gender': 'male'}>>>>>> ext_info | profile{'gender': 'male', 'name': 'xiaoming', 'age': 27}>>>>>>除了|操作符之外，还有另外一个操作符|=，类似于原地更新。>>> ext_info |= profile>>> ext_info{'gender': 'male', 'name': 'xiaoming', 'age': 27}>>>>>>>>> profile |= ext_info>>> profile{'name': 'xiaoming', 'age': 27, 'gender': 'male'}看到这里，有没有涨姿势了，学了这么久的 Python ，没想到合并字典还有这么多的方法。本篇文章的主旨，并不在于让你全部掌握这 7 种合并字典的方法，实际上，你只要选用一种最顺手的方式即可。但是在协同工作中，或者在阅读他人代码时，你不可避免地会碰到各式各样的写法，这时候你能下意识的知道这是在做合并字典的操作，那这篇文章就是有意义的。欢迎所有开发者扫描下方二维码填写《开发者与AI大调研》，只需2分钟，便可收获价值299元的「AI开发者万人大会」在线直播门票！推荐阅读GitHub标星2000+，如何用30天啃完TensorFlow2.0？8比特数值也能训练模型？商汤提训练加速新算法丨CVPR 2020400&nbsp;多行代码！超详细中文聊天机器人开发指南 | 原力计划微软为一人收购一公司？破解索尼程序、写黑客小说，看他彪悍的程序人生！机器学习项目模板：ML项目的6个基本步骤BM、微软、苹果、谷歌、三星……这些区块链中的科技巨头原来已经做了这么多事！你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/105479499
8			['fatal: early EOF fatal: index-pack failed_******* ▄︻┻┳═一  *******-CSDN博客']			使用git clone克隆代码时出现以下报错，这个一般是由于代码仓库太大，而本地网络不是很稳定都可以造成以下问题解决方法如下方法一：1、关闭compression$ git config --global core.compression 02、做一个部分克隆来截断下来的信息量$ git clone --depth 1 <repo_URI>3、转到新目录并检索克隆的其余部分$ git fetch --unshallow或者交替克隆$ git fetch --depth=21474836474、最后拉取所有$ git pull --all方法二：可以将这些行添加到全局git配置文件.gitconfig中，这个文件在用户家目录下[user]	name = xxxx	email = xxxx@admin.com	packedGitLimit = 512m 	packedGitWindowSize = 512m [http]	postBuffer = 1048576000	lowSpeedLimit = 0	lowSpeedTime = 999999[core]	compression = 0[pack] 	deltaCacheSize = 2047m 	packSizeLimit = 2047m 	windowMemory = 2047m其实只需要以下内容即可[user]	packedGitLimit = 512m 	packedGitWindowSize = 512m [pack] 	deltaCacheSize = 2047m 	packSizeLimit = 2047m 	windowMemory = 2047mgit常用命令整理一、基础命令git clone：这是一种较为简单的初始化方式git init 和 git remote：这种方式稍微复杂一些，当你本地创建了一个工作目录，你可以进入这个目录，使用'git init'命令进行初始化git remote add 命令来增加一个远程服务器端，例如：git  remote  add  origin  git://github.com/jQuery/jquery.git二、 Git 常用命令1) 远程仓库相关命令检出仓库：    $ git clone git://github.com/jQuery/jquery.git查看远程仓库：$ git remote -v添加远程仓库：$ git remote add [name] [url] 删除远程仓库：$ git remote rm [name]修改远程仓库：$ git remote set-url --push [name] [newUrl]拉取远程仓库：$ git pull [remoteName] [localBranchName]推送远程仓库：$ git push [remoteName] [localBranchName]*如果想把本地的某个分支test提交到远程仓库，并作为远程仓库的master分支，或者作为另外一个名叫test的分支，如下：$ git push origin test:master         // 提交本地test分支作为远程的master分支$ git push origin test:test           // 提交本地test分支作为远程的test分支2）分支(branch)操作相关命令查看本地分支：$ git branch查看远程分支：$ git branch -r创建本地分支：$ git branch [name]   #注意新分支创建后不会自动切换为当前分支切换分支：$ git checkout [name]创建新分支并立即切换到新分支：$ git checkout -b [name]删除分支：$ git branch -d [name]  # -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项合并分支：$ git merge [name]      #将名称为[name]的分支与当前分支合并创建远程分支(本地分支push到远程)：$ git push origin [name]删除远程分支：$ git push origin :heads/[name] 或 $ gitpush origin :[name] 创建空的分支：(执行命令之前记得先提交你当前分支的修改，否则会被强制删干净没得后悔)$ git symbolic-ref HEAD refs/heads/[name]$ rm .git/index$ git clean -fdx3）版本(tag)操作相关命令查看版本：$ git tag创建版本：$ git tag [name]删除版本：$ git tag -d [name]查看远程版本：$ git tag -r创建远程版本(本地版本push到远程)：$ git push origin [name]删除远程版本：$ git push origin :refs/tags/[name]合并远程仓库的tag到本地：$ git pull origin --tags上传本地tag到远程仓库：$ git push origin --tags创建带注释的tag：$ git tag -a [name] -m 'yourMessage'4) 子模块(submodule)相关操作命令添加子模块：$ git submodule add [url] [path]如：$git submodule add git://github.com/soberh/ui-libs.git src/main/webapp/ui-libs初始化子模块：$ git submodule init  # 只在首次检出仓库时运行一次就行更新子模块：$ git submodule update  # 每次更新或切换分支后都需要运行一下删除子模块：（分4步走哦）1) $ git rm --cached [path]2) 编辑".gitmodules"文件，将子模块的相关配置节点删除掉3) 编辑" .git/config"文件，将子模块的相关配置节点删除掉4) 手动删除子模块残留的目录5）忽略一些文件、文件夹不提交在仓库根目录下创建名称为".gitignore"的文件，写入不需要的文件夹名或文件，每个元素占一行即可，如targetbin*.db三、 git 命令详解①git pull：从其他的版本库（既可以是远程的也可以是本地的）将代码更新到本地，例如：‘git pull origin master’就是将origin这个版本库的代码更新到本地的master主枝②git add：是将当前更改或者新增的文件加入到Git的索引中，加入到Git的索引中就表示记入了版本历史中，这也是提交之前所需要执行的一步，例如’git add app/model/user.rb’就会增加app/model/user.rb文件到Git的索引中③git rm：从当前的工作空间中和索引中删除文件，例如’git rm app/model/user.rb’④git commit：提交当前工作空间的修改内容，例如’git commit -m story #3, add user model’，提交的时候必须用-m来输入一条提交信息，该功能类似于SVN的commit⑥git push：将本地commit的代码更新到远程版本库中，例如’git push origin’就会将本地的代码更新到名为orgin的远程版本库中⑦git log：查看历史日志⑧git revert：还原一个版本的修改，必须提供一个具体的Git版本号，例如’git revert bbaf6fb5060b4875b18ff9ff637ce118256d6f20’，Git的版本号都是生成的一个哈希值上面的命令几乎都是每个版本控制工具所公有的，下面就开始尝试一下Git独有的一些命令：⑨git branch：对分支的增、删、查等操作，例如’git branch new_branch’会从当前的工作版本创建一个叫做new_branch的新分支，'git branch -D new_branch’就会强制删除叫做new_branch的分支，'git branch’就会列出本地所有的分支⑩git checkout：Git的checkout有两个作用，其一是在不同的branch之间进行切换，例如’git checkout new_branch’就会切换到new_branch的分支上去；另一个功能是还原代码的作用，例如’git checkout app/model/user.rb’就会将user.rb文件从上一个已提交的版本中更新回来，未提交的内容全部会回滚⑪git rebase：实际上是将分支点从C移到了G，这样分支也就具有了从C到G的功能⑫git reset：将当前的工作目录完全回滚到指定的版本号，假设我们有A-G五次提交的版本，其中C的版本号是 bbaf6fb5060b4875b18ff9ff637ce118256d6f20，我们执行了’git reset bbaf6fb5060b4875b18ff9ff637ce118256d6f20’那么结果就只剩下了A-C三个提交的版本⑬git stash：将当前未提交的工作存入Git工作栈中，时机成熟的时候再应用回来⑭git config：利用这个命令可以新增、更改Git的各种设置，例如’git config branch.master.remote origin’就将master的远程版本库设置为别名叫做origin版本库⑮git tag：可以将某个具体的版本打上一个标签，这样你就不需要记忆复杂的版本号哈希值了，例如你可以使用’git tag revert_version bbaf6fb5060b4875b18ff9ff637ce118256d6f20’来标记这个被你还原的版本，那么以后你想查看该版本时，就可以使用 revert_version标签名，而不是哈希值了Git			https://blog.csdn.net/m0_37886429/article/details/101278059
9			['Z平台-开源免费的JAVA快速开发平台_★ 赵戬 ★ 一名JAVA程序员的博客-CSDN博客']			平台简述Z平台是开源免费的JAVA快速开发平台，通过Z平台集成开发环境，以零编码、动态配置的方式能够快速开发BS管理系统。同时该平台还可以做为APP、微信、各种小程序等项目的服务端来使用，为前端项目提供数据接口。并且Z平台也内置了代码生成器组件，可以通过生成代码方式来完成项目的开发工作。源码仓库https://gitee.com/zj1983/zz使用教程基础教程：https://edu.csdn.net/course/detail/29220演示系统https://www.zframeworks.com/开发技巧https://blog.csdn.net/qq_38056435/article/details/103386195平台价值提升软件开发速度，缩短软件开发周期。降低软件开发BUG率，缩短软件测试周期。降低项目所需高级开发人员比例，减少项目用工成本支出。平台特点永久开源免费Z平台为开源免费项目，可以应用在所有商业或非商业项目中进行使用。学习成本低Z平台所使用的框架都是热门的开源技术框架。学习资料丰富。核心框架为Spring + SpringMVC + Mybatis组成。技术成熟稳定Z平台所应用的基础框架都是经过长时间沉淀成熟稳定的开源框架。在稳定性方面值得信赖。适用用户Z平台功能非专业人士企业信息管理人员计算机专业学生程序员安装部署YESYESYESYES平台功能YESYESYESYES表单开发YESYESYESYES报表开发NOYESYESYES工作流程配置NOYESYESYES自动任务配置NONOYESYES接口开发NONOYESYES框架修改NONONOYES			https://blog.csdn.net/qq_38056435/article/details/70212001
10			['软件测试计划_九天揽月-CSDN博客']			引言：编号确定项目描述1确定范围确定被测项目中功能模块，子功能模块等需要测试的范围。2确定需求确定每个功能结果定义，确定此功能是否存在缺陷。3确定策略确定对项目做哪些测试。如：功能测试，性能测试等。4确定方法确定对每个策略是用哪些方法。如：边界值，等价类等。5确定工具如:功能测试使用Seleium，性能测试使用Jmeter等。6确定资源测试需要的设备，服务器、参与测试的人员、测试任务的分工，测试工作的进度。7交付文档确定测试工作中生成哪些文档，可提交文档有哪些。测试项目：项目名称XX项目使用背景//用户 协会分会负责人、期刊客户开发者XX集团项目简介学术专著出版平台” 定位是一家图书产品联合创建、销售、返利的平台；平台联合各专业协会、学会、出版社等机构，组织大批专家人才建立“专家指导委员会”，为图书进行策划、上报、审校、出版、运营等服务；主要业务情景是：策划人寻求参编人，共同创建图书及销售，参编人支付参编图书的预购款，该笔资金作为公司运营图书的成本，等待图书出版后，让消费者以个人名片或链接的形式进行购买图书，参编人员不仅可以通过图书评职称、扩大知名度、传播学术价值，另外让参编人通过销售，实现“0”元出书并且获得额外收入；策划人在发展参编和策划人同时，获得相应奖励。测试目的：编号目的1软件测试是为了发现错误而执行程序的过程。2测试是为了证明程序有错，而不是证明程序无错。3一个好的测试用例在于它发现至今未发现的错误。4一个成功的测试是发现了至今未发现的错误的测试。文档受众：编号人员原因1产品设计人员明确说明测试范围，方法，工作周期信息。2产品研发人员明确说明测试范围，方法，工作周期信息。3产品测试人员明确说明测试范围，方法，任务分工，预计完成时间。4备注此为内部开发文档，不做外部参考。测试参考文档编号文档名称作用1需求文档确定项目功能模块，功能运行结果。2技术文档确定项目中使用开发语言，数据库数据限制。3项目模型文档初步了解项目页面内容，方便编写用例。测试提交文档：编号文档名称作用1测试计划明确说明测试范围，方法，工作周期信息。2测试用例明确说明测试工作的细节测试工作。3缺陷报告明确说明项目中的缺陷描述，与修复情况。4测试报告明确说明测试结果，测试模块，缺陷分布情况等等信息。术语定义：项目术语：编号缩写、术语解释12测试专业术语：编号术语解释1单元测试开发者编写的一小段代码，检验被测代码的一个很小的、很明确的功能是否正确。2集成测试开发者编写的多个段代码单元，组合到一起形成集成测试，检查多个单元组合功能是否正确。3冒烟测试针对产品的基本功能进行测试。4功能测试又称正确性测试，它检查软件的功能是否符合规格说明。5可靠性测试对服务器施加一定压力，测试服务器是否可以长期稳定运行。6压力测试对服务器施加一定压力后进行功能测试，测试服务器在一定压力下是够可以正常计算。7负载测试对服务器施加压力，测试服务器可以容纳多少人访问，多少人访问后出现BUG。8易用性测试主要从使用的合理性和方便性等角度对软件系统进行检查。用户来测。9兼容性测试测试Web页面是否支持所有浏览器，访问后页面所有功能无异常。10安全测试服务器数据安全性，用户数据安全性，用户操作安全性，用户财产安全性、公司财产安全性。11数据完整性测试对数据及数据库能否正常运行访问的测试。12回归测试开发修改后的BUG在测试一遍。1314缺陷优先级：级别描述P0严重级别比较高的，影响测试进行或者系统无法继续操作，立即修复，1天。P1基本功能没有实现，对系统操作有影响，2－3天。P2一般性功能，页面缺陷，4－5天。P3准备在下一轮测试前修改完毕，准备在下一版本中修改。严重程度定义：级别严重程度描述S0数据丢失，数据计算错误、数据传递错误、对数据库造成破坏，造成操作系统或其他支撑系统崩溃、非正常关闭和非正常死机。S1应用系统崩溃、非正常关闭和无响应，但没有造成数据丢失。系统的主要功能不能正确实现或不完整。S2规定的非主要功能没有实现或不完整、影响系统的运行；设计不合理造成性能低下。S3不影响业务运行的功能问题。S4软件设计和功能实现等不完全合理之处提出建议。用例优先级定义：级别优先级描述P0确保系统基本功能及主要功能的测试用例P1确保系统功能的完善方面的测试用例P2关于用户体验，输入输出的验证；较少使用或辅助功能的测试用例。测试策略：1、单元测试单元测试测试目标开发者编写的一小段代码，检验被测代码的一个很小的、很明确的功能是否正确。测试范围测试整个项目中的每一行代码进行测试。完成标准代码的一个很小的、很明确的功能都正确。需要考虑的特殊事项//使用工具Python + Selenium + unittest2、集成测试：集成测试测试目标开发者编写的多个段代码单元，组合到一起形成集成测试，检查多个单元组合功能是否正确。测试范围开发者编写的多个段代码单元，组合到一起形成的集合。完成标准多个单元组合功能正确。需要考虑的特殊事项//使用工具3、冒烟测试：冒烟测试测试目标版本是否值得系统测试测试范围1、返测上一版本提交的测试报告。2、测试系统的基本功能。完成标准基本功能通过，并继续测试。需要考虑的特殊事项此阶段不超过1天。4、功能测试：功能测试测试目标确保测试计划中所列出的测试范围，保证其功能正常。测试范围1、按照测试计划所规定的测试范围。2、利用有效的和无效的数据来执行各个用例、用例流或功能3、以核实以下内容：1）在使用有效数据时得到预期的结果。2）在使用无效数据时显示相应的错误消息或警告消息。完成标准按照测试计划的测试通过标准，完成测试。需要考虑的特殊事项确定或说明那些将对功能测试的实施和执行造成影响的事项或因素。（内部的或外部的）使用工具Seleium + python +谷歌易用性测试：易用性测试测试目标模拟真实用户，无经验用户，测试系统的易用性测试范围前台完成标准成功地核实出前台各个网页符合可接受易用性标准。需要考虑的特殊事项无7、兼容测试：兼容测试测试目标测试Web页面是否支持所有浏览器，访问后页面所有功能无异常测试范围前台完成标准使用多个不同浏览器访问后界面无异常即为通过。需要考虑的特殊事项浏览器版本；浏览器类型是否都测到。8、可靠性测试：可靠性测试测试目标使用LR模拟真实用户对服务器施加一定压力测试范围项目服务器。完成标准持续运行特定时间不出现问题。需要考虑的特殊事项测试机是否满足需求。9、压力测试：压力测试测试目标使用LR模拟真实用户对服务器施加压力。测试范围项目服务器。完成标准直到服务器卡死。获得服务器资源，最大与链接数等数据。需要考虑的特殊事项测试机是否满足需求。使用工具Jmeter + fiddler +火狐10、负载测试:负载测试测试目标使用LR模拟真实用户对服务器施加一定压力，对服务器进行主要功能测试。测试范围项目服务器&前台界面完成标准对服务器施加一定压力后前台功能正常，访问时间3-8之内。需要考虑的特殊事项测试机是否满足需求。使用工具Jmeter + fiddler +火狐11、数据完整性测试：数据完整性测试测试目标确保数据库设计完整性。测试范围数据库及表结构完成标准数据库约束、完整性等设置达到需求标准。需要考虑的特殊事项数据遭到破坏，易恢复性。12、回归测试：回归测试测试目标确保BUG修复的完整性。测试范围项目中出BUG的部分完成标准项目中出现的BUG完成修复，并将缺陷保存下来。需要考虑的特殊事项出BUG的功能和BUG相关的功能都需要回测。13、功能测试范围：模块功能应用策略备注四、测试规则：1、准入规则编号测试策略进入准则1单元测试项目编码阶段，开发人员每编写完一个单元时进入测试2集成测试项目编码阶段，开发人员每编写完多个单元时进入测试3功能测试项目系统测试阶段，开发人员根据需求开发完成时，进入测试。4易用测试功能测试完成后进入测试。5兼容测试6可靠测试功能测试完成后进入测试。7压力测试8负载测试9数据完整性性能测试完成后进入测试。10回归测试提交的缺陷报告修改后。2、暂停/退出规则编号1软件系统在进行单元、集成、确认、系统、安装、验收测试时，发现缺陷达到一定数量或出现重大错误导致无法测试时，暂停测试返回开发。2发生其他未知因素需要暂停时，测试应随之暂停，并备份暂停点数据。测试资源：硬件资源编号CPU内存硬盘系统软件12.54+100+Win7Jmeter，seleium，AppScan测试人力资源：编号角色人员具体职责1确认需求明确需求2制定计划决定测试策略，人员分工，测试周期3准备测试环境测试工作开始前准备工作4执行测试工作编写用例，执行用例，提交缺陷报告，回测等。5编写测试报告编写项目的测试结果。测试工作进度：编号任务范围人员时间1确认需求2定制测试计划3准备测试环境4单元测试5集成测试6冒烟测试功能测试兼容测试易用性测试7可靠性测试压力测试负载测试8安全测试9数据完整性测试10回归测试11编写测试报告系统风险1、系统风险：（1）、计划的测试时间，不能满足测试组的要求，主要是功能冻结后的系统测试的时间可能不够。（2）、测试资源的及时到位（设备和人员）。（3）、需求不明确可能导致开发的产品与目标不一致。（4）、测试人员对测试工具的使用熟悉程序不够；（5）、被测试产品存在重大错误，以至于测试无法继续，需要开发组进行额外的调试和修改才能继续；（6）、硬件、软件或网络环境出现故障等2、应急措施：（1）、如果上述潜在的可能事件发生，则通过适当加班来保证计划的按时完成。（2）、如果是由于被测试产品存在重大错误而严重影响测试进度，则考虑按照测试暂停标准来暂停该测试。（3）、如遇到功能需求不明确，需要沟通协商解决。（4）、人员不足，则加班、或者进行不同组人员调动，按照测试进度完成测试任务。测试的完成标准完成标准单元测试完成标准:(1)、按照单元测试计划完成了所有规定单元的测试(2)、达到了测试计划中关于单元测试所规定的覆盖率的要求(3)、软件单元功能与设计一致(4)、在单元测试中发现的错误已经得到修改，各级缺陷修复率达到标准2、集成测试完成标准（1）、按照集成构件计划及增量集成策略完成了整个系统的集成测试（2）、达到了测试计划中关于集成测试所规定的覆盖率的要求（3）、被测试的集成工作版本每千行代码必须发现至少2个错误（不含优化级别错误）（4）、集成工作版本满足设计定义的各项功能、性能要求（5）、在集成测试中发现的错误已经得到修改，各级缺陷修复率达到标准3、功能/易用测试完成标准（1）、功能测试用例设计已经通过评审（2）、按照功能测试计划完成了功能测试（3）、达到了功能测试计划中关于功能测试所规定的覆盖率的要求（4）、系统达到详细设计定义的各项功能，性能（5）、在功能测试中发现的错误已经得到修改，各级缺陷修复率达到标准4、兼容测试完成标准（1）、兼容测试用例设计已经通过评审（2）、按照兼容测试计划完成了兼容测试（3）、达到了兼容测试计划中关于兼容测试所规定的浏览器的要求（4）、在兼容测试中发现的错误已经得到修改，各级缺陷修复率达到标准5、系统测试完成标准（1）、系统测试用例设计已经通过评审（2）、按照系统测试计划完成了系统测试（3）、达到了测试计划中关于系统测试所规定的覆盖率的要求（4）、被测试的系统每千行代码必须发现至少1个错误（不含五级错误）（5）、系统满足需求规格说明书的要求（6）、在系统测试中发现的错误已经得到修改，各级缺陷修复率达到标准6、验收测试完成标准（1）、软件需求分析说明书中定义的所有功能已全部实现，性能指标全部达到要求。（2）、在验收测试中发现的错误已经得到修改，各级缺陷修复率达到标准（3）、所有测试项没有残余紧急、严重级别错误。（4）、需求分析文档、设计文档和编码实现一致。（5）、验收测试工件齐全（测试计划、测试用例、测试日志、测试通知单、测试分析）7、可靠/压力/负载测试完成标准（1）、性能测试用例设计已经通过评审（2）、按照性能测试计划完成了性能测试（3）、达到了性能测试计划中关于性能测试所规定要求（4）、在性能测试中不通过的用例已经得到修改，性能达到预计标准8、缺陷修复率标准（1）、紧急、严重级别错误修复率应达到100%（2）、普通级别错误修复率应达到95%以上（3）、优化级别错误修复率应达到60%以上注：项目紧急时，普通级别错误修复率达60%以上；优化级别错误修复率达20%即可。9、覆盖率标准（1）、测试用例执行覆盖率应达到100%（功能测试用例均已执行）（2）、测试需求执行覆盖率应达到100%（业务测试用例均已执行）			https://blog.csdn.net/grl18840839630/article/details/101264120
11			['深度学习概述：NLP vs CNN_AI科技大本营-CSDN博客']			作者 | Manish Kuwar译者 | 苏本如，责编 | 郭芮头图 | CSDN 下载自视觉中国出品 | CSDN（ID：CSDNnews）以下为译文：当今，人工智能已经不仅仅是一个技术术语了。这项技术在过去十年的时间内几乎将其影响扩展到了所有行业。现在，每家公司都希望在其系统中实现这一尖端技术，以降低成本、节省时间，并通过自动化使整个工作流程更加高效。最初的人工智能和深度学习算法比较简单，就像我们所知的简单感知器模型和单层神经网络一样。随着时间的推移和更加专注的研究，我们已经拥有了具有多层结构的复杂神经网络。一些公司在他们的软件和服务中使用了LSTMs、GANs、变分自编码器等算法。本文在以下部分列出了人工智能领域当前最热门的技术以及正在研究这些热门技术的公司。看了这些创新之后，准备好大吃一惊吧。如果你想对这些技术作进一步的探索，你可以从这里开始。自然语言处理 – NLP自然语言处理（Natural Language Processing - NLP）是人工智能研究的热点之一。NLP处理语言（文本数据）并执行诸如翻译、音译、语义分析、聊天机器人开发、文本拟态、文本转语音等任务。在当前的场景中，NLP对组织非常重要，因为它通过自动化过程和减少人工干预而提升了客户支持。除此之外，NLP技术也被用于基于对从社交媒体平台挖掘的文本进行语义分析而开发营销策略。然而NLP的难点在于，大多数NLP算法都是基于复杂的深层神经网络，如RNNs、LSTMs和GRUs。为了让自己有一个基本的印象，你可以把它们看作是我们先前提到的基于数据输入（即记忆）的传统神经网络。Dialogflow和Moveworks是基于NLP技术的两家领先公司。让我们简单地看一下这两家公司的情况。DialogflowDialogflow公司是谷歌的一家子公司，它专注于NLP技术的最新研究以及与之相关的模块、API和平台的开发。你可以将Dialogflow公司提供的服务集成到Amazon Alexa、Siri、Cortana，当然还有Google Home中。Dialogflow的服务使用长短期记忆神经网络（LSTM）和递归神经网络（RUN）来执行与文本和语言相关的任务。Dialogflow以其聊天机器人（chatbot）服务而闻名。除此之外，像Dominos、Mercedes、Giorgio Armani这样的公司正在借助Dialogflow服务在其系统中嵌入聊天机器人和文本到语音服务。你可以轻松阅读Dialogflow的服务文档，并在几分钟内在你的网站/移动应用程序上准备好自己的聊天机器人。MoveworksMoveworks是一家总部位于加州山景城（加州Mountain View）的价值2亿美元的公司。它是由Bhavesh Shah、Jiang Chen、Vaibhav Nivargi和Varun Singh联合创建的。每一个聊天机器人和NLP产品都是基于递归神经网络（RNN）或其后继技术。Moveworks服务也不例外，它的服务的核心也是基于其自定义的递归神经网络和基于相关数据训练的LSTM网络。Moveworks为企业提供客户支持自动化和NLP解决方案。它曾服务于Nutanix、Autodesk和Western Digital等巨头。如果你想减少IT支持并使其自动化以节省成本，那么强烈建议你考虑Moveworks。卷积神经网络 - CNNs卷积神经网络（CNNs）是应用最广泛的深度学习算法。CNN算法本来是对传统神经网络的一个小小的改进，它进而发展成为人工智能领域的一个革命性概念。今天，CNNs技术已经被广泛地应用于目标检测、人脸识别、计算机视觉和预测技术。卷积神经网络涉及到深层神经网络中层的堆叠，并在其中对输入图像进行填充、卷积、缩放等操作。CNNs最棒的部分是将图像转换成数组（几乎类似于一维矩阵），然后再进行数学运算，最终得到所需的输出。要实现CNN，你只需要大量的数据和一台计算机来训练你的人工智能模型。下面是几家专注于计算机视觉和CNNs技术研究的公司。Matterport当Matterport公司的研发团队发表了一篇关于Mask RCNN的论文时，它引起了人们的注意。这家总部位于加州Sunnyvale的公司，是计算机视觉产品的赞助人之一。此外，他们还开发了一种高效的计算机视觉目标检测算法（Mask-RCNN），它是Fast RCNN和Faster RCNN的后继算法。Mask-RCNN使用边界框坐标和masking技术来识别已训练模型中的对象。除了目标检测之外，Matterport的Mask RCNN技术还被用于视频呼叫应用程序中来更改背景。今天，Matterport已经将增强现实技术与深度学习结合起来，并发明了现代最好的三维相机之一。Matterport生产的三维相机，可以用来可视化房屋结构，提供虚拟参观，并显示一个令人愉快的平面图。Neurala这家位于波士顿的机器学习和软件开发公司迎合了寻求过程自动化和计算机视觉解决方案的组织的需要。Neurala模型已经被用在了全球超过5,000万台设备上。CB Insights已将Neurala评为“100家最有前途的人工智能公司”。多亏了它的那些拥有博士学位的创始人，他们让Neurala的人工智能软件在包括智能手机在内的各种轻型设备上运行。这些软件在各种设备上提供服务，包括个人电脑、智能手机、无人机、机器人和智能设备。这些是可能可以帮助你建立基于人工智能系统的最可靠和最好的公司。在盲目信任互联网上的任何公司之前，强烈建议你先查看这些公司，它们可能可以帮助你降低成本，并为你提供最先进的服务。原文：https://hackernoon.com/a-deep-learning-overview-nlp-vs-cnn-8gcj3222推荐阅读追忆童年，教你用Python画出儿时卡通人物如何用NLP辅助投资分析？三大海外机构落地案例详解Gary Marcus：因果熵理论的荒诞和认知科学带给AI的11个启示 | 文末赠书AI 终极问题：我们的大脑是一台超级计算机吗？借助大数据进行社交媒体营销，企业们得这么玩！力挺比特币的世界第2交易员：仅次于索罗斯，连续25年无亏损你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106561099
12			['为什么校招面试中总被问“线程与进程的区别”？我该如何回答？_AI科技大本营-CSDN博客']			作者 | 宇宙之一粟责编 | 徐威龙出品 | AI 科技大本营（rgznai100）进程与线程？（Process vs. Thread？）面试官（正襟危坐中）：给我说说“线程”与“进程”吧。我（总是不太聪明的样子）：“限乘？”、“进什么城（程）？”面试官：“操作系统中的进程与线程，你回去了解一下。门在左边，记得关门。”当翻译过来后，这两个概念都带了个“程”字，但进程的英文：Process，而线程的英文：Thread，好像并没有什么联系。大多数初学者一开始都会被这两个概念弄的晕头转向，包括我本人。当你看完这篇文章，可能你就有了新的理解。不信，你接着往下看看。进程和线程基础（理论概念）1. 定义看了下面的定义，可能会有点晕，但我还是要把他写下来（为了严谨）。进程是资源（CPU、内存等）分配的基本单位，具有一定独立功能的程序关于某个数据集合上的一次运行活动，进程是系统进行资源分配和调度的一个独立单位。线程是进程的一个实体，是独立运行和独立调度的基本单位（CPU上真正运行的是线程）。线程自己基本上不拥有系统资源，只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈)，但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。2.区别进程是资源分配的基本单位；线程是程序执行的基本单位。进程拥有自己的资源空间，没启动一个进程，系统就会为它分配地址空间；而线程与CPU资源分配无关，多个线程共享同一进程内的资源，使用相同的地址空间。一个进程可以包含若干个线程。3. 优劣正是因为这二者有区别，所以带来的各自的优劣线程之间的通信更方便，同一进程下的线程共享全局变量、静态变量等数据，而进程之间的通信需要以通信的方式（Inter Process Communication，IPC)进行。不过如何处理好同步与互斥是编写多线程程序的难点。线程的调度与切换比进程快很多，同时创建一个线程的开销也比进程要小很多。但是多进程程序更健壮，多线程程序只要有一个线程死掉，整个进程也死掉了，而一个进程死掉并不会对另外一个进程造成影响，因为进程有自己独立的地址空间。除此之外，推荐看一下阮一峰的一篇博客：进程与线程的一个简单解释，用图解释十分生动形象。为什么这个问题是面试高频？既然这个问题是面试当中会被经常问到的，所以我去网上找一个答案，背出来不就好了。但是，真的背答案就可以了吗？我们来分析一下为什么众多面试官老是问这个问题，他应该并不是想听到一个对书本上概念的重复。那么，他究竟想考什么？侧重点一：面试官想要了解面试者对这一知识点的理解程度（因为这是操作系统中不得不提的一个概念）。如果这个概念回答不上来，意味着面试者对操作系统的学习并不深。侧重点二：面试官可以对你的回答作进一步展开，通过你的回答某个侧重点方向来进一步提问你对你自己回答的理解。（这个高频问题的价值所在）。比如：当你回答到：进程与线程的内存结构不同。进程与进程之间不能共享内存，而线程可以。那么面试官就可以就内存这一点深入提问——内存如何寻址？当你回答：线程之间通信很方便，进程与进程通信不方便。那么问题就又来了，你给我说一下进程之间怎么通信？进程之间通信方法有哪些？不同通信方法有哪些优劣点？一个更满意的答案？如何作答，才能展示一个让面试官更满意的答案？这里就不得不用张三丰教给张无忌的太极拳的那一招——忘掉。。。对就是把上面的概念全都忘掉。只留一个目的：“把敌人打败”。最后用自己的一招一式（理解）来回答。再谈“进程”与“线程”（口语表述）进程的本质：正在执行的一个程序，可以进程比作一个容器或者工厂通过上图，方便我们了解并记忆：进程与进程之间相对独立进程可以包括几个或者上百个线程在运行。内存（逻辑内存）包括在进程里面，每个进程的内存都是互相独立的，但从一个更高的层次上看，不同的进程也共享着一个巨大的空间，这个空间就是整个计算机。进程共有文件/网络句柄（handle），这样可以打开同一个文件，抢同一个网络端口。从不同的视角来看进程：线程的本质：真正运行的是一个一个的线程同理，上图我们知道线程包含：栈（堆栈）：主线程的main函数、进行函数调用的参数和返回地址、局部变量等内容都会被压入栈内PC（Program Couner）：程序计数器，PC的指针指向代码所在的内存地址。TLS（Thread local storage）：分配内存，存放变量当有了上面的问题做引子后，面试官就可以借此引出更多话题：1. 如何通信（沟通）的内容通信是人的基本需求，进程与进程之间是相互独立的，也有通信需求。根据这一问题就可以展开内容提问：进程/线程如何通信答：进程可以通过管道、套接字、信号交互、共享内存、消息队列等等进行通信；而线程本身就会共享内存，指针指向同一个内容，交互很容易。通信方式的差异，比如进程间共享内存和消息队列有何异同？2. 如何同步（协调）的内容一旦有了通信，人与人之间就会产生矛盾，进程也一样。这些矛盾就会体现在如何同步上。在单个CPU下，实际上在任何时刻只能有一个进程处于执行状态。而其他进程则处于非执行状态。我们是如何确定在任意时刻到底由哪个进程执行，哪些不执行呢？进而又可以引出锁的概念？（如何进行进程调度？）线程之间的关系是合作关系。既然是合作，那就得有某种约定的规则，否则合作就会出问题。（如何进行线程同步？）3. 关于内存原理相关问题进程要分配内存，所以开销很大，进程只需要分配栈，分配一个PC就好，内存开销小。这一块就可以问到了操作系统中的内存原理相关的内容。总结总之，如果上述内容你都了解，那肯定是不怕被问到（大佬，请收下我的膝盖）；如果看了此篇文章之后，你能答出个大概，我相信面试官也会放过你，毕竟，我们也真的不是背书机器。所以，我们在回答过程中，尽量别给自己挖坑，用自己理解的知识点进行回答。切忌背书式的回答，模棱两可，因为这样面试官几个连环炮就容易暴露问题了。如果你能看到这，能否给我点个关注，点个赞让我也收到鼓励。如果觉得我写的内容有误，也欢迎评论指出。注意，要敲黑板啦。进程是什么？它指的是一个运动中的程序。从名字上看，进程表示的就是进展中的程序。一个程序一旦在计算机里运行起来，它就成为一个进程。进程与进程之间可以通信、同步、竞争，并在一定情况下可能形成死锁。那么线程是什么？我们知道，进程是运转的程序，是为了在CPU上实现多道编程而发明的一个概念。但是进程在一个时间只能干一件事情。如果想同时干两件事，办法就是线程。线程是进程里面的一个执行上下文或者执行序列。最后，祝大家答的愉快！面试过！过！过！本文首发于 CSDN博客，原文链接：https://blog.csdn.net/yuzhou_1shu/article/details/105326063推荐阅读GitHub标星2000+，如何用30天啃完TensorFlow2.0？清华周界详解《基于图神经网络的事实验证》 |  百万人学AI百年 IBM 终于 All In 人工智能和混合云！微软为一人收购一公司？破解索尼程序、写黑客小说，看他彪悍的程序人生！机器学习项目模板：ML项目的6个基本步骤BM、微软、苹果、谷歌、三星……这些区块链中的科技巨头原来已经做了这么多事！你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/105423835
13			['一群阿里人如何用 10 年自研洛神云网络平台？技术架构演进全揭秘_AI科技大本营-CSDN博客']			【编者按】逍遥子曾说，对阿里人来说「打仗是最好的团建，参加过双 11 的叫战友」，同样，参加过多次双 11 考验的系统技术都会成为真正意义上的基础设施，阿里云飞天系统的云网络平台洛神就是如此。本文作者 —— 阿里云智能事业群研究员、网络产品线负责人祝顺民（花名江鹤）就是洛神背后的核心铸造者。在「CSDN 在线峰会 —— 阿里云核心技术竞争力」上，江鹤为详细分享洛神网络的 10 年演进之路。复制链接或点击「阅读原文」可免费观看江鹤分享视频：https://edu.csdn.net/course/play/28249/388353作者 | 阿里云研究员祝顺民（江鹤）编辑 | 唐小引出品 | CSDN（ID：CSDNnews）今天的主题是《云网络技术架构的演进之路》，主要介绍阿里云网络产品从无到规模应用的 10 年过程中，云网络技术平台洛神是怎么发展的。阿里云飞天洛神云网络平台阿里云系统叫飞天，云网络平台称为洛神，洛神和飞天系统的关系如下图所示，洛神云网络平台是阿里云飞天操作系统内核的核心组件和系统服务，伴随着飞天系统一起诞生、成长。图 1 阿里云网络洛神平台与飞天操作系统整个飞天系统架构分为几个层次，底层数据中心基础设施，包含物理资源、机房、服务器，还有多地域和可用区（AZ）、物理网络等；其上是飞天操作系统的核心，支撑了整个云计算的虚拟化，包括计算平台神龙，存储平台盘古，以及网络平台洛神；基于系统核心组件，构建了面向用户的系统服务以及原生服务，支撑不同行业客户在阿里云上构建自己的应用系统。飞天洛神平台的诞生洛神伴随飞天系统诞生，是云计算产业发展的结果，云计算首先是计算虚拟化，并提供给不同的用户使用，当用户在云上使用计算资源时，网络地址独立规划，不同用户地址可以相同，且虚拟机可以在不同机房之间迁移，地址保持不变，同时云计算是一个基础设施，支持海量用户同时使用，这些关键需求要求提供一个超大规模的虚拟化大二层的网络。图 2 云计算租户网络应用需求这些需求，使用传统商用物理交换机和路由器构建的网络，无法满足云计算的需求，也满足不了阿里云运营这张网络的需求，主要原因在于：1）在百万级别的租户之下，如果每个租户都给到一张独立的地址空间，地址空间的数量非常庞大，传统交换机里面使用 VRF 来隔离租户之间的空间，意味着交换机要支持海量的 VRF 资源，这是目前硬件能力无法支持的。2）服务器和虚拟机的数量增长，网络节点以及路由表的规模快速增大，虚拟机通讯和迁移要求路由表能快速同步。3）当用户对云上网络层面的功能需求越来越多的时候，比如说用户在云上不仅希望有个虚拟机，还可以有个公网 IP；比如说用户可以自行定义这张网络 IP、路由和安全组。另外随着 NFV 技术发展，用户希望在云上运行虚拟化业务网元，例如负载均衡、NAT 网关、VPN 网络，要求对应的底层网络能快速进行功能迭代，以满足新需求和新技术的快速应用。4）更关键的是，传统物理交换机是一个黑盒，各个厂商实现各异，接口各不一样，同时不支持设备内功能定制开发，让设备的运维成为一个难题。图 3 传统网络设备应对云计算的约束所以阿里云网络选择了一条自研的路，使用三大技术满足云计算的需求：1）虚拟化网络；2）SDN 技术，软件来定义网络；3）整个技术栈自研。阿里云网络洛神平台就此诞生。图 4 洛神网络 10 年演进洛神云网络平台随着阿里云飞天系统一起，已经经历了 10 年演进，分为两个阶段，洛神 1.0 时代，主要定位为支撑超大规模和多租户的网络。随着阿里云的业务快速发展，尤其近几年规模增长特别快，2018 年开始推出洛神 2.0，洛神 2.0 的特点主要在高性能、弹性开放能力。现在阿里云上运行着很多超级规模的用户，比如阿里巴巴集团的淘宝业务，这些超大规模用户对网络的性能提出了很高的要求，特别是在例如双 11 这种大型活动期间，网络流量的峰值是十分巨大的，这对网络的性能提出了非常高的要求。同时阿里云提供了丰富的基础 IaaS、 PaaS 产品与服务，第三方的生态伙伴可以在阿里云上提供众多的 SaaS 服务，包括网络类的 SaaS 服务，例如网络管理类、网络数据分析类、网络日志管理类、网络转发类的软件。这些软件在阿里云上很好的运行，需要网络提供弹性，开放的能力，就是洛神 2.0 的定位。飞天洛神 1.0：超大规模、多租户网络洛神 1.0 平台由很多网络组件组成，架构上主要分成两类：AVS（Apsara vSwitch）和各种网关网元。AVS 是部署在每个物理服务器上的软件网元。最早基于 XEN 的 Kernel 架构实现，转发性能不高，单机只有 15 万 PPS。随后演进到了 KVM 的架构，在这个架构下，vSwitch 还是在 Kernel 的，但已能提供单机 120 万 PPS 性能。然后在 Intel 推出 DPDK 版本后，AVS 进行了架构升级，使用用户态 DPDK，转发性能提升至 600 万 PPS。AVS 的演进，是为了追求网络性能大幅提升的过程，是阿里云网络持续迭代和改造网络软件架构的原动力。关于业务网关，最早也是基于 Kernel 网络协议开发。例如 Load Balance 设备，第一代也是基于 Kernel 开发，当时整台物理机性能大约只有 300 万 PPS。为了追求更高的性能，这些网元基于 DPDK 进行了重新构建，既包括公网网关、私网网关，还有 SLB、NAT 网关、VPN 网关等等，转发性能提升了 20 倍。洛神 1.0，基于 DPDK 的 AVS 和各种网关，最终搭建出一张支持超大规模租户、超大规模虚拟机的网络。图 5 洛神 1.0 支持超大规模租户网络如图所示，在这张超大规模的租户网络里面，我们达到的核心能力包含：1）多租户隔离，每个区域可以支持百万规模的 VPC。如果说一个租户对应一个 VPC，一个区域就可以支持百万规模的租户；2）单个 VPC 支持超大规模虚拟实例，目前有一些超大规模的 VPC 中实际部署并运行了超过 30 万的 ECS，也就意味着此租户的这张网络里面有 30 万个以上的 IP， ECS 之间的通讯，包括东西和南北向的流量，都能很好的在虚拟网络里面运行。3）最后就是基于 AVS 能力，单个 ECS 能够提供百万级 PPS 能力的转发性能。飞天洛神 2.0：连接全球，高性能，弹性开放网络洛神 1.0 支撑了阿里云网络从无到有，至规模应用，随着阿里云的业务不断增长，租户越来越多，也带来了越来越多的 VPC。租户购买的虚拟机越来越多，也代表着网络里的节点越来越多，转发性能要求越来越高。此外，也有越来越多的租户在云上不但仅仅满足简单的连通性要求，提出了丰富的业务网元处理诉求。例如用户原来在自己的数据中心里面构建了防火墙和 NAT，或者自己买了一些 Load balancer 的设备，现在业务系统搬到阿里云上之后，希望原来的网络功能继续在阿里云使用。在此背景下，洛神平台升级到 2.0。如图所示，洛神 2.0 提供了更加丰富的转发网元，既有神龙 MOC 卡，还有专用可编程芯片以及通用 ECS；同时洛神 2.0 新构建了一个 NFV 平台，支持业务网元不再依赖传统 X86 服务器，直接基于 NFV 平台构建，例如 NAT、SLB、VPN 产品，同时 NFV 平台支持开放能力，第三方的网元也可以基于 NFV 平台部署，通过 VPC 内和 VPC 间的访问。图 6 洛神 2.0 架构图洛神 2.0 的基础是阿里云遍布全球的网络基础设施和 Region 数据中心的基础设施。目前在全球有 20 个 Region、61 个可用区，100 多个 POP 节点。当国内用户需要出海或者国外的用户需要进中国的时候，不再需要自己购买很多物理形态的设备和网络连接，并花很长的时间搭建出网络基础设施。基于洛神 2.0，阿里云的网络产品就可以分钟内快速构建出运营网络、基础设施。洛神 2.0 的 AVS 和业务网关都进行了架构跨越式升级，AVS 基于软硬件一体化方式，使用神龙 MOC 卡实现快速转发，转发性能提升数倍，达到千万 PPS。图 7 软硬件一体的新一代 AVS洛神 2.0 另一个跨越式架构升级是提供新一代的 NFV 平台，通过网元逻辑部署在通用 ECS 上，提供弹性和开放能力。当第三方厂商将其应用移植到阿里云之后，就可以在阿里云市场里对阿里云的租户进行售卖和提供能力，形成了一个非常好的生态：1）网元可以给大量租户使用；2）客户的白天流量很大，半夜的流量很少时，阿里云会自动帮你缩容。如果说在某个时刻点突然碰到高峰，阿里云会自动协助扩容，也就意味着第三方的网元利用洛神 NFV 平台，不仅可以开放部署，还可以享受到自动的弹性。图 8 洛神 2.0 NFV 开放平台架构飞天洛神的未来洛神 2.0 中，还有一个齐天智能网络平台，是未来演进的重要能力，未来的网络将是一个智能化的网络。当客户业务部署在阿里云的时候，如果网络发生波动或故障，需要非常快速的排查问题，因为网络是属于最底层的服务，当业务不能对外提供服务的时候，首先会想到网络问题。这个时候网络可能是正常的，有可能是上层业务的问题。如何让上层业务团队快速定界问题所在，如何让网络的管理员能快速自证清白，提升未来生产运营服务的效率，就这是未来智能化网络需要解决的问题。图 9 洛神的未来演进未来阿里云洛神平台的发展方向就是智能化，未来会通过自研网元，采集网络中数据，并且用大数据的方式去做产品问题分析和资源规划，目前洛神齐天系统已经构建在一部分能力，在内部使用，未来会提供给阿里云的客户，让客户可以智能化的运营自己的网络基础设施，这就是阿里云洛神平台未来演进的方向。作者简介：祝顺民（花名江鹤），阿里云智能事业群研究员，网络产品线负责人，在软件定义网络（SDN）和网络功能虚拟化（NFV）领域有多年技术研发和管理经验。他带领团队历时多年，从无到有，打造了业内最丰富的云网络产品线，并铸造了阿里云飞天系统的云网络平台-洛神，服务了百万用户，并历经 8 次天猫双十一考验，已经成为真正意义上的基础设施。欢迎所有开发者扫描下方二维码填写《开发者与AI大调研》，只需2分钟，便可收获价值299元的「AI开发者万人大会」在线直播门票！推荐阅读GitHub标星2000+，如何用30天啃完TensorFlow2.0？8比特数值也能训练模型？商汤提训练加速新算法丨CVPR 2020400&nbsp;多行代码！超详细中文聊天机器人开发指南 | 原力计划微软为一人收购一公司？破解索尼程序、写黑客小说，看他彪悍的程序人生！机器学习项目模板：ML项目的6个基本步骤BM、微软、苹果、谷歌、三星……这些区块链中的科技巨头原来已经做了这么多事！你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/105479498
14			['前百度主任架构师创业，两年融资千万美元，他说AI新药研发将迎来黄金十年..._AI科技大本营-CSDN博客']			「AI技术生态论」 人物访谈栏目是CSDN发起的百万人学AI倡议下的重要组成部分。通过对AI生态专家、创业者、行业KOL的访谈，反映其对于行业的思考、未来趋势的判断、技术的实践，以及成长的经历。2020年，CSDN将对1000+人物进行访谈，形成系列，从而勾勒出AI生态最具影响力人物图谱及AI产业全景图。本文为 「AI技术生态论」系列访谈第13期。百万人学AI你也有份！参与文章评论，评论区留言入选，可获得价值299元的「2020 AI开发者万人大会」在线直播门票一张。作者 | Just出品 | AI科技大本营（ID:rgznai100）要在AI医疗领域创业，扎实的AI技术和对医疗行业背景的了解缺一不可，这也是许多创业者想要在此掘金却望而止步或中道崩溃的原因。但是，也总有一些开拓者能不断探索前路，望石智慧创始人兼CEO周杰龙就是其中之一。早在硕士研究生时期，周杰龙的研究方向就是人工智能。2011年，他加入百度，曾任百度主任架构师，负责百度搜索机器学习排序、反垃圾和移动云语音搜索、拍照搜索等项目。两年后，他带领团队把深度学习引入到百度搜索引擎，用机器学习方法替代了人工规则，他称这是全球首次成功将深度学习应用于大规模搜索系统。百度工作4年后，他把目光聚焦在医疗行业，并最终确定在深耕新药研发赛道创业。作为一名从互联网跨界到医药研发的创业者，他越来越发现做药物研发与搜索引擎的逻辑共性，一个非常基础的环节就是不断验证候选化合物与靶点的匹配过程，这类似于用户在搜素引擎中不断得到相匹配的搜索列表。“这些蛋白其实都是原子构成的，在成药的分子里常用的有机化学元素也就10种，你可以把它看成是10个字母，一个分子式是由10个字母所构成的一个句子，靶点是大分子，相当于很长的篇章，所以它像是句子跟篇章之间匹配的关系。”经过近一年的调研后，2018年，望石智慧正式成立，周杰龙带领团队正式利用AI技术新工具专注早期新药研发。应用深度学习，在10^60化学空间中搜索有效分子一般而言，新药研发要先确定靶点，然后去做化合物的发现以进行药物候选，候选药物审批过关后去申报IND，临床试验后才能最终申报上市。总之，这是一个产业链很长的行业。而在药物发现过程中，AI在每一环节都可以发挥作用。要发现一种新药，相当于要从10的60次方化学空间里去寻找一个有效化合物分子，这是一个庞大空间的搜索过程。早期，化学家通过已有的医药研发知识，比如在了解靶点架构的基础上展开医药设计，但有时需要的靶点结构并没有被测定，只能找到少数合适的分子化合物。不过，AI的出现可以从已有的庞大药物数据中挖掘更多靶点结构，从而给人类专家在早期药物设计上提供更多思路。“这就好比要装修一个房子，人类设计师凭个人经验可能只能想出几种构图，但AI学习了大量的建筑和装修设计数据后能推荐出数十种甚至上百种构图，这样就能辅助人类设计师发现新颖的房屋设计图。”周杰龙对AI科技大本营（ID:rgznai100）解释。当前，深度学习已经在CV、NLP等领域展现出强大的能力，同样地，也可在药物研发的早期环节药物设计阶段发挥强力作用。药物设计一般会经历多个环节，包括候选分子库生成、分子活性预测、分子性质预测、分子结构优化等。基于GAN和ANN的候选分子库生成技术已得到普遍应用，并展现出良好的性能。机器学习一直是分子活性预测的重要工具，在深度学习发展以前，各种传统的机器学习方法（包括SVM，RF等）已得到普遍应用，并成为非常有竞争力的预测手段。近年来，深度学习技术和计算能力的发展，带来更大的应用空间，研究人员开始逐步采用3D CNN和GCN等网络技术，展现出更优的性能和潜力，为药物设计提供更有效的手段。对于早期新药发现的AI技术链条，不同的环节有不同的技术路线。周杰龙介绍，通过机器学习，尤其深度学习来做化合物发现，会借鉴搜索推荐技术来预测其一系列性质。而逆合成反应可能又会用翻译模型进行逆合成反应路线设计和分析。最后，信号通路是蛋白与蛋白、基因和蛋白之间的相互作用网络，概率图模型则可以应用到信号通路研究进行建模和推断。基于上述AI技术，望石智慧目前构建了面向小分子新药发现的智能化药物分子设计及知识图谱两大平台。两大新药研发的“杀手锏”，覆盖药物早期发现智能化药物分子设计平台是借助分子进行药物设计，针对药物设计前期的苗头、先导以及候选药物环节，望石智慧用产品的不同子模块去解决对应的问题。“一个分子可以认为是由骨架和药效团构成，类似于树干和树枝的关系。在药物设计中，骨架跃迁好比对药物分子进行树干部位的修改，而骨架衍生则是对分子的树枝进行变化。”周杰龙解释说，通过骨架跃迁，药物化学专家可以找到破专利的新分子，然后通过骨架衍生找到先导化合物，之后通过分子优化模块，去改善先导化合物的某个性质，在此基础上设计出质量更好的新分子。在技术上，这三个子模块也有不同的目标和实现方法。骨架跃迁中一个重要的目标是，找到从整体和药效团角度来看都与输入的参考分子比较相似的分子。因此这里可以借用很多深度学习算法模型，例如语言模型中的翻译模型，将两个相似分子分别看成两种不同的语言，但它们都有核心相同的内容。这样就可以借助模型生成大量相似的分子，供后续筛选、排序使用。后续的筛选和排序算法也涉及到很多深度学习或机器学习方法，是一个很复杂的系统。骨架衍生是为了从一个不错的骨架生成更有活性的分子，模型能够根据骨架学习到该骨架背后的靶点信息，从而更好地生成可能的侧链。而分子优化是为了获得在某个性质上更优质的分子，在有一个比较准确的评价方法基础上，可以通过强化学习等方法对整个分子生成过程中进行策略的调整，同时也限制分子生成的相似度，这样来保证生成的分子具有更优质的性质。目前智能化药物分子设计平台已在学术界和工业界推广使用。另一方面，望石智慧的药物知识图谱平台成为药物研发中的另一关键利器。药物知识图谱平台将为药物研发算法模型提供源源不断的高质量数据，同时，由于基于知识与情报的AI医药研发领域是以专利为核心，所以药物知识图谱平台还可以提供靶点、适应症、药物以及基因相互作用和属性信息，帮助药企在立项、调研等工作中提供有价值的参考信息。深度学习技术在知识图谱中的发挥依赖于大规模高质量数据。公开的专利和发表的文献是药物数据的重要来源，CV和NLP技术是数据的自动化解析和知识图谱构建的手段。周杰龙表示，对于早期的小分子新药发现，这两大平台已基本涵盖了应有的功能，这也是望石与药企展开项目合作的基础。但由于医药研发周期比较长，望石智慧会在新药发现的不同阶段提供相应价值进行变现。“不同于基于文本的搜索推荐，上线小流量验证都在线上完成，迭代非常快，但做新药发现无论是推荐一条合成路线还是分子设计，需要花几个月时间才能够把分子合成结果进行反馈。”他说。当然，目前的AI平台设计也需要不断做技术迭代，包括解决多靶点的问题，与DNA编码化合物库技术、高通量技术以及其他技术的结合。更重要的是，借助平台与合作方进行深度落地实验。一方面他们需要用历史数据进行回顾，确保算法性能，另一方面也需要专家把控，来确定化合物的效用，如此才能验证平台的能力。目前望石智慧有多个合作项目，其中一个项目是寻找新冠病毒的特效药。今年2月初，通过其两大平台，他们利用新冠病毒RNA的聚合酶抑制剂和核酸，对几十篇新专利进行信息挖掘，运用知识图谱技术进行结构化梳理，找到1400多个化合物并将相应化合物信息与医药机构进行公开分享。同时他们对新冠病毒RdRp靶点利用计算做了同源建模，利用分子动力学对该复合物进行模拟，然后基于望石智慧分子设计平台，将分子和靶点RdRp进行对接。目前，他们正在与中国医学科学院协和药物所合作一项新冠项目。结语2017年，AI+医学影像在资本的助推下成为创业风口，伴随2020年新冠疫情的发生，AI医药研发也旋即升温。周杰龙认为，公众对这一行业的认知更加迫切，也更明晰，他相信未来十年是中国新药研发的“黄金十年”，而AI会起到非常重要的作用。他预测，未来2-3年是证明AI技术辅助医药研发的一个非常关键的时期。“ 一个候选药物从早期研发到进入临床要几年时间，如果有多个AI设计的药物进入临床，应该就足以说明这件事情的价值非常大，临床试验成功当然更好。”望石智慧正在朝这一目标前行。3月中旬，他们宣布融到了由美元基金长岭资本和线性资本联合投资的近千万美元A轮融资，计划下一阶段在核心产品研发，加强与药企CRO上下游之间的合作，以及在引入更多AI和药物研发优秀人才等方面进行投入。“AI技术生态论”近期系列文章：第12期：循环智能杨植麟：“人机耦合”将是对话语义应用的新趋势第11期：堪称奇迹！8天诞生一个产品，这家创业公司做到了第10期：红外光抗疫、成功预测新基建，投资280家企业的光学博士到底是谁？欢迎所有开发者扫描下方二维码填写《开发者与AI大调研》，只需2分钟，便可收获价值299元的「AI开发者万人大会」在线直播门票！推荐阅读GitHub标星2000+，如何用30天啃完TensorFlow2.0？8比特数值也能训练模型？商汤提训练加速新算法丨CVPR 2020400&nbsp;多行代码！超详细中文聊天机器人开发指南 | 原力计划微软为一人收购一公司？破解索尼程序、写黑客小说，看他彪悍的程序人生！机器学习项目模板：ML项目的6个基本步骤BM、微软、苹果、谷歌、三星……这些区块链中的科技巨头原来已经做了这么多事！你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/105479497
15			['深度学习面试必备的25个问题_AI科技大本营-CSDN博客']			作者 | Tomer Amit译者 | 弯月，编辑 | 屠敏出品 | CSDN（ID：CSDNnews）在本文中，我将分享有关深度学习的25个问题，希望能够帮助你为面试做好准备。1.为什么必须在神经网络中引入非线性？答：否则，我们将获得一个由多个线性函数组成的线性函数，那么就成了线性模型。线性模型的参数数量非常少，因此建模的复杂性也会非常有限。2.说明解决神经网络中梯度消失问题的两种方法。答：使用ReLU激活函数代替S激活函数。使用Xavier初始化。3.在图像分类任务中，相较于使用密集神经网络（Dense Neural Network，DNN），使用卷积神经网络（Convolutional Neural Network，CNN）有哪些优势？答：虽然两种模型都可以捕获彼此靠近的像素之间的关系，但CNN具有以下属性：它是平移不变的：对于过滤器而言，像素的确切位置是无关的。更不容易发生过度拟合：一般而言CNN中的参数比DNN要少很多。方便我们更好地理解模型：我们可以查看过滤器的权重，并可视化神经网络的学习成果。分层性质：通过使用较简单的模式描述复杂的模式来学习模式。4. 说明在图像分类任务中可视化CNN特征的两种方法。答：输入遮挡：遮挡输入图像的一部分，看看哪部分对分类的影响最大。 例如，针对某个训练好的图像分类模型，将下列图像作为输入。如果我们看到第三幅图像被分类为狗狗的概率为98%，而第二幅图像的准确率仅为65%，则说明眼睛对于对分类的影响更大。激活最大化：创建一个人造的输入图像，以最大化目标响应（梯度上升）。5. 在优化学习速率时，分别尝试学习速率：0.1、0.2，…，0.5是好办法吗？答：这种方法并不好，建议使用对数比例来优化学习速率。6. 假设一个神经网络拥有3层的结构和ReLU激活函数。如果我们用同一个值初始化所有权重，结果会怎样？如果我们只有1层（即线性/逻辑回归）会怎样？答：如果所有权重的初始值都相同，则无法破坏对称性。也就是说，所有梯度都会更新成同一个值，而且神经网络将无法学习。但是，如果神经网络只有1层的话，成本函数是凸形的（线性/ S型），因此权重始终会收敛到最佳点，无论初始值是什么（收敛可能会较慢）。7.解释Adam优化器的概念。答：Adam结合了两个想法来改善收敛性：每个参数更新可加快收敛速度；动量可避免卡在鞍点上。8.比较批处理，小批处理和随机梯度下降。答：批处理是指在估计数据时获取整个数据；小批处理是通过对几个数据点进行采样来进行小批量处理；而随机梯度下降是指在每个时期更新一个数据点的梯度。我们需要权衡梯度计算的准确度与保存在内存中的批量大小。此外，通过在每个epoch添加随机噪声，我们可以通过小批处理（而非整个批处理）实现正规化效果。9.什么是数据扩充？举个例子。答：数据扩充是一种技术，通过操作原始数据来增加输入数据。例如，对于图像，我们可以执行以下操作：旋转图像、翻转图像、添加高斯模糊等。10. 解释GAN的概念。答：GAN（Generative Adversarial Network）即生成对抗网络，通常由两个神经网络D和G组成，其中D指的是判别器（Discriminator），而G指生成网络（Generative Network）。这种模型的目标是创建数据，例如创建与真实图像并无二样的图像。假设我们想要创建一只猫的对抗示例。神经网络G负责生成图像，而神经网络D则负责判断图像是否是猫。G的目标是“愚弄”D——将G的输出始终分类为猫。11.使用Batchnorm有什么优势？答：Batchnorm能够加快训练过程，而且（一些噪音的副产品）还具有调节作用。12.什么是多任务学习？应该在什么时候使用？答：当我们使用少量数据处理多个任务时，多任务处理将很有用，而且我们还可以使用在其他任务的大型数据集上训练好的模型。通过“硬”方式（即相同的参数）或“软”方式（即对成本函数进行正则化/惩罚）共享模型的参数。13.什么是端到端学习？列举一些优点。答：端到端学习通常是一个模型，该模型能够获取原始数据并直接输出所需的结果，而无需任何中间任务或功能工程。其优点包括：无需手工构建功能，而且通常可以降低偏差。14.如果在最后一层中，我们先使用ReLU激活函数，然后再使用Sigmoid函数，会怎样？答：由于ReLU始终会输出非负结果，因此该神经网络会将所有输入预测成同一个类别。15.如何解决梯度爆炸的问题？答：解决梯度爆炸问题的一个最简单的方法就是梯度修剪，即当梯度的绝对值大于M（M是一个很大的数字）时，设梯度为±M。16.使用批量梯度下降法时，是否有必要打乱训练数据？答：没有必要。因为每个epoch的梯度计算都会使用整个训练数据，所以打乱顺序也没有任何影响。17.当使用小批量梯度下降时，为什么打乱数据很重要？答：如果不打乱数据的顺序，那么假设我们训练一个神经网络分类器，且有两个类别：A和B，那么各个epoch中的所有小批量都会完全相同，这会导致收敛速度变慢，甚至导致神经网络对数据的顺序产生倾向性。18.列举迁移学习的超参数。答：保留多少层、添加多少层、冻结多少层。19. 测试集上是否需要使用dropout？答：不可以使用！dropout只能用于训练集。dropout是训练过程中应用的一种正则化技术。20.说明为什么神经网络中的dropout可以作为正则化。答：关于dropout的工作原理有几种解释。我们可以将其视为模型平均的一种形式：我们可以在每一步中“去掉”模型的一部分并取平均值。另外，它还会增加噪音，自然会产生调节的效果。最后，它还可以稀释权重，从根本上阻止神经网络中神经元的共适应。21. 举个适合多对一RNN架构的例子。答：例如：情绪分析，语音中的性别识别等。22.我们什么时候不能使用BiLSTM？说明在使用BiLSTM我们必须做的假设。答：在所有双向模型中，我们都可以假设在给定的“时间”内访问序列的下一个元素。文本数据（例如情感分析、翻译等）就是这种情况，而时间序列数据则不属于这种情况。23. 判断对错：将L2正则化添加到RNN有助于解决梯度消失的问题。答：错误！添加L2正则化会将权重缩小为零，在某些情况下这实际上会让梯度消失的问题更严重。24. 假设训练错误/成本很高，而且验证成本/错误几乎与之相等。这是什么意思？我们应该做些什么？答：这表明欠拟合。我们可以添加更多参数，增加模型的复杂性或减少正则化。25. 说明为何L2正则化可以解释为一种权重衰减。答：假设我们的成本函数为C(w)，我们再加上一个c|w|2。使用梯度下降时，迭代如下：w = w -grad(C)(w) — 2cw = (1–2c)w — grad(C)(w)在该等式中，权重乘以因子<1原文链接：https://towardsdatascience.com/50-deep-learning-interview-questions-part-1-2-8bbc8a00ec61【End】推荐阅读半小时训练亿级规模知识图谱，亚马逊AI开源知识图谱嵌入表示框架DGL-KE首次揭秘！大麦如何应对超大规模高性能选座抢票？AI 四巨头 Google、DeepMind、Microsoft、Uber 深度学习框架大比拼马化腾、马云并列成为中国首富；百度回应“将上线电商直播”；.NET 5 Preview 2 发布 | 极客头条程序员职场背锅甩锅指南警惕！新骗术出现：这些虚假二维码生成器已成功盗取 4.6 万美元！“出道” 5 年采用率达 78%，Kubernetes 的成功秘诀是什么？你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/105400808
16			['小白必看：合理搭建巨量引擎账户结构要点总结！_suisuiwang的博客-CSDN博客']			可能看到这个题目有人就要问了，账户结构又不直接影响转化和流量，随便设置一下就好了，有必要这么大动干戈地讨论怎么搭建吗？但到底是不是这样的呢？我们可以结合手上现有的案例去思考一下，例如，要复盘这次在巨量引擎上的广告投放，我们是不是得清楚自己投了哪些产品，投放到了哪些地域？用了什么定向？使用了哪些创意……等一系列问题，但一个一个去查广告是不是很繁琐？这个时候，如果你提前搭建了一个更有利于我们查看投放数据的账户结构，这些问题就迎刃而解了。接下来，我们就来重点说说合理搭建账户的重要性。为什么要搭建合理的账户结构？必要性1：搭建了健康有序的账户有助于进行广告管理，可以为后续的账户优化建立良好的秩序基础。必要性2：可以细化投放策略，提升投放效果。必要性3：清晰的创建思路，可以节省投放时间，及时发现问题并处理。必要性4：便于数据统计与分析，可清晰的对比测试结果。怎么搭建？5大要点要注意既然有这么多优势，那该怎么搭建呢？现在，我们就来梳理一下广告优化的操作流程，操作流程共分为7个步骤，即账户结构规划，创建广告组，创建广告计划，创建广告创意，广告投放，报表分析和广告优化，虽然看起来比较简单，但还是有些要点需要注意。接下来，我们就来详细了解具体账户搭建过程中的要点。要点1：搭建之前，首先要考虑整体账户结构规划。在使用巨量引擎广告投放平台之前，即我们要投放什么产品，要了解广告组和广告计划的搭建思路，广告创意如何搭建，从哪些维度去思考等等，之后我们进行广告创意，完成这些操作之后，我们就可以进行广告投放了，广告投放过程中会产生投放数据，那么，为了得到更好的优化效果，就需要我们对这些投放的数据进行数据分析，根据数据分析的结果，再反哺我们的广告优化。这样，我们才能在账户搭建过程中，不断完善我们的账户和创意投放。在这里要跟大家强调一下，巨量引擎账户搭建还是有一定的科学方法可遵循的。要点2：要对每个模块的功能要了如指掌。推广管理：在推广这个界面，可以进行广告的创建与修改。一个广告账户包含三个层级，广告组，广告计划和创意，广告组管理广告计划，广告计划管理创意 。报表版块：可以展示账户的推广数据，通过数据报告进行数据的监控与分析，根据数据进行广告的调整与优化。财务版块：进行账号流水的查询与核对。工具板块：辅助账户优化工具。分为6大类，账户辅助，定向辅助，创意辅助，优化辅助，计划辅助，开放平台，可根据投放需求选择合适的工具进行使用。账号管理：包含账户相关、消息中心，主页关联、预览绑定，可通过此模块管理或修改账户相关信息，查看消息、对公验证或进行广告预览的设置等。要点3：要对广告账户的基本结构有了解。一个账户包含广告组、广告计划、广告创意三个基本层级，支持多创意投放功能。一个广告账户可以包含多个广告组。广告组，主要管理推广目的，预算，一个账户最多可创建500个。广告计划，主要管理定向、出价、投放时间、预算，一个广告组最多可创建500个。广告创意，主要管理广告位、广告创意，一条计划最多可以创建30个。一个广告组可以包含多个广告计划，一个广告计划又可以包含多个广告创意。要点4：具体账户搭建思路。账户搭建之前我们要分析广告主推广目标、公司状况、产品特点和行业情况。怎么理解？首先，我们需要根据客户的需求来明确客户的推广目的，推广目标、推广需求、明确推广目的、投放位置、平台。第二，根据公司不同产品线收益及其他推广平台的成本表现、预算情况，明确推广品类，这样可以帮助我们便于搭建账户后分配预算和出价。第三，我们需要根据产品定位、卖点、受众特点，去帮助适配创意及定向。第四，根据竞争对手特点、精品卖点、构思文案方向，形成差异化营销。在对广告主的推广目标、公司状况、产品特点和行业情况等进行分析之后，还要选择适合广告主的搭建方法。是按照素材形式搭建？还是根据广告主产品搭建？还是针对投放地域搭建、还是根据投放时间进行搭建也是有技巧的。一般情况下，选择按照素材形式搭建，适用于产品相对单一但素材制作能力较强的广告主。选择产品搭建的，适用于产品类型丰富的广告主。选择针对投放地域搭建，适用于业务有明显地域特征的广告主。选择根据投放时间搭建，则适用于品牌活动较频繁的广告主。接下来我们来列举几个常见的账户结构案例，第一个案例，某游戏类客户，它是通过投放平台、广告形式定性划分的。首先，客户在投放时，有IOS的下载需求，也有安卓的下载需求，那客户其实在IOS和安卓进行了详细了的区分，分为ios视频和iOS图片，安卓视频和安卓图片，在ios视频下他又进行了一个细致的梳理，从兴趣定向、兴趣关键词和DMP人群包进行了广告计划的搭建，在计划的搭建过程中，根据兴趣定向中他又进行了不同维度的A、B测试，从男性18-50岁和性别不限18-50岁来测试一下，看哪种性别会更适合，这样在广告计划的搭建过程中维度就清晰多了。其实，在广告组的设置中，我们可以灵活的去进行，如把固定需要测试的位置放在广告组的位置，把灵活需要测试的位置放在广告计划的维度。第二个案例，某相册APP，它是按照用户使用的场景来进行划分的，按照素材和定向来划分广告计划的，其实，这种主要是针对使用场景比较多的广告主而言。最后一个案例是，某团购类APP，这个比较特殊，因为客户其实对他的一个城市及地域的投放有严格的要求，所以在设置时，他按照地域来进行设置，广告组也是按照地域来设置，广告计划是按照单品和活动来设置，单品分为水果、零食、粮油，活动又分为一元购、新用户福利。所以，还是要根据自己的实际需求来进行合理搭建。要点5：涉及到数字时要考虑清楚。这里要注意的是广告组日预算不能低于1000元，预算修改幅度不能低于100元，在实际操作中要考虑进去。还有投放时要根据成本合理选择投放方式，如果说预算很有限，现在就只想要很低的成本，这个时候就建议优先选择低成本。如果说要优先跑量，那就最大化加强跑量效率。如果说要均衡投放，那就尽量均匀地消耗预算，避免流量突增。关于如何在巨量引擎上搭建合理账户结构，暂时先给大家介绍这么多，毕竟再多秘籍，也得靠一步一步实践，希望大家都能搭建出科学合理的账户结构！			https://blog.csdn.net/suisuiwang/article/details/101313916
17			['flink 1.9.0 编译：flink-fs-hadoop-shaded 找不到_九师兄-CSDN博客']			1. 编译lcc@lcc flink$ mvn clean install -Dmaven.test.skip=true -Dhadoop.version=2.8.3  -Dmaven.javadoc.skip=true -Dcheckstyle.skip=true报错[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 03:23 min[INFO] Finished at: 2019-09-24T16:35:49+08:00[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal on project flink-s3-fs-hadoop: Could not resolve dependencies for project org.apache.flink:flink-s3-fs-hadoop:jar:1.9-SNAPSHOT: Failure to find org.apache.flink:flink			https://blog.csdn.net/qq_21383435/article/details/101293031
18			['自动化神经网络理论进展缓慢，AutoML算法的边界到底在哪？_AI科技大本营-CSDN博客']			作者 | 夕颜出品  | CSDN（ID:CSDNnews）「AI 技术生态论」 人物访谈栏目是 CSDN 发起的百万人学 AI 倡议下的重要组成部分。通过对 AI 生态顶级大咖、创业者、行业 KOL 的访谈，反映其对于行业的思考、未来趋势判断、技术实践，以及成长经历。本文为 「AI 技术生态论」系列访谈第二十五期。百万人学 AI 你也有份！今日起，点击阅读原文报名「2020 AI开发者万人大会」，使用优惠码“AIP211”，即可免费获得价值299元的大会在线直播门票一张。限量100张，先到先得。每个人都希望有免费的午餐，就像希望 AutoML 能够提供一个一统天下的算法，可以轻松搞定一切机器学习问题一样。但这本身就是一个悖论，没有一套可以适用于所有问题的算法和系统。AutoML 如今已经算不上是一个新鲜词汇了，自 2018 年谷歌发布第一款 AutoML 产品以来，围绕这项技术的讨论就一直存在。有人将之视为实现通用人工智能的利器，但是也有人反对将这项技术的作用过度夸张化。但得到大家一致同意的一点是，AutoML 在实现 AI 推广中，的确有着非凡的意义。今天，CSDN 采访到在国内第一批从事 AutoML 研究的专家之一——第四范式副总裁、主任科学家涂威威，从他的专业视角，审视自动化机器学习的技术和应用进展现状，并尝试找到推进这项技术继续完善的突破口所在。师从周志华，受戴文渊触动走向AutoML研究之路涂威威开始关注 AutoML，源于他在百度的工作经历。大学期间，涂威威所学的专业是计算机，研究生阶段在南京大学师从 AI 大师周志华，从此与机器学习结下不解之缘。在他求学期间，机器学习并没有现在这样火，但涂威威敏感地捕捉到，这将是一个很有潜力的发展方向。在机器学习还未大火之时，涂威威偶然间获得了一次到百度实习的机会。在这里，他认识了他的 导师，在迁移学习领域已经有所建树，也是后来创立了第四范式的戴文渊。初入百度，涂威威惊讶于机器学习在百度广告系统中的重要作用，因为在那个时代，百度已经在依靠机器学习获得可观的广告收入。涂威威与戴文渊团队一起做广告点击率预估时，为了让效果付费的成果更好，他们开始研究精准匹配、点击率预测等当时还比较困难的问题，并构建出点击率预测模型。相比于一般的专家系统，百度的搜索系统规则成千上亿条，大量的数据人根本无法理解，机器在这时就可以做到人所不能做到的，机器总结出足够复杂的模型，用机器学习的方式处理数据，相比于原有的系统，收入提升 8 倍多。这样，在涂威威离开百度之前，已经在广告系统中应用深度学习方法，构建了一整套机器学习底层框架和训练系统。做这些事情的过程中，涂威威和戴文渊其实已经开始思考，如何才能将这些能力带给更多企业和行业，因为不是所有的企业都有着充足的时间、金钱和 AI 人才，来做这些耗资巨大的研究和部署，如果可以用一种简单的方法，把 AI 技术更好地输入给需要的企业，为什么不做？这也是后来戴文渊创立第四范式的初衷——希望能降低 AI 技术和应用门槛，让AI能够在各行各业落地，实现AI for Everyone的目标。涂威威非常认同这样的愿景和业务价值，因此加入了第四范式并开始了创业之旅。从范式平台，看AutoML典型技术迭代路线几年下来，如今涂威威带领的自动机器学习算法研发团队已发展到上百人的规模，按照研发的技术方向，下设独立的小分队，囊括了结构化数据、图像识别、自然语言处理、语音识别、交互式 AutoML优化、半监督学习、强化学习等多个方向。在这样的分工下，第四范式AutoML产品HyperCycle ML进行着持续优化迭代。在涂威威的解说下 CSDN 了解到，作为国内较早的 AutoML 产品，它们经历过同类产品类似的技术迭代，从中我们得以一窥这项技术的发展轨迹。在最早的版本中，第四范式AutoML能做的事情实际上更多地是分类任务，研究的重点是特征工程，比如组合特征。什么是组合特征？比如新闻资讯类软件为用户做推荐，对于男性用户，不同职业的人关注点不一样，即便是同一个人，不同时间段想看的内容可能也有巨大的差别，这就需要对用户的职业、性别、爱好等特征做组合，刻画微观场景，推荐效果才会有提升。问题是，这个组合会产生的可选结果可能是个天文数字，如何从中选择最优组合，是一件非常困难的事情。针对这个问题，第四范式自研了自动特征组合AutoCross，取得了非常好的效果。涂威威发现，通过这种显式组合特征的方式，效果甚至比一些深度学习系统还要更好。随着业务的拓展，涂威威发现需要解决的问题越来越多，除了二分类问题，还需要解决多分类问题、回归问题，等等，以及这些结构化数据问题之外的非结构化数据和半结构化数据的问题。随后，涂威威又相继带领团队开发出了模型自带自动化特征的线性分形分类器、自动时序树模型工具HE-TreeNet、自动时序嵌入（TemporalGo）、自动多表时序特征（FeatureZero）、自动深度稀疏网络AutoDSN（Deep Sparse Network）、AutoSSL（自动半监督学习）、AutoPU、AutoWSL、AutoCV、AutoSpeech、AutoNLP、AutoRL、多保真度优化、交互式AutoML等多项AutoML技术，已涵盖结构化数据、非结构化数据（图像、文本、语音等）、半结构化数据（异构信息网络、知识图谱等）等全场景的AutoML探索。在实际落地中，算法只是其中一环，解决一个业务问题，除了算法之外，还会面临数据、模型应用系统等问题。首先是数据，比如高质量标注数据远远不够，离线数据分析时数据产生不一致，都会造成坏的影响。此外，他们需要做非结构化数据和半结构化数据的自动机器学习处理，技术难度更大。再者，实际生产过程中还会产生目标需要探索的问题。涂威威举了两个例子，第一个例子是多目标优化，当前AutoML系统优化的目标是固定的，往往现实的问题是多种目标综合的，比如需要在效果和成本之间做微妙的选择，这样的多目标的探索，往往人在获得结果之前也没有办法有效评判。这样的情况目前的AutoML就很难支持。第二个例子是可解释性问题，自动化机器可以找到一个解，但这个解可能并不是用户想要的，用户可能想要的是一个可解释的模型。然而，涂威威说到，可解释性本身即存在着很大的不确定性，因为每个人的理解不同，与个人判断有很大的关系，让模型具有可解释性就难上加难。最后，涂威威提到了自动化机器学习的一个弊端，即在没有任何业务背景的情况下，机器会进行各种各样的尝试，但人无法将丰富的经验加入到自动机器学习系统中。对此，第四范式提出了交互式自动机器学习。目前，这种交互式自动化机器学习的方式已经在第四范式进行小规模尝试，现在的产品中也已经有了应用，下文将会有详细解释。总结下来，第四范式做自动化机器学习经历了很多版本的迭代，从最早的只有二分类拓展到多分类、回归，从结构化数据拓展到图像、视频、文本、NLP等非结构化数据，到覆盖低质量数据的自动弱监督学习，再到保护隐私的自动多方机器学习……第四范式的自动化机器学习产品随着业务的不断扩展，不断优化迭代，未来也还将继续拓展至更多的技术方向和应用领域。虽然AutoML还有很长的路要走，但是涂威威相信，随着全球研究者和众多厂商的加入，AI规模化应用之路必将会创造更大的价值。为此，第四范式打造了全球首个AutoML社区，同时也是最大的AutoML算法比赛平台—— AutoML.ai，通过发布企业AI应用建设需求的AutoML赛事，让更多的科研机构、技术大牛参与进来，去解决企业复杂的业务问题。该平台上的比赛不仅可以保护企业数据安全，且比赛结果具备极高的复现性，可将模型快速应用于企业实际业务场景。技术的更新迭代永远不能脱离真实场景，这也是第四范式在AutoML路上不断前行的动力。AutoML热门研究方向那么多，哪些意义更大？AutoML 作为自动化机器学习的统称，下面还有很多分支研究方向，其中比较热门的方向包括神经网络架构搜索（NAS）、效率和泛化性优化、动态 AutoML、强化学习 AutoML 等。在涂威威眼里，这些方向的研究和应用现状是怎么样的？哪些方向研究对于实际应用的意义更大呢？神经网络架构搜索AutoML 技术将来要想取得重大突破，在更广阔的搜索范围里搜索真正的新架构是一个重要方向。第四范式在神经网络架构搜索方面有一些探索，比如如何更加高效地进行神经网络架构搜索，如何搜索更加高效的神经网络架构以实现更低成本的推理，等等。效率和泛化性此外，AutoML 的热点研究方向包括效率和泛化性，第四范式在提高模型和算法的效率和泛化性上的做法，也许也值得大家参考。在效率和泛化性上，第四范式主要的思路是算法与底层架构相结合。比如结构化数据上，第四范式有探索自动特征组合、自动多表时序特征、自动模型融合、自动深度稀疏网络等，图像有探索自动分类、自动检测、自动图像分割、自动生成等，文本、语音也有相关的分类、识别、问答、增强、生成等的自动化探索。在效率上，一方面在算法层面，第四范式通过多种手段来提升 AutoML 的效率，包括优化配置搜索空间、提升配置评估效率以及利用类似于基于迁移学习的多保真度优化、超高维超参数优化、混合优化策略等提升配置优化算法的效率。除了算法层面，同样重要的是，在自动机器学习计算上有很多的重复可复用的计算，当前主流的计算框架（比如 Tensorflow、PyTorch 等）只是为单次机器学习模型训练优化，并没有为自动机器学习的计算进行优化，第四范式重新为自动机器学习设计了底层计算架构，对多次模型学习提供了配置评估和优化上横向和纵向的动态计算优化，同时在参数上探索参数共享计算等，可以做到只用增加单次模型学习 60% 左右的计算代价，就可以获得数十次的配置评估，比原来的架构有数量级的效率提升。在泛化性上，第四范式也探索了诸如弱监督学习、元学习、动态环境学习等自动化。举例来说，弱监督学习领域的噪声标记学习、半监督学习、PU 学习等不同的学习场景的关键问题很不一样，有的由于标记比较少，需要探索更好的配置评估方式，提出更能估计泛化能力的度量指标，有的需要探索更有效的模型训练方式。再比如，对于动态环境学习，需要能够根据数据分布的变化，自动进行模型适配。动态环境中的 AutoML另一个值得关注的点，是在动态环境中进行 AutoML 与静态环境相比难度更大，具体来说难在哪？第四范式是怎么做的？涂威威表示，相比静态环境，动态环境的核心是环境是动态变化的，机器学习如何有效地应对动态环境至今都是国际学术界一个开放的问题，也是目前研究的热点难点方向之一。动态变化可能包含很多方面的变化，比如数据分布、数据表达、问题目标变化等。对此，第四范式主要从几个方面进行应对：一方面对于特征表达的学习，第四范式有动态的特征学习，可以更快地适应数据的变化，并从特征的自动化设计上，更好地应对潜在动态的变化；另一方面从模型的学习方法上，第四范式利用自动迁移学习技术，检测分布变化，并自动适配对应的迁移学习技术，还会利用动态集成学习技术，通过分布变化，自动适配不同类型的模型等。在架构层面上，第四范式也构建了针对 AutoML 的在线学习架构，对于快速变化的环境，尽快地进行模型迭代，让 AutoML 更加快速有效地适应环境的变化。强化学习与 AutoML 结合近年来，强化学习得到了业界和学界的关注和重视，自然有人会探索强化学习与 AutoML 相结合的方法，以释放 AutoML 的更大能量。然而，此前涂威威曾在公开演讲中表示，用强化学习进行 AutoML 有种“杀鸡用牛刀”的感觉。几年过去，强化学习与 AutoML 结合的时机成熟了吗？对此，涂威威表示仍然对这个问题保留原有的看法，在他看来，目前还没有真正有突破性进展的结果，主要的原因是强化学习本身的发展不是一蹴而就的，而是一个艰难的长期问题。当前我们经常看到的强化学习研究，其实多是强化学习应用的研究，而针对强化学习核心问题的研究其实极少，且难度极大，强化学习本身依然是一个很难的问题，因此与 AutoML 相结合，仍然任重而道远。安全性与可解释性安全性和可解释性也是 AutoML 的热点研究方向，第四范式在提高 AutoML 的安全性和可解释性上，也有一些独特的思路。涂威威提到，在 AutoML 的安全性上，第四范式针对不同的场景探索不同的技术方案，如隐私保护的自动机器学习、自动多方机器学习、基于多方安全计算的自动机器学习、自动联邦迁移学习等。他说到，一方面，需要降低具有很强安全性的机器学习算法落地的门槛，机器学习算法已经有很高的门槛了，因此安全的机器学习算法门槛更高，难度更大，所以需要自动化的安全机器学习算法来降低落地使用门槛；另外一方面，以往非自动的安全自动机器学习算法，都需要人类专家的介入，在一定程度上引入了数据和模型安全的风险，自动化安全机器学习技术能够在降低门槛的同时，进一步提升安全性。其次，与其他非技术问题相比，安全性在技术问题上的解决方案是相对成熟的，或者是有技术解决方案的，但是目前落地缺乏法律法规和行业标准的支持，第四范式也在积极与众多研究机构与厂商推进如IEEE 联邦学习、多方安全计算等标准的建立和完善。在可解释性上，第四范式支持了多种主流的可解释性方案，比如数据，特征、模型，学习过程的可解释方案等，也基于二次学习、博弈论等技术探索了独有的可解释方案。涂威威说，实际上，可解释机器学习关于“什么是可解释”是模糊的，每个人对可解释的理解不同，因此，他个人认为一个好的可解释模式应该是交互式的，由 AutoML 给出结果，人来判断是否符合自己的可解释标准以及一致性标准，人再给出反馈，修改 AutoML 学习目标，让 AutoML 更新结果。这也是第四范式交互式 AutoML 的重要功能之一。另外，值得一提的是，第四范式目前也在推进可解释机器学习相关的标准制定。难点它还是难点，就在那里然而，虽然不断地有企业推出各种声称简单便捷的 AutoML 产品和服务，但是在实际落地过程中还是有很多的难点，用涂威威的话来说，这些难点过了很久还是难点，就在那里。首先，在“道”，即理论上，AutoML 还有太多的问题没有得到回答，比如说神经网络，目前来看，神经网络理论本身就处于很不成熟的状态，更不用说自动化神经网络了，理论进展非常缓慢。其次，一个不可忽略的点，是在于这个技术能够解决的问题的边界上，大家思考得并没有那么清楚。它到底能解决什么问题？因为一统天下的算法是不存在的，自动化机器学习理论上来说就像是一个一统天下的算法，然而我们都知道，一套能够解决所有问题的算法本身就是悖论。再者，AutoML 长期存在比较严重的问题，一是效率，二是效果。如何比现在的效果更好？如何让 AutoML 更高效？最后，涂威威还提到了目前完全黑盒的 AutoML 方案有很多弊端，主要体现在三个方面，第一个方面是优化的目标是固定的，往往现实的问题是多种目标综合的，比如一个场景里面，线性模型的 AUC 值是 0.80，深度神经网络模型的 AUC 值是 0.81，按照效果优先，应该选择深度神经网络模型，但是线性模型上线所需要的计算资源是 1 0台普通服务器，而深度神经网络模型可能需要 1000台高配 GPU 服务器，对于一些关键业务，可能还是会选择深度学习模型，但是对于一些成本敏感或者业务规模不大的业务，可能更加适合的是线性模型，因为成本更低，效果却差不多。这样的多目标的探索，往往人在获得结果之前也没有办法有效评判，需要在精度和成本之间做微妙的选择。这种情况目前的AutoML 就很难支持。第二个方面就是数据科学家可能有很多自己拥有的业务背景知识，有很多有用的信息没有办法输入给黑盒的 AutoML 工具，比如有些有价值的特征，或者有一些根本不需要尝试的参数组合，一定程度上会影响 AutoML 得到的效果和效率。第三个方面就是实际业务可能会对实际的机器学习流程有一些定制化的需求，比如只能使用某一类的数据处理工具，这样的一些需求在目前黑盒的 AutoML 方案上无法得到满足。第四范式认为，更好的方式应该是人机结合的交互式自动机器学习。总之，虽然现在大家可以拿出一些很好的解决方案，有些地方甚至比人做得更好，但是更多时候是会发现，即使是做得很好的地方，机器与最优秀的人类专家相比还是有一定的差距。因此，无论是效果还是效率，AutoML都有很大的进步空间。未来研究方向当前，AutoML 技术生态与落地是否已经处于成熟阶段，相信是很多人关心的问题。虽然热度不减，但涂威威认为 AutoML 技术上依然需要不断完善，其实用户需要的就是“物美价廉”的技术，从算法效果和效率上，交互式自动机器学习方案的探索上，都还有很大的进步空间。他说到，在全球研究者和众多厂商的推进下，目前 AutoML 技术现在已经在很多场景落地，得到了应用，但现在更大的问题是如何实现规模化地落地，“不是在某一些团队内部，也不是在某一两个场景上应用，而是在更多行业、更多场景中落地。”涂威威表示，AutoML 未来的技术突破需要在理论和算法层面有更深层次的研究。理论上，我们需要探索 AutoML 算法的边界，因为根据没有免费的午餐定理，没有可以解决所有问题的通用算法，AutoML 算法泛化性如何等问题是需要深入探索的；算法上，就是在 AutoML 的三要素上联合探索：(1)如何设计更好的 AutoML 配置空间；(2)如何更好地更高效地对机器学习的配置进行评估；(3)如何更好更快地搜索更有效的配置；期望获得更好的自动机器学习效果，或者提升自动机器学习的效率，亦或提供对机器学习关键问题的一些新的 insight。在 AutoML 技术的落地层面上，还需要解决 AutoML 上下游的问题，上游更多是数据治理的问题，需要探索为机器学习设计的数据治理方式，下游更多的是模型应用的问题，需要探索如何更有效地利用机器学习模型来提升业务。另外，未来 AutoML 研究的新范式——交互式自动机器学习，通过人机结合的方式完成机器学习流程的搭建，也是非常重要的值得探索的研究方向。【End】6月2日20:00，CSDN 创始人&董事长、极客帮创投创始合伙人蒋涛携手全球顶级开源基金会主席、董事，聚焦中国开源现状，直面开发者在开源技术、商业上的难题，你绝不可错过的开源巅峰对谈！立即免费围观：更多精彩推荐☞我只是追个直播，结果被拉进大咖们的群面对面群聊……☞微信公众号关闭iOS端虚拟支付业务；苹果「Apple 登录」存安全漏洞；谷歌推迟发布Android 11 Beta| 极客头条☞可怕！CPU 竟成了黑客的帮凶！☞如何用NLP辅助投资分析？三大海外机构落地案例详解☞这 10 个云计算错误，会让你的业务一蹶不振！☞好扑科技结合区块链行业发展趋势，重磅推出“好扑区块链合伙人”计划你点的每个“在看”，我都认真当成了喜欢			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106484804
19			['如何在高精度下求解亿级变量背包问题？_AI科技大本营-CSDN博客']			导读：国际顶级会议WWW2020将于4月20日至24日举行。始于1994年的WWW会议，主要讨论有关Web的发展，其相关技术的标准化以及这些技术对社会和文化的影响，每年有大批的学者、研究人员、技术专家、政策制定者等参与。以下是蚂蚁金服的技术专家对入选论文《Solving Billion-Scale Knapsack Problems》做出的解读。作者 | 齐冯出品 | AI科技大本营（ID:rgznai100）背包问题 (knapsack problem) 是经典的整数规划问题，求解如何从多个物品中选取一个子集放入背包，在容量限制下最大化子集的效用。互联网场景下很多问题可以看成超大规模的背包问题或者它的变种问题，比如红包营销，用户流量分配等，都有某种总资源的限制，需要在大量的用户粒度的决策中选取一个子集来最大化业务收益。由于背包问题是 NP-hard，求解复杂度高，所以精确算法无法做较大规模的求解。而近似类算法对问题的形式化有具体要求，实际业务的需求一般不会严格符合背包问题的定义，所以需要求解算法有更强的泛化性和通用性。因此，如何在高精度下求解超大规模背包问题及其变种问题仍然是一个挑战。简介据我们所知，我们的工作是最早做到对亿级变量的背包问题求解工作之一。我们的问题形式化涵盖了互联网海量数据场景下的泛化背包问题。它的“物品”有两个维度：用户和选项，即“为每位用户选择哪些选项”。它的“背包容量”扩展到了多个维度，即每个用户的每个选项可以消耗多个不同的资源。同时我们还支持对每个用户的选项做任意整数规划的约束。整个形式化的数学表达如下：上式中的 (2) 为资源约束，我们称之为“全局约束”，一般不超过几十个或上百个，（3）为每个用户的选则规则，我们称之为“局部约束”，数量可以上亿。为了求解这个超大规模的泛化背包问题，我们采用拉格朗日松弛求解整数规划（Lagrangian relaxation for integer programming）的框架，将全局约束（2）乘以拉格朗日乘子后合并到目标函数（1）中。给定拉格朗日乘数，松弛过的问题可以拆解为每个用户独立的整数规划问题，这些问题数量可能上亿，可以并行化求解。当这些整数规划的约束（3）符合特定的层级化形式时，我们提出了可以求得最优解的多项式复杂度算法，而更复杂的约束形式则可以通过通用整数规划求解器求解。为了求得拉格朗日乘数的最优解，我们采用同步坐标梯度下降（Synchronous Coordinate Descent）法在当前拉格朗日乘数的基础上对每个乘数做独立并行优化。整个算法交替进行拉格朗日乘数更新和独立并行整数规划这两个步骤，直到收敛。解决方案的实现在 Apache Spark 上通过 MapReduce 接口完成。解读拉格朗日松弛求解整数规划的框架只提供最优解的验证条件，我们仍需要对具体的算法验证解的正确性和求解流程的速度。为验证正确性，我们对比了我们的算法的解和单纯形法 (Simplex) 求解线性松弛后问题的解，还评价了我们解的对偶间隙 (duality gap)，发现我们求得的目标函数值距离理论上界差距极其微小（~1%），在超大规模问题上可以忽略不计。速度方面，我们的 MapReduce 实现求解亿级变量十级资源约束的问题可以在一小时内收敛，占用 200 个 Spark executors (1600 个CPU)。此外，我们对比了同一框架下常用的对偶梯度下降（Dual Descent）和我们设计的SCD这两种方法。后者不仅在需要的迭代轮数上少于前者，在约束的遵守上也远比前者严格。我们设计的 SCD 利用拆解后的子问题的具体形式预判重要的拉格朗日乘数值，用高精度扫描最优解附近的乘数值，从而达到快速准确的收敛。前景我们泛化后的背包问题形式覆盖了很多互联网场景下的用户颗粒度的多资源分配问题，同时我们的实现以及业务落地经验证明了我们求解方法在超大规模优化问题上的可操作性和实际效果。因此这个解决方案应该对互联网行业有较强的参考价值。今日福利遇见陆奇同样作为“百万人学 AI”的重要组成部分，2020 AIProCon 开发者万人大会将于 7 月 3 日至 4 日通过线上直播形式，让开发者们一站式学习了解当下 AI 的前沿技术研究、核心技术与应用以及企业案例的实践经验，同时还可以在线参加精彩多样的开发者沙龙与编程项目。参与前瞻系列活动、在线直播互动，不仅可以与上万名开发者们一起交流，还有机会赢取直播专属好礼，与技术大咖连麦。门票限量大放送！今日起点击阅读原文报名「2020 AI开发者万人大会」，使用优惠码“AIP211”，即可免费获得价值299元的大会在线直播门票一张。限量100张，先到先得！快来动动手指，免费获取入会资格吧！点击阅读原文，直达大会官网。你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/105629136
20			['阿里云科学家入选计算机顶会HPCA名人堂，他是什么来头？_AI科技大本营-CSDN博客']			记者 | 夕颜出品 | CSDN（ID:CSDNnews）近日，由 IEEE 主办的高性能计算架构国际研讨会 HPCA 公布了最新一期名人堂（Hall of Fame）名单，来自阿里云基础设施服务器研发团队的科学家蒋晓维成为名人堂新晋成员。这是首次有云计算领域学者入选，蒋晓维也是阿里巴巴第二位入选 HPCA 名人堂的科学家。HPCA名人堂很牛吗 ？蒋晓维这次入选的 HPCA 名人堂是什么来头？关注计算机结构的朋友一定听说过，它与计算机体系结构国际研讨会（ISCA）和微体系结构国际研讨会（MICRO）齐名，并列计算机体系结构领域三大国际顶会，论文专业领域包含 CPU 体系结构、高性能计算、AI 芯片、I/O、安全、新介质研究等，过去 26 年来推动了多项计算机系统技术的发展，包括 90 年代末的 SMT 技术、2005 年的虚拟化技术、10 年的非易失内存技术等。计算机体系结构领域的三大顶会分别有自己的名人堂，HPCA 名人堂成员包括 GPU 之父 Bill Dally 等。HPCA 名人堂的入选要求极为严苛，入选标准为发表 8+ 篇 HPCA 论文，论文平均入选率仅为 15%。（HPCA 名人堂名单）以往，入选 HPCA 名人堂者基本都是来自于各国的大学和研究所，纯粹来自工业界背景的人很少，华人更是凤毛麟角，自成立以来仅有 5位华人学者获得 HPCA 名人堂荣誉，其中就包括蒋晓维在阿里巴巴的同事——阿里巴巴达摩院高级研究员、平头哥首席科学家谢源。。在一众计算机体系架构顶尖专家中脱颖而出，实属不易。这不禁让人好奇，蒋晓维是怎样一个人，为何能获此殊荣。幸运的是，恰逢此次 HPCA 宣布名人堂入选人之际，CSDN 有了一次与蒋晓维本人交流的机会，来深入了解一下蒋晓维其人。别人家的小孩，长成芯片“大佬”在计划之中自从 2014 年加入阿里云基础设施服务器研发团队，蒋晓维至今仍在这里从事服务器研发工作，研究领域覆盖 CPU 架构、芯片设计、网络、虚拟化技术等。在此之前，他曾在 Intel 和 Google 工作，在芯片、软硬件领域硕果累累。在 Intel，他是首颗超低功耗 CPU Quark D1000 的首席架构师，定义了其核心架构，参与 多代芯片端到端开发测试流片和产品化；同时他也是 Edison SoC 芯片的架构师。在 Google，他参与Google第二代SDN网络的研发工作，负责其中的数据面开发。加入阿里后，他完成了国内首颗 x86 CPU 的特性定制化工作，使阿里成为国内第一家部署基于定制CPU 的自研服务器的公司，开了国内定制化 CPU和自研服务器 的先河；在网卡领域，他定义了高性能虚拟网络硬件转发，确定了阿里云网络转发性能业内第一的地位。从现在的视角会看，蒋晓维的这些成果在当时的背景下，无一不对当时的计算机体系架构领域产生重大影响。不积跬步无以至千里，蒋晓维是如何从幼龄小童成长为一个对中国乃至世界计算机领域做出贡献的科学家呢？从小到大，蒋晓维的成长经历都是典型的父母眼里别人家的小孩。他生于六朝古都南京，是土生土长的南京人。小学时期，蒋晓维就在计算机上显示出了不同于同龄小孩的聪慧，尤其擅长数学，小学三年级就被南京市选拔开始学习编程，六年级获南京市青少年奥林匹克信息学竞赛一等奖，初中和高中分别获得过全国数学联赛江苏赛区一等奖。在这个过程中，蒋晓维不仅对编程产生浓厚的兴趣，还对程序如何运行在底层硬件这件事很感兴趣，毕竟在那个年代，包括 CPU 在内的计算机底层芯片对于人们生活的影响还没有很明显的体现，程序能在一枚小小的芯片上运行对蒋晓维来说是件新奇的事。当时，他曾接触过一个叫做 laser310的芯片，可能现在很多人已经认不出这是何物，它其实就是一个只有 8K 的 ROM，固化了 basic编程语言的 CPU。按照蒋晓维的话来说，回头看当时的底层芯片，真是“low”到不可思议。但随着 CPU 技术的发展，现在底层芯片已经发生了翻天覆地的变化，性能得到千万，甚至上亿倍的提升。进入互联网时代，分布式、大数据、AI 各种应用层出不穷，大家都在追求极致的性能。小时候这股“开挂”的石头后劲十足，一直持续到他进入南京大学学习电子工程，又一路顺利读研，最终在北卡州立大学完成博士学业，取得计算机工程博士学位。在读博期间，蒋晓维接触到很多计算机领域的专家，为他之后学业和职业生涯奠定了基础。蒋晓维是个目标明确的人，认准了计算机架构这个方向才定向选择到北卡州立大学求学，并选择了自己的导师和专注于计算机结构领域实验室。在实验室里，蒋晓维认识了很多对他产生重大影响的专家们，其中对他影响最大的，是他博士期间的导师（Yan Solihin）。这位来自印尼的教授是美国电子电气工程师协会 Fellow，在计算机体系结构各个领域都有很多学术贡献，也对 x86 等各种商业化产品产生过较大的影响，比如 x86 里的CAT，即Cache QoS技术，他就是最早的发起者，硬件安全领域中的安全处理器，最早也是由他提出的。在这位态度严谨的导师的影响下，蒋晓维专精于计算机体系结构，同时也广泛涉猎操作系统，具备安全等跨领域能力，对他后来在工作中的选择方向和领域产生了比较重要的影响。毕业后，蒋晓维先后在 Intel、Google 和阿里巴巴从事计算机结构、网络、服务器等领域工作。在全球顶尖科技公司工作，三家公司给了他不同的感受和经历。打造首颗低功耗CPU Quark D1000Intel 是蒋晓维毕业后走向职业生涯的第一个“东家”。这家老牌公司虽然员工平均年龄是三家公司中最大的，但创新能力也是有目共睹。在这里，蒋晓维是史上首颗低功耗x86 CPU Quark D1000，以及 Edison SoC 的架构师。图源：视觉中国Quark 一族早已历经数次迭代，性能早已不同往日，但回顾起来，Quark D1000 对于Intel 公司，乃至整个 CPU 技术和市场，都有着重要的意义。不仅是 Quark D1000，蒋晓维作为首席架构师研发的 Edison SoC 对于 Intel 来说也是至关重要的产品。在 2014 年的 CES 展上，Intel CEO 发布了这枚片上系统，引起了很大关注。加入阿里巴巴之后，等到 2015 年蒋晓维路过杭州的云栖小镇，当地已然围绕 Edison SoC 形成了一个创业生态社区。在Google打造第二代SDN网络在 Google，蒋晓维感受到了与 Intel 不一样的氛围。这里的员工平均年龄更小，非常鼓励创新。图源：视觉中国在这里，蒋晓维从事的是数据中心负责业界最大规模的第二代 SDN 网络研发工作，团队里有一群来自于美国各个大学，拥有教授背景的同事。学术界其实很早就已经提出 SDN 的概念，Google 是第一家将之实现并在数据中心大规模部署的公司。在阿里，开创国内定制化CPU和自研服务器先河时间来到 2014 年，蒋晓维加入了阿里巴巴，与一群更加年轻的人共事。与 Google 总体单调的业务相比，阿里的业务场景更加复杂，给了做技术的人更多与业务结合，以业务驱动技术创新的发挥空间。他做的第一件重大成果就是完成了国内 x86 CPU 定制化和自研服务器的工作，打造了离线和在线业务负载tracing 能力，开创了国内定制化 CPU和自研服务器 的先河。阿里的初衷，是让底层的 x86 CPU 和服务器能够更好地适配阿里云的计算需求，蒋晓维团队的工作主要是填充之间的 gap。所以，他们针对阿里云在性能在功耗各方面的特征和需求展开 trace，捕捉这些业务的特征。有了这个能力之后，阿里一方面可以有的放矢地进行性能优化，更重要的是联合英特尔针对性地对 CPU 和服务器做后期改动，让 CPU和服务器更好地满足阿里云的需求。同时，蒋晓维团队还针对阿里业务定义 benchmark，让定制 CPU 和自研服务器从系统层面、微架构层面确保性能符合业务需求。值得一体的是，在阿里，蒋晓维与同被入选 HPCA 名人堂的谢源一起工作，两人不仅在工作上交集颇多，也保持着不错的私人关系。在蒋晓维看来，谢源是华人中的旗帜性人物，由于在计算结构领域的前沿方向性贡献，谢源是目前唯一一个“集齐”ISCA、MICRO、HPCA 三大顶会名人堂荣誉的华人。虽然不在同一个部门，但工作上的共性让两人之间有了一些合作，比如合作发表关于图计算与高性能计算集群 EFLOPS 论文。除去这些荣誉上的光环，蒋晓维眼中的谢源是个平易近人，球技不错的足球爱好者。在专业上，谢源是个对技术方向性具有前瞻和预判的科学家，此前在 AMD 建立中国团队的经历，也让他在学术和工业两方面具备了良好的判断力和执行力。在笔者看来，同被选入顶会名人堂，可能与蒋晓维和谢源两人身上普通人不具备的共同特质分不开，比如对技术创新促进技术发展，通过技术手段应用于实践，给上层业务带来红利，反过来业务也可以驱动底层创新这一点坚信不疑，换句话说，他们都是 believer。高性能计算的未来：新内存介质将涌现，图计算是方向性领域在学术上，今年 2 月份，蒋晓维系统性介绍了阿里云的高性能 AI 集群的节点架构、网络架构、和通信算法的一篇论文被 HPCA 收录，该论文名为《EFLOPS: Algorithm and System Co-design for a High Performance Distributed Training Platform》，展示了他对于高性能计算的思考和 EFLOP S集群为阿里巴巴业务带来的价值。图源：视觉中国这个由来自多个部门的阿里技术专家参与打造的集群可将大模型的训练速度提升4 倍，并支持千万分类模型的训练；在提升翻译模型精度的同时，能将训练时间从100 小时降低至12小时。关于高性能计算的未来，蒋晓维也给出了一些自己的看法和预测。他说到，高性能计算，包括 CPU 所擅长的通用型计算未来都会出现一些新的技术与趋势。首先，是新的内存介质和计算架构会源源不断地涌现，比如就在过去几年，非易失内存 Apache Pass 的出现使得内存能力得到巨大提升，同时也改变和计算架构，很快会有其他介质的内存出现，in memory computing技术也会不断产品化。此外，未来软硬一体化的设计思路将会更多，结合业务上层的软件特质，对底层架构设计产生打的影响。最后一点是图计算将是未来计算领域的方向之一，包括稀疏性计算在图计算和一些 AI 场景下都会体现出重要性。无论是 CPU ，GPU还是计算体系，这方面都有很大的提升空间，在不久的将来可能会出现相应的解决方案。除了大的方向性趋势，蒋晓维还从实处聊了聊当前的处理器现状。进入智能计算时代，IoT 已成为未来应用的新趋势已成为业界的共识，因此针对 IoT 的底层计算设备至关重要。MCU 处理器就是非常适合 IoT 场景的底层硬件。在微控制器架构层面上，现状是 x86 和 Arm 等架构并存，的从专业视角来看，哪种架构形式将成为主导呢？MCU 架构又会向什么方向和趋势发展？首先，蒋晓维说到，MCU 处理器其实更多地是针对 IoT 场景，之前这一领域是 Arm 的强项，但最近几年 RISC-V 崛起，不断地吞噬这一部分市场。就 MCU 来说，从软件生态上来看，Arm 和 RISC-V 一定会是主导，但除了这两个架构之外，可能也会有其他在这个生态之外的单个点的架构的存在。而对于数据中心的服务器，不管是公有云还是私有云，因为 x86 上层的软件生态构建相对完善，x86 的服务器可能在未来一段时间都是主导，但是 Arm 也是一股不容小觑的崛起的势力，在中国，Arm 已经在服务器侧推出了一些产品。深度优化底层硬件在图计算领域的性能表现至关重要在 HPCA 收录的文章中，除了 EFLOPS，蒋晓维和研发团队还率先提出 CPU 架构上运行图计算的六大瓶颈，并给出相应的优化方案，为图计算未来的芯片架构和服务器架构奠定了理论基础。之所以在图计算上投入巨大精力，是因为阿里云认为图计算是一个继 AI 之后额方向性领域。蒋晓维认为，图计算与 AI 的处境有所不同，深度学习目前已经有了很不错的底层硬件支持，比如谷歌的 TPU，阿里云的含光 800、GPU 等，但是图计算上，无论是 CPU 或 GPU，还是服务器架构，底层硬件对图计算的支持几乎是空白。未来，业务必将向图计算发展，因此，深度优化底层硬件在图计算领域的性能表现，这对于学术界和业界来说都具有重要的意义。推荐阅读全网唯一秃头数据集：20万张人像，网罗各类秃头Uber 前无人驾驶工程师告诉你，国内无人驾驶之路还要走多久？干货 | 基于SRS直播平台的监控系统之实现思路与过程程序员在这些地方敲代码，普通笔记本根本扛不住借助大数据进行社交媒体营销，企业们得这么玩力挺比特币的世界第2交易员：仅次于索罗斯，连续25年无亏损你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106610209
21			['斩获GitHub 2000+ Star，阿里云开源的Alink机器学习平台如何跑赢双11数据“博弈”？..._AI科技大本营-CSDN博客']			作者 | 郭芮来源 | CSDN（ID:CSDNnews）「AI技术生态论」人物访谈栏目是CSDN发起的百万人学AI倡议下的重要组成部分。通过对AI生态顶级大咖、创业者、行业KOL的访谈，反映其对于行业的思考、未来趋势的判断、技术的实践，以及成长的经历。2020年，CSDN将对1000+人物进行访谈，形成系列，从而勾勒出AI生态最具影响力人物图谱及AI产业全景图！本文为该系列访谈的第九期，通过和阿里资深算法专家、Alink创始人杨旭的一对一访谈，深入批流一体机器学习平台Alink的“台前幕后”。百万人学AI你也有份！参与文章评论，评论区留言入选，可获得价值299元的「2020 AI开发者万人大会」在线直播门票一张。每次购物狂欢都是技术平台的一场数据“博弈”。去年双十一，阿里旗下的电子商务平台天猫就再一次刷新了数据记录，而强大的系统处理性能更是让业界敬佩不已：单日数据处理量达到970PB，每秒处理峰值数据高达25亿条，并帮助天猫产品推荐的点击率提高了4％——这一连串的数据背后，离不开Alink的支撑。作为业界同时支持批式算法、流式算法的机器学习平台之一，Alink基于Flink开发而来，提供了丰富的算法组件库和便捷的操作框架，且目前已被广泛运用在阿里内部的搜索、推荐、广告等多个核心实时在线业务中，以及支持Kafka、HDFS和HBase等一系列开源数据存储平台。在本文中，CSDN有幸采访到了Alink创始人杨旭，他将从一线开发的视角，带我们了解这个开源机器学习平台的技术路径、典型应用案例及发展规划等内容。杨旭，机器学习Alink创始人，阿里巴巴集团计算平台事业部的资深算法专家，阿里云机器学习算法平台PAI中基础机器学习算法的负责人。Alink衍生背景：算法工程师的开发诉求随着大数据时代的到来和人工智能的崛起，机器学习所能处理的场景更加广泛和多样。构建的模型需要对批量数据进行处理，为了达到实时性的要求还需要直接对流式数据进行实时预测，还要具备将模型应用在企业应用和微服务上能力。为了取得更好的业务效果，算法工程师们需要尝试更多更复杂的模型，需要处理更大的数据集，使用分布式集群已经成为常态；为了及时对市场的变化进行反应，越来越多的业务选用在线学习方式直接处理流式数据、实时更新模型。杨旭解释道，“我们团队一直从事算法平台的研发工作，感受到了高效能的算法组件和便捷操作平台对开发者的帮助。”针对正在兴起的机器学习广泛而多样的应用场景，他和所带领的团队在2017年开始基于Flink研发新一代的机器学习算法平台，使得数据分析和应用开发人员能够轻松搭建端到端的业务流程。Alink究竟是什么？Alink 是阿里巴巴计算平台事业部PAI团队从2017年开始基于实时计算引擎 Flink 研发的新一代机器学习算法平台，提供丰富的算法组件库和便捷的操作框架，开发者可以一键搭建覆盖数据处理、特征工程、模型训练、模型预测的算法模型开发全流程。项目之所以定为Alink，是取自相关名称（Alibaba, Algorithm, AI, Flink, Blink）的公共部分。借助Flink在批流一体化方面的优势，Alink能够为批流任务提供一致性的操作。杨旭提到，在2017年初，他们通过调研团队看到了Flink在批流一体化方面的优势及底层引擎的优秀性能，于是基于Flink重新设计研发了机器学习算法库，即Alink平台。该平台于2018年在阿里集团内部上线，随后不断改进完善，在阿里内部错综复杂的业务场景中锻炼成长。“作为业界首个同时支持批式算法、流式算法的机器学习平台，Alink 提供了 Python 接口，开发者无需 Flink 技术背景也可以轻松构建算法模型。”据杨旭介绍，Alink 已被广泛运用在阿里巴巴搜索、推荐、广告等多个核心实时在线业务中。在此前落幕的天猫双 11 中，单日数据处理量达到 970PB，每秒处理峰值数据高达 25 亿条。Alink 成功经受住了超大规模实时数据训练的检验，并帮助提升 4% CTR（商品点击转化率）。Alink功能简介1、丰富的算法库Alink拥有丰富的批式算法和流式算法，帮助数据分析和应用开发人员能够从数据处理、特征工程、模型训练、预测，端到端地完成整个流程。如下图所示，Alink提供的开源算法模块中，每一个模块都包含流式和批式算法。比如线性回归，包含批式线性回归训练、流式线性回归预测和批式线性回归预测。2、友好的使用体验“为了提供更好的交互式和可视化体验，我们在开源的同时推出了PyAlink，用户可以通过PyAlink的Python包以notebook的方式使用Alink。”杨旭表示，PyAlink不仅支持单机运行，也支持集群提交，并且打通了Operator(Alink算子)和DataFrame的接口，从而使得Alink整个算法流程无缝融入Python。PyAlink也提供使用Python函数来调用UDF或者UDTF。PyAlink在notebook中使用如下图，展示了一个模型训练预测，并打印出预测结果的过程。3、与Spark对比在离线学习算法方面，Alink 跟 SparkML 性能对比基本相当，下图给出的是一些经典算法的性能对比：通过上图可以看出，Alink在大部分算法性能优于Spark，个别算法性能比Spark弱，整体是一个相当的水平。但是，“在功能的完备性方面，Alink更有优势”，Alink除了覆盖Spark的算法，还包含流式算法、流批混跑、在线学习、中文分词等。阿里和Alink的开源之路在2018年，GitHub新增活跃用户数量超过了前六年的总和，相较于2017年新增了40%的组织机构和30%的代码仓库。从全球趋势来看，开源无疑是软件发展的大势所趋。目前在国内，阿里是贡献开源最出色的企业。GitHub上有大量的开源项目由阿里创建，据阿里经济体GitHub开源生态报告统计，国内Top10的开源项目中，阿里的开源项目有6个。在谈Alink开源之前，杨旭首先介绍了与之相关的Flink与FlinkML。“Flink是一个面向数据流处理和批量数据处理的可分布式的开源计算框架，我们看好Flink引擎的优秀性能，希望基于Flink解决流程机器学习场景的问题。”FlinkML为Flink自带的机器学习算法库，分为旧的版本和新的版本。“在做Alink前，我们首先认真调研了当时的FlinkML（即旧版本FlinkML）的情况，其仅支持10余种算法，支持的数据结构也不够通用，在算法性能方面做的优化也比较少，而且其代码也很久没有更新。所以，我们放弃了基于旧版FlinkML进行改进、升级的想法，决定基于Flink重新设计研发机器学习算法库，随后发展为现在的Alink。”Alink在发展的过程中一直与Flink社区紧密关联，在每年的Flink Forward大会上，团队一直有汇报项目的进展，共同探讨技术问题，获取反馈和建议。随着Alink功能的不断增强和完善，“社区中欢迎Alink进行开源的呼声日益高涨，我们也开始和Flink社区更紧密联系，推动开源Alink的代码进入FlinkML。”与此同时，社区中更多的人意识到旧版FlinkML的问题，决定整个废弃掉旧版FlinkML，建设新版FlinkML。“我们积极参加新版FlinkML API的设计，分享Alink API设计的经验；Alink的Params等概念被社区采纳；之后开始为新版FlinkML贡献算法实现代码，已提交了40余个PR，包括算法基础框架、基础工具类及若干算法实现。”Alink包含了非常多的机器学习算法，在向FlinkML贡献的过程中，需要社区commiter的讨论设计与审查代码，这个过程有助于代码的精益求精，但由于社区commiter的资源有限，代码完全贡献到FlinkML的过程会持续很长时间。“这时，我们不得不考虑是否有其他方式，可以让用户先用起来”，“Alink单独开源是个很好的解决方式”，它与向FlinkML继续贡献算法实现，可以同时进行。用户的使用反馈也有助于更好的改进算法实现。此想法获得了社区的支持，获得了阿里内部的支持，在Flink Forword Asia 2019大会上，Alink正式宣布开源。目前，Alink开源已经四个多月，在这段时间里Alink在开源社区的声望越来越高，Alink在Github上已经有2000多颗Star，400多次fork。杨旭感叹道，“目前为止，我们的开源用户群已经将近1000人，并且已经有多位社区开发者向Alink提交算法code，有几十位社区的Alink用户向我们提出Alink算法bug或者算法改进需求。Alink开发团队也积极和社区互动，共同推进Alink平台的发展。”一方面，Alink团队积极支持社区用使用Alink，帮助数百位社区用户解决他们在使用Alink算法遇到的困难。另一方面，针对社区用户提出的算法bug和算法改进需求，Alink团队第一时间作出响应，对这些bug和改进需求进行排期，并在开发完成后及时开源到社区，解决社区用户的需求。“虽然Alink的开源已经取得了阶段性成果，我们仍然在积极向FlinkML贡献代码”，杨旭最后表示，他希望将更多优秀的机器学习算法贡献给Flink项目，也希望和社区一起努力，共同促进Flink社区机器学习生态的发展和繁荣。【End】推荐阅读半小时训练亿级规模知识图谱，亚马逊AI开源知识图谱嵌入表示框架DGL-KE首次揭秘！大麦如何应对超大规模高性能选座抢票？AI 四巨头 Google、DeepMind、Microsoft、Uber 深度学习框架大比拼马化腾、马云并列成为中国首富；百度回应“将上线电商直播”；.NET 5 Preview 2 发布 | 极客头条程序员职场背锅甩锅指南警惕！新骗术出现：这些虚假二维码生成器已成功盗取 4.6 万美元！“出道” 5 年采用率达 78%，Kubernetes 的成功秘诀是什么？你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/105400810
22			['2020 AI产业图谱启动，勾勒中国AI技术与行业生态_AI科技大本营-CSDN博客']			《2020年国务院政府工作报告》提出，重点支持「两新一重」建设。其中「两新一重」中的第一个「新」，就是新基建，而人工智能是新基建的重要组成部分。新基建首次被纳入政府工作报告后，各大科技厂商纷纷押注，重金投向「新基建」。例如腾讯已经宣布未来五年将投入5000亿，用于新基建的进一步布局，而人工智能将是其重点投入领域。新基建的提出，加速了AI技术向垂直行业和传统产业渗透的进程，「AI全民化」已不再是一句口号，而正在实实在在的变为现实。当刷脸支付成为了新零售的主要付款方式、当无人机灌溉着广袤的农田，当自动化成为生产车间的主力，当医疗影像通过智能算法来进行诊断，当收费站与智能汽车系统完美融合……AI正在重塑着这个世界。2020 AI产业图谱：勾勒中国AI技术与行业生态为了更好地理解AI领域的新基建，及其背后所代表的的众多产业，我们将人工智能产业链划分为三个层次：基础层、技术层、应用层。其中基础层主要包括数据、芯片、算法三个方面，技术层主要是计算机视觉、自然语言处理、知识图谱、机器学习等相关AI核心技术，应用层则多是技术使用者，覆盖了驾驶、安防、金融、医疗、家居、教育等智能学习技术应用的场景。2016年AlphaGo横空出世，2017年中国的AI产业进入发展的快车道，2018年CSND推出AI产业图谱，并吸引了众多企业的参与。一张图谱当然不足以代替整个产业链，但我们希望通过不断完善的图谱，勾勒出蓬勃的中国AI生态。对图谱感兴趣并希望加入图谱的企业，可以点击【阅读原文】或扫描【下方二维码】提交相关信息，并经由CSDN审核之后，加入图谱。寻找榜样的力量：聚焦开源、创新与落地此外，为了寻找出AI生态里的代表性企业，CSDN自2018年起也开启了一年一度的AI评选活动。今年的评选活动在前两届的基础上再度升级，设立了【AI开源贡献奖Top 5】、【AI新锐公司奖Top 10】、【AI优秀案例实践奖Top 30】三大奖项。我们相信，榜样的力量将成为促进AI行业不断发展的重要基石，而CSDN将与这些榜样一起，助力AI时代的「新基建」。欢迎感兴趣的企业扫描下方的【海报二维码】参与评选。活动官网：https://bss.csdn.net/m/topic/ai_selection/index申报地址：http://csdnprogrammer.mikecrm.com/WpA03hJ点击“阅读原文”加入图谱			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106449557
23			["IDEA debug提示Connected to the target VM, address: '127.0.0.1:xxxxx', transport: 'socket'的原因_SunFlowerXT的博客-CSDN博客"]			不知道各位友友们是否遇到这种情况，我遇到很久了，一直没有注意如何解决，都直接是run启动，但是很不方便，有的时候想要断点调试就不行，今天有空，就好好来看了一下。百度了很多方法，使用了也没有效果，跟网上资料说的maven版本不兼容，ctrl+shift+F8 去除勾选什么的没有关系。很明显这里不是显示启动信息的地方，这是显示变量的地方，就是断点进入后变量的展示位置。那么去找你的Console控制台吧一开始这里并没有console控制台；然后点击箭头指向的小图标之后，就出现了console了。 有的console和endpoints是合并的，仔细一点就好了。如果你也出现了这样的问题，大可以去试一试。我的就是因为Console不小心hide了造成的，希望能对你有所帮助。			https://blog.csdn.net/SunFlowerXT/article/details/101274034
24			['产品经理面试——简历填写_Zone.chou-CSDN博客']			前言经过深思熟路后，本渣决定参加放弃研发转投产品了。本科，硕士所学的东西也不能说可惜，这些都会帮助我在产品岗位更进一步。那么在研究与学习之下，我来研究一下产品经理的简历如何投能不当炮灰。（现在写好加深一下印象，也方便一下一年后写简历参加秋招的小周。）思路：PM的面试简历，就是一份产品产品用户：HR，业务主管简历被HR的阅读时间很短，15S左右，且较为粗略。所以就需要满足 dont make me think 的原则，简洁明了，让人抓住重点。用户需求1.满足需求：挖掘相关经历 2.设计功能：改写经历 3.用户体验：呈现那么在实习或者项目经历的时候，需要满足STAR法则situaton：事情发生的情况（目前...的问题）task:任务是什么（为解决..问题）action：采取了行动（分析..后采取..措施）result：取得的成就（优化了百分之..，获得..）简历模块基本信息--（必要）：姓名大写加粗，电话加-，如xxx-xxxx-xxxx，邮箱不要使用qq邮箱，籍贯可以不写。教育信息--（必要）：排名和绩点可以不要工作（实习经历）&项目/社团经历（必要）：STAR所获奖励（可能需要）技能知识（可能需要）：计算机能力，语言自我评价：（可能需要）社交网络账号：（可能需要）有一定贡献的细节：word转换成pdf，命名使用“姓名_学校（学历）_求职岗位_简历”，A4纸面试考察细节：需求分析，交互设计，逻辑思维 ，沟通表达加分能力：项目执行，数据分析。市场分析，管理领导穿着得体，了解公司产品。带足简历资料，模拟面试			https://blog.csdn.net/qq_41536104/article/details/101310465
25			['附代码 | OpenCV实现银行卡号识别，字符识别算法你知多少？_AI科技大本营-CSDN博客']			作者 | 李秋键责编 | Carol头图 | CSDN 付费下载自视觉中国随着计算机视觉在我们生活中的应用越来越广泛，大量的字符识别和提取应用逐渐变得越来越受欢迎，同时也便利了我们的生活。像我们生活中的凭借身份码取快递、超市扫码支付的机器等等。字符识别是模式识别的一个重要应用，首先提取待识别字符的特征；然后对提取出来的特征跟字符模板的特征匹配；最后根据准则判定该字符所属的类别。不同的训练方法，不同的特征提取， 不同的匹配规则，就相应的有不同的字符识别方法，基本上很多就是在这些地方做改进，或者是采用新的规则。但是万变不离其宗。1、模板匹配字符识别算法。模板匹配字符识别算法是图像识别中的经典算法之一，该算法的核心思想是：通过比较待识别字符图像的字符特征和标准模板的字符特征，计算两者之间的相似性，相似性最大的标准模板的字符即为待识别的字符。2、神经网络字符识别算法主要思想：通过神经网络学习大量字符样本，从而得到字符的样本特征。当对待识别的字符进行识别时，神经网络就会将待识别字符的特征和之前得到的样本特征匹配，从而识别出字符。3、支持向量机主要思想：同上，都是先得到样本特征，进行训练，然后再分类。SVM应该算是用的最多的分类方法，一般大多适合于二分类问题，在这里就需要使用多分类器来构造。今天我们就简单的利用OpenCV处理通过提取轮廓和匹配等方式来实现模式匹配的字符识别。效果图如下：实验前的准备首先我们使用的python版本是3.6.5所用到的库有cv2库用来图像处理；Numpy库用来矩阵运算，这里主要用来对图像像素值相关性处理；imutils库可以轻松实现基本图像处理功能，如平移，旋转，调整大小，骨架化和显示Matplotlib图像。程序的搭建1、参考图像的读取和处理：参考图像如下，因为银行卡号主要只有0~9这几个数字，为了方便识别数字，我们直接利用这张图片里的数值作为匹配样式：所以下面我们要做的事很明显，就是要将其中每个数字隔开方便后面匹配。代码如下：#定义了一个字典 FIRST_NUMBER  ，它将第一个数字映射到相应的信用卡类型。FIRST_NUMBER = {    "3": "American Express",    "4": "Visa",    "5": "MasterCard",    "6": "Discover Card"}#参考数字图像，用于匹配#灰度化及二值化ref=cv2.imread("1.png")ref = cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY)ref = cv2.threshold(ref, 10, 255, cv2.THRESH_BINARY_INV)[1]#查找轮廓，从左往右排序refCnts = cv2.findContours(ref.copy(), cv2.RETR_EXTERNAL,    cv2.CHAIN_APPROX_SIMPLE)refCnts = imutils.grab_contours(refCnts)refCnts = contours.sort_contours(refCnts, method="left-to-right")[0]digits = {}#对于其中每一个轮廓进行提循环，i为数字名称,c为轮廓，我们将每个数字0-9（字典键）与第30行的每个roi   图像（字典值）相关联 。for (i,c) in enumerate(refCnts):    (x,y,w,h)=cv2.boundingRect(c)    roi=ref[y:y+h,x:x+w]    roi=cv2.resize(roi,(57,88))    digits[i]=roi#初始化几个结构化内核，构造了两个这样的内核 - 一个矩形和一个正方形。我们将使用矩形的一个用于Top-hat形态运算符，将方形一个用于关闭操作。rectKernel=cv2.getStructuringElement(cv2.MORPH_RECT,(9,3))sqKernel=cv2.getStructuringElement(cv2.MORPH_RECT,(5,5))2、获取数字位置分组：这里需要识别的图片为：我们需要进行的处理包括二值化和Top-hat形态操作，最后通过findContours函数框出位置。其中代码如下：#加载信用卡图像image=cv2.imread("3.jpg")image=imutils.resize(image,width=300)gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)#执行Top-hat形态操作，将结果存储为 tophat,Top-hat操作显示了深色背景下的亮区（即信用卡号）tophat=cv2.morphologyEx(gray,cv2.MORPH_TOPHAT,rectKernel)#计算沿x方向的渐变在计算gradX   数组中每个元素的绝对值之后 ，我们采取一些步骤将值缩放到范围[0-255]（因为图像当前是浮点数据类型）。要做到这一点，我们计算 MINVAL#   和 MAXVAL   的 gradX   （线72），然后由我们的缩放方程上显示  线73（即，最小/最大归一化）。最后一步是将gradX转换   为 uint8   ，其范围为[0-255]gradx=cv2.Sobel(tophat,ddepth=cv2.CV_32F,dx=1,dy=0,ksize=-1)gradx=np.absolute(gradx)(minval,maxval)=(np.min(gradx),np.max(gradx))gradx=(255*((gradx-minval)/(maxval-minval)))gradx=gradx.astype("uint8")#执行gradX 图像的Otsu和二进制阈值，然后是另一个关闭操作,对数字分段gradx=cv2.morphologyEx(gradx,cv2.MORPH_CLOSE,rectKernel)thresh=cv2.threshold(gradx,0,255,cv2.THRESH_BINARY|cv2.THRESH_OTSU)[1]thresh=cv2.morphologyEx(thresh,cv2.MORPH_CLOSE,sqKernel)#找到轮廓并初始化数字分组位置列表cnts=cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)cnts=imutils.grab_contours(cnts)3、切割字符：接着循环遍历轮廓，同时根据每个的宽高比进行过滤，允许我们从信用卡的其他不相关区域修剪数字组位置，然后从左到右对分组进行排序，并初始化信用卡数字列表。部分代码如下：locs = []#循环遍历轮廓，同时根据每个的宽高比进行过滤，允许我们从信用卡的其他不相关区域修剪数字组位置for (i, c) in enumerate(cnts):    # compute the bounding box of the contour, then use the    # bounding box coordinates to derive the aspect ratio    (x, y, w, h) = cv2.boundingRect(c)    ar = w / float(h)    # since credit cards used a fixed size fonts with 4 groups    # of 4 digits, we can prune potential contours based on the    # aspect ratio    if ar > 2.5 and ar < 4.0:        # contours can further be pruned on minimum/maximum width        # and height        if (w > 40 and w < 55) and (h > 10 and h < 20):            # append the bounding box region of the digits group            # to our locations list            locs.append((x, y, w, h))#从左到右对分组进行排序，并初始化信用卡数字列表locs = sorted(locs, key=lambda x:x[0])output = []#遍历四个排序的分组并确定其中的数字,循环的第一个块中，我们在每一侧提取并填充组5个像素（第125行）# ，应用阈值处理（第126和127行），并查找和排序轮廓（第129-135行）。for (i, (gX, gY, gW, gH)) in enumerate(locs):    # initialize the list of group digits    groupOutput = []    # extract the group ROI of 4 digits from the grayscale image,    # then apply thresholding to segment the digits from the    # background of the credit card    group = gray[gY - 5:gY + gH + 5, gX - 5:gX + gW + 5]    group = cv2.threshold(group, 0, 255,        cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]    # detect the contours of each individual digit in the group,    # then sort the digit contours from left to right    digitCnts = cv2.findContours(group.copy(), cv2.RETR_EXTERNAL,        cv2.CHAIN_APPROX_SIMPLE)    digitCnts = imutils.grab_contours(digitCnts)    digitCnts = contours.sort_contours(digitCnts,        method="left-to-right")[0]    # loop over the digit contours    for c in digitCnts:        # compute the bounding box of the individual digit, extract        # the digit, and resize it to have the same fixed size as        # the reference OCR-A images        (x, y, w, h) = cv2.boundingRect(c)        roi = group[y:y + h, x:x + w]        roi = cv2.resize(roi, (57, 88))        # initialize a list of template matching scores        scores = []        # loop over the reference digit name and digit ROI        for (digit, digitROI) in digits.items():            # apply correlation-based template matching, take the            # score, and update the scores list            result = cv2.matchTemplate(roi, digitROI,                cv2.TM_CCOEFF)            (_, score, _, _) = cv2.minMaxLoc(result)            scores.append(score)        # the classification for the digit ROI will be the reference        # digit name with the *largest* template matching score        groupOutput.append(str(np.argmax(scores)))    # draw the digit classifications around the group    cv2.rectangle(image, (gX - 5, gY - 5),        (gX + gW + 5, gY + gH + 5), (0, 0, 255), 2)    cv2.putText(image, "".join(groupOutput), (gX, gY - 15),        cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 255), 2)    # update the output digits list    output.extend(groupOutput)# display the output credit card information to the screenprint("Credit Card Type: {}".format(FIRST_NUMBER[output[0]]))print("Credit Card #: {}".format("".join(output)))cv2.imshow("Image", image)cv2.waitKey(0)到这里，我们整体的程序就搭建完成，下面为我们程序的运行结果：源码地址：链接：https://pan.baidu.com/s/16t7ZK4j1F6yzp2ynVQol0w提取码：k5ra作者简介：李秋键，CSDN博客专家，CSDN达人课作者。硕士在读于中国矿业大学，开发有taptap竞赛获奖等等。推荐阅读全球Python调查报告：Python 2正在消亡，PyCharm比VS Code更受欢迎来了来了！趋势预测算法大PK！10行Python代码能实现什么高端操作?无代码来了，还要程序员吗？没错，你离分布式搜索只差一个Elasticsearch入门！再见，Eclipse | 原力计划区块链共识算法总结 | 原力计划你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106416276
26			['微服务的理想与现实_AI科技大本营-CSDN博客']			来源 | 京东智联云开发者随着云原生微服务的日益火热，很多人都开始对微服务的相关知识内容感兴趣。本篇内容，旨在扫盲（意思是小白可入），希望能对大家有帮助。如有问题，欢迎大家一起讨论，共同学习进步。微服务从哪里来？--- 服务架构的演进史互联网初期， 2G还是个时髦词儿，人们的需求也很朴实，一个静态网站告诉大家我是谁、一个留言板让大家能够与我联系，就能满足信息传播和互相交流的需要。于是码农们给我们提供了这样一套解决方案：界面+业务处理+数据处理，通过一个zip包就可完成所有的事情，这也就是服务架构的单体架构时代。图片为作者原创随着3G的普及，越来越多的人们可以通过PC上网了，此时BBS、门户咨询网站的出现开始吸引着大量观众。当漂亮的交互更能抓人眼球、有趣的信息瞬间引爆千万用户在线围观时，“并发“问题产生了，于是码农们加班奋战，将系统分为前端和后端，通过拆分出可复用的中间件，来提升业务处理能力、解决并发问题，这便是分层架构时代的到来。图片为作者原创后来，互联网进入微博时代，几乎网民都有Blog，打开手机就刷weibo。而此时的分层架构面对更复杂服务要求时，在应用扩展、服务调用、扩容等方面都越发桎梏，于是服务架构走进了面向服务的架构（SOA）时代。SOA网上说的很多，这里列举几个关键词：中心化的服务治理， ESB（企业服务总线）中心化、服务之间通过精确定义的接口进行通讯、耦合度更低、扩展性更高、维护成本较高。图片为作者原创又过了几年，电商掀起了各个时节的线上大促活动，与之伴随而来的还有持续交付、灰度发布、服务限流、容错保护、链路跟踪、日志监控、弹性伸缩等等一大串需求，也还有程序员日益见秃的头顶和度数越来越深的眼镜。当运维压力已经赶不上业务的快速发展时，微服务时代来临。可以这样理解，微服务架构也是SOA架构分布式化的一种实现方式。它的优势在于小而治之、去中心化，但与之对应的问题是，你要管理越来越多的微服务。而如何进行微服务拆分和服务治理，是十分考验能力的试金石。纵观前后，服务架构历次的迭代更新，都是围绕着用户如何节约成本和提升效率，来解决不断出现的新问题，微服务就是服务架构演进史的产物之一。微服务是什么？图片为作者原创微服务最流行的定义是由 Martin Fowler 与 James Lewis 于 2014 年共同提出。引用老爷子们的说法：微服务是架构层的一个概念，通过分解（业务单元），将项目拆解出 n 个单元，互相没有强依赖关系（解耦），自我准备需要的依赖条件，进而达到可以独立运行、独立部署，不再受环境与地点上的限制。微服务架构风格是一种使用一套小服务来开发单个应用的一种方式，每个服务运行在自己的进程中，并使用轻量级机制通信，通常采用HTTP资源API这样轻量的机制来相互通信，这些服务围绕业务功能进行构建，并能够通过自动化部署机制来独立部署，这些服务使用不同编程语言实现，以及不同数据存储技术，并保持最低限度的集中式管理。根据InfoQ发布的2019架构和设计领域趋势报告显示，微服务架构已经走过了盲目追捧阶段，开始逐渐走向成熟，走向切实可落地阶段。图片来源：InfoQ发布的2019架构和设计领域趋势报告如何选？适合的才是最好的在我们选择之前，先来看看有什么能够选的？1、微服务框架的分类目前市场上已经出现的微服务框架非常多，他们各有所长。常见的微服务框架都有哪些呢?如果从常见微服务框架形式分类来看，目前主要分为4类。组件类——用户可以按需加载使用。常见的有Kubernetes、Eureka/Consul/etcd、ZipKin/Jagger等。集成类——集成类的优势在于简化了分布式系统基础设施的开发，提供服务发现注册、配置中心、消息总线、负载均衡、数据监控等内容。常见的有Spring Cloud、Dubbo等。网格类——常见的有Istio、Linkerd、Kong Mesh等。无服务类——目前主要是大厂在使用，常见的有Knative、OpenFaaS、Kubeless、Fission等。如果按照目前的主流生态体系来看，目前有三大生态体系：Spring Cloud家族（https://www.springcloud.cc/）Dubbo家族（http://dubbo.apache.org/en-us/ ）云原生家族（https://www.cncf.io/）这里特提供了官方地址供大家学习，本文不再详细展开讨论，每一个家族都需要熬夜掉一把头发，潜心实践和学习才能掌握。总而言之，微服务的核心是服务治理，而服务治理就需要好的微服务框架，要不然微服务化可能是灾难！但做为用户，选择适合自己实际情况的才是最好的。2、选择适合自己的微服务框架那么我们该如何选择微服务框架呢？依赖于业务特点和技术能力。先选定方向，再研究技术细节。下面关于方向选择的思路供各位参考：如果你的业务模块与服务治理是整合在一起的，依托特定开发语言和开发框架，通过配置来调整规则和策略，依赖业务上线对服务治理进行功能升级，那么你可能比较适合集成方式进行服务治理。你可以在Spring Cloud、Dubbo生态中去选择合适自己的方式。如果你的业务模块与服务治理是分开的，与开发语言无关、与开发框架无关，通过动态调整来配置运行时各种规则和策略，服务治理功能升级独立于业务模块，那么你可能比较适合服务网格的方式进行服务治理。你可以在云原生家族中去选择合适自己的方式，比如尝试下Istio。3、什么时候引入微服务？微服务不是万能的，换句话说，微服务未见得适合于你。1）天时在业务运行初期，如果你是单业务系统架构，如果业务量不大且复杂性不高，如果你追求快速响应、开拓服务、节省成本、提高效率，那么你可能真的用不着微服务。比如你如果只是要用CMS做一套公司网站，那真的可能不用微服务这把牛刀。当随着你的业务进入扩展期，你的系统架构开始走向面向服务架构，业务不断扩大，业务系统复杂性不断提高，但效率在不断下降，那么这个时候你可以开始考虑业务拆分、使用微服务了。2）地利如果要进行微服务改造，还需要具备一定的资源条件，如物理机资源、网络资源。举个例子：假设一个电商平台，现状如图。如果业务框架不那么复杂则可考虑不用微服务架构。而如果需要进行微服务改造，那么至少需要准备规划好如下资源：硬件资源：主机/容器、数据库软件资源：注册中心、拆分的服务、负载均衡、网关、缓存、监控软件人力资源：至少需要架构师构建微服务、前端、后端、测试，其中运维的角色可以由研发+微服务平台 代替。3）人和如果要享受微服务带来的优势，就需要接受微服务带来的挑战。比如：虽然微服务的服务边界限定，每个团队可以独立维护演进自己的服务，但是当服务扩展到几十个甚至上百个后，就需要考虑分布式带来的复杂性。如果说不同服务可独立部署、独立扩展，那么维护不同版本和版本兼容就是需要面对的挑战。如果说不同的服务可以采用不同的技术栈，只需按照约定好的通信协议即可完成交互，那么服务之间的认证/鉴权/证书管理，共享数据与分离数据后如何保持一致性等一系列复杂的问题等，就是需要面对的挑战。……总之，使用微服务也是需要天时地利人和的，使用的过程也不要一蹴而就，服务治理是巨大的挑战。你的业务适合微服务吗？我收集整理了几个最简单的问题以供快速自测：微服务化的实施方法简单来说4步走：其实每一步都有可能会有“坑”，这里面都是学问，都可独立成章。因此，本文不详细展开，以后再写相关内容文章详谈。但是，Demo可以快速帮我们一探究竟，这里我选择了京东智联云的微服务平台来做体验。为啥是云上公有云产品？因为微服务的体系太复杂庞大了，如果你自建，1个人搞不定1个团队的工作内容。还因为在“云”上是托管服务，少运维，所见即所得组件多，开源、多可用区、上手够简单够快、学习和使用成本低呀。简单总结下我的学习路径：1、体验“云”上高可用注册中心如果您已有成熟的微服务项目，目前正在上云过程中，希望享受 “云” 带来的注册中心多可用区部署、最大程度的保证集群高可用的能力。那么可以直接使用微服务平台的命名空间注册中心功能。目前JDSF支持的微服务框架包含：SpringCloud、Dubbo、JSF。使用大致步骤如下：入门示例参见：https://docs.jdcloud.com/cn/jd-distributed-service-framework/basic-example 。2、体验调用链分析服务便捷定位性能瓶颈如果您已有成熟的微服务项目，目前正在上云过程中，希望通过 “云” 计算能力，更加便捷全面掌握服务间调用关系、精准发现系统的瓶颈和隐患的服务、减少运维投入，那么可以直接使用微服务平台的调用链分析服务。目前支持协议：ZipKin、Thrift、HTTP。入门示例参见：https://docs.jdcloud.com/cn/jd-distributed-service-framework/basic-example 。3、体验应用部署如果您已经使用云上JDSF服务，还可以使用JDSF平台提供的成熟、灵活的部署方案发布微服务应用。目前应用类型支持：云主机应用、K8S应用。入门示例参见：https://docs.jdcloud.com/cn/jd-distributed-service-framework/demo-deploy-k8s4、体验开放服务给客户使用如果需要开放服务给你的用户使用，假如你已经使用了微服务平台的注册中心服务，微服务网关可以在调用时自动完成服务发现、负载均衡，无需再使用其他负载均衡或网关服务。假如你的服务已经通过其他方式在内网发布到了负载均衡服务上，也可以通过微服务网关实现与API网关的无缝对接，避免公网暴露，不再需要申请公网IP和产生公网流量费用。入门示例参见：https://docs.jdcloud.com/cn/jd-distributed-service-framework/gw_vpc怎么样，微服务看上去是不是又没有那么抽象、那么难、那么抓狂了呢？更多内容下次再分解。推荐阅读全球Python调查报告：Python 2正在消亡，PyCharm比VS Code更受欢迎来了来了！趋势预测算法大PK！10行Python代码能实现什么高端操作?无代码来了，还要程序员吗？没错，你离分布式搜索只差一个Elasticsearch入门！再见，Eclipse | 原力计划区块链共识算法总结 | 原力计划你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106416277
27			['机器学习模型的超参数优化 | 原力计划_AI科技大本营-CSDN博客']			作者 | deephub责编 | 王晓曼出品 | CSDN博客头图 | CSDN付费下载自东方IC引言模型优化是机器学习算法实现中最困难的挑战之一。机器学习和深度学习理论的所有分支都致力于模型的优化。机器学习中的超参数优化旨在寻找使得机器学习算法在验证数据集上表现性能最佳的超参数。超参数与一般模型参数不同，超参数是在训练前提前设置的。举例来说，随机森林算法中树的数量就是一个超参数，而神经网络中的权值则不是超参数。其它超参数有：神经网络训练中的学习率支持向量机中的 c cc 参数和 γ \gammaγ 参数k 近邻算法中的 k kk 参数……超参数优化找到一组超参数，这些超参数返回一个优化模型，该模型减少了预定义的损失函数，进而提高了给定独立数据的预测或者分类精度。分类算法中的超参数超参数优化方法超参数的设置对于模型性能有着直接影响，其重要性不言而喻。为了最大化模型性能，了解如何优化超参数至关重要。接下来介绍了几种常用的超参数优化方法。1.手动调参很多情况下，工程师们依靠试错法手动对超参数进行调参优化，有经验的工程师能够很大程度上判断超参数如何进行设置能够获得更高的模型准确性。但是，这一方法依赖大量的经验，并且比较耗时，因此发展出了许多自动化超参数优化方法。2. 网格化寻优（Grid Search）网格化寻优可以说是最基本的超参数优化方法。使用这种技术，我们只需为所有超参数的可能构建独立的模型，评估每个模型的性能，并选择产生最佳结果的模型和超参数。网格化寻优方法：以一个典型的核函数为 RBF 的 SVM 分类模型为例，其至少有两个超参数需要优化——正则化常数 c cc 和 核函数参数 γ \gammaγ。这两个超参数都是连续的，需要执行网格化寻优为每个超参数选择合理取值。假设 c∈10,100,1000,γ∈0.1,0.2,0.5,1.0c\in {10,100,1000}, \gamma \in {0.1,0.2,0.5,1.0}c∈10,100,1000,γ∈0.1,0.2,0.5,1.0。那么网格化寻优方法将对每一对( c cc ,γ \gammaγ)赋值后的 SVM 模型进行训练，并在验证集上分别评估它们的性能（或者在训练集内进行 cross-validation）。最终，网格化寻优方法返回在评估过程中得分最高的模型及其超参数。通过以下代码，可以实现上述方法：首先，通过 sklearn 库调用 GridSearchCV 。from sklearn.datasets import load_irisfrom sklearn.svm import SVCiris = load_iris()svc = SVR()from sklearn.model_selection import GridSearchCVfrom sklearn.svm import SVRgrid = GridSearchCV(        estimator=SVR(kernel='rbf'),        param_grid={            'C': [0.1, 1, 100, 1000],            'epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],            'gamma': [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]        },        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)然后拟合网格:grid.fit(X,y)输出结果:#print the best score throughout the grid searchprint grid.best_score_#print the best parameter used for the highest score of the model.print grid.best_param_网格化寻优的一个缺点是，当涉及到多个超参数时，计算数量呈指数增长。并且这一方法并不能保证搜索会找到完美的超参数值。随机寻优（Random Search）通常并不是所有的超参数都有同样的重要性，某些超参数可能作用更显著。而随机寻优方法相对于网格化寻优方法能够更准确地确定某些重要的超参数的最佳值。随机寻优方法：随机寻优方法在超参数网格的基础上选择随机的组合来进行模型训练。可以控制组合的数量，基于时间和计算资源的情况，选择合理的计算次数。这一方法可以通过调用 sklearn 库中的  RandomizedSearchCV 函数来实现。尽管 RandomizedSearchCV 的结果可能不如 GridSearchCV 准确，但它令人意外地经常选择出最好的结果，而且只花费 GridSearchCV 所需时间的一小部分。给定相同的资源，RandomizedSearchCV 甚至可以优于的结果可能不如GridSearchCV准确。当使用连续参数时，两者的差别如下图所示。网格化寻优 VS 随机寻优随机寻优方法找到最优参数的机会相对更高，但是这一方法适用于低维数据的情况，可以在较少迭代次数的情况下找到正确的参数集合，并且花费的时间较少。通过以下代码，可以实现上述方法：首先，通过 sklearn 库调用 RandomizedSearchCV 。from sklearn.datasets import load_irisfrom sklearn.ensemble import RandomForestRegressoriris = load_iris()rf = RandomForestRegressor(random_state = 42)from sklearn.model_selection import RandomizedSearchCVrandom_grid = {'n_estimators': n_estimators,               'max_features': max_features,               'max_depth': max_depth,               'min_samples_split': min_samples_split,               'min_samples_leaf': min_samples_leaf,               'bootstrap': bootstrap}rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model然后进行计算：rf_random.fit(X,y)输出结果：#print the best score throughout the grid searchprint rf_random.best_score_#print the best parameter used for the highest score of the model.print rf_random.best_param_Output:{'bootstrap': True, 'max_depth': 70, 'max_features': 'auto', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 400}贝叶斯优化方法（Bayesian Optimization）前面两种方法能够针对单独超参数组合模型进行训练，并评估各自的性能。每个模型都是独立的，因此很易于进行并行计算。但是每个模型都是独立的，也导致模型之间不具有指导意义，前一模型的计算结果并不能影响后一模型的超参数选择。而贝叶斯优化方法（顺序优化方法的一种，sequential model-besed optimization, SMBO）则可以借鉴已有的结果进而影响后续的模型超参数选择。这也限制了模型训练评估的计算次数，因为只有有望提高模型性能的超参数组合才会被进行计算。贝叶斯优化是通过构造一个函数的后验分布（高斯过程）来工作的，该后验分布最好地描述了要优化的函数。随着观测次数的增加，后验分布得到改善，算法更加确定参数空间中哪些区域值得探索，哪些区域不值得探索。当反复迭代时，算法会在考虑到它对目标函数的了解的情况下，平衡它的探索和开发需求。在每个步骤中，高斯过程被拟合到已知的样本（先前探索的点），后验分布与探索策略（例如UCB（上置信限，upper confidence bound）或EI（预期改善, expected improvement））被用于确定下一个应该探索的点。通过贝叶斯优化方法，可以更高效的探索超参数变量空间，降低优化时间。基于梯度的优化方法（Gradient-based Optimization）基于梯度的优化方法经常被用于神经网络模型中，主要计算超参数的梯度，并且通过梯度下降算法进行优化。这一方法的应用场景并不广泛，其局限性主要在于：超参数优化通常不是一个平滑的过程超参数优化往往具有非凸的性质进化寻优（Evolutionary Optimization）进化寻优方法的思想来源于生物学概念，由于自然进化是不断变化的环境中发生的一个动态过程，因此适用于超参数寻优问题，因为超参数寻优也是一个动态过程。进化算法经常被用来寻找其他技术不易求解的近似解。优化问题往往没有一个精确的解决方案，因为它可能太耗时并且计算资源占用很大。在这种情况下，进化算法通常可以用来寻找一个足够的近似最优解。进化算法的一个优点是，它们可以产生出不受人类误解或偏见影响的解决方案。作为一个一般性的经验法则，任何时候想要优化调整超参数，优先考虑网格化寻优方法和随机寻优方法！总结在本文中，我们了解到为超参数找到正确的值可能是一项令人沮丧的任务，并可能导致机器学习模型的欠拟合或过拟合。我们看到了如何通过使用网格化寻优、随机寻优和其他算法来克服这一障碍。版权声明：本文为CSDN博主「deephub」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/m0_46510245/article/details/1059109076月2日20:00，CSDN 创始人&董事长、极客帮创投创始合伙人蒋涛携手全球顶级开源基金会主席、董事，聚焦中国开源现状，直面开发者在开源技术、商业上的难题，你绝不可错过的开源巅峰对谈！立即免费围观：推荐阅读追忆童年，教你用Python画出儿时卡通人物如何用NLP辅助投资分析？三大海外机构落地案例详解What?! Python一行代码，能玩这么多童年的游戏？我只是追个直播，结果被拉进大咖们的群面对面群聊……借助大数据进行社交媒体营销，企业们得这么玩！力挺比特币的世界第2交易员：仅次于索罗斯，连续25年无亏损你点的每个“在看”，我都认真当成了喜欢			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106512309
28			['idea新建spring boot项目使用maven引入依赖失败，pom.xml文件中project标签报错_weixin_43665271的博客-CSDN博客']			笔者之前准备做一个MongoTemplate的小demo，建完项目后发现pom.xml一直报错，也没有jar包导入。    网上有病乱投医近一天未果，心态微崩，很急很气（之前没有遇到过）。    按照网上的帖子什么清楚缓存重启啦、重新导入啊，皆无功而返。    后来想到可能是网络原因，jar包没下完整。然后找到本地仓库，果然很多.lastupdate文件。    将其删掉后，reimport依赖，令人抓狂的红线终于消失。    ps：本地仓库可以通过（File-settings-maven)处local repository查看。			https://blog.csdn.net/weixin_43665271/article/details/101274533
29			['项目章程_软小鬼的博客-CSDN博客']			经过两个月的学习，终于到了考完PMP的这一天。我决定要说说这两个月的考试心得。学习PMP真的不是一件容易的事情啊，要看一本巨厚的PMbok指南，还得看汪博士解读PMP考试，还得看一本上课的讲义。这些书都必须看两遍以上。我相信有考过PMP认证的，都知道前面的两本书。当你真的认真看完这两本书你就知道了，什么叫成本管理就，范围管理，风险管理，进度管理。什么叫变更管理控制流程。产品范围与项目范围的区别是什么。这些都会一清二楚了。这两个月大概是我今年学习都东西最多的时候了。这两个月，篮球没打了，吉他没弹了。就专注学习了。不过有舍就有得，我也学习到新的知识了。也是很不错的，人生就是要不断的改变自己，提升自己。项目经理除了书本里面学习的技术知识以外，还有一个很重要的技能。就是交际能力，项目经理有90%的时间都在沟通。如果交际能力不强，那做起项目管理就很吃力。其实我觉得很多事情都可以把它当成一个项目来做，只是项目有简单和复杂之分！是项目的话，就会用到项目管理的技能或者知识。开始项目的第一步就是制定项目章程。其实制定项目章程不是项目经理的工作，一般都是项目的发起人去编写制作的。项目经理也没有权利去制定项目章程，但是项目经理可以协助发起人制定项目章程。在制定项目章程的过程中，项目经理也可以更能了解为什么要做这个项目，清楚这个项目的成功率有多少。项目章程很重要的，对项目经理来讲项目章程可以让他顺利的开展后期的工作。因为项目章程的主要作用就是确立项目的合法地位，明确项目的高层级目标和授权项目经理利用组织资源执行项目。如果说在公司里项目没有项目章程的话，这个项目执行起来会有很多阻力。部门之间的不支持，不合作。没有资源执行项目，就单单这个就让项目经理很难受了。所以结论就是，如果公司真的要开始一个新的项目。项目经理一定要求有项目章程，因为这个你工作顺利的一个保证啊。不管项目是复杂还是简单的都需要，简单的可以是几页，复杂也可以是上百页。要制定项目章程就要明白这分文件需要包含什么样的内容。项目章程包含的内容4项3高3总3人，这个是比较好记忆的。只要你的项目正常包含了以上内容，就说明你做了一份比较完整的项目章程了了。4项：项目目的，可测量的项目目标与成功标准，项目的审批要求，项目的退出标准。3高：高层级需求，高层级描述与边界定义，可交付成果。3总：总体里程碑计划，总体风险，总体预算。3人：项目相关方，发起人，授权的项目经理。当把这些完整写出来后，然后分发给项目的相关方，再来一个项目启动会，获取发起人的签字与任命你作为项目经理去开展这个项目。这就是一个比较完美的开头了。其实生活很多事情皆为项目，并不是每件事情都要写一个文档，要签字要授权。我觉得更多的是一种思维的方式，当你需要做一件事情时需要想一下这个事情的目标是什么？成功的标准是什么？跟这个事情有关的人员有哪些？还有这个事情做起来风险大不大？想明白这些事情，做起来成功率就会大很多了。项目管理是一个技术，更应该是一个思维模式。项目管理知识不应该只能在公司项目里，应该更多的融入生活中的思维模式中。大概这样，我觉得才是学到了PMP吧。我是一名Android开发工程师，向往成为项目经理。加油！			https://blog.csdn.net/Ruan_Number3/article/details/100668494
30			['人工智能和机器学习之间的差异及其重要性，一定要分清楚！_AI科技大本营-CSDN博客']			作者 | Rana Rajut译者 | 天道酬勤，责编 | Carol封图 |  CSDN 下载自视觉中国人工智能和机器学习技术正在彻底改变世界，使世界更加先进，但有些人对这两个术语的真正含义感到困惑。有时，在其他情况下它们用作同义词；它们被用作独立或并行的进展。但是，如果你想以有效和有用的方式使用这两者，必须找到两者之间的区别。如果你也是对这两个词的含义、用途和优势感到困惑的人之一，下面我们将分享人工智能和机器学习之间的关键区别。我们来看一下：什么是机器学习?它是人工智能的一个分支，通过研究计算机算法，让计算机程序通过经验自动改进。例如，如果你向任何机器学习模型提供你喜欢的歌曲列表，以及诸如舞蹈，乐器或节奏等音频静态信息，它将自动执行并生成推荐系统，向你推荐你将来喜欢的druckkings mobile的音乐。这种类型的机器学习称为监督学习，其算法能够对目标预测输出和输入特征之间的关系和依赖关系进行建模，我们可以通过这些关系预测新数据的输出值。机器学习的另一种类型是无监督学习，这是用于模式检测和描述建模的一系列机器学习算法。什么是人工智能?除了机器学习之外，人工智能是完全广泛的，而且范围也有所不同。您可以使用“Artificial”一词来理解，它指的是人为的东西，即非自然的事物，而“Intelligence”指的是理解和思考的能力。大多数人认为人工智能是一个系统，这是不正确的。它不是一个系统，而是在系统中实现了人工智能。你可以用其他定义来理解人工智能的含义，例如，它是一项对计算机进行训练，让它们完成人类目前可以做得更好的事情的研究。因此，我们可以说人工智能是一种智能，我们有机会为机器人添加人类所拥有的的所有能力。  人工智能的目的是增加成功的机会，不是提高准确性，模拟自然智能来解决复杂问题，它作为一个智能工作的计算机程序。结论现在你知道了人工智能和机器学习的主要区别，我们可以说，机器学习就是通过经验来寻找它学习的模式，而人工智能是利用经验来获取知识和技能，并将这些知识应用于新的环境。之后，为了更好地利用人工智能，许多组织都试图与人工智能分开。原文链接：https://hackernoon.com/differences-between-ai-and-machine-learning-and-why-it-matters-jk4w3vg7本文为 AI 科技大本营翻译，转载请注明出处。推荐阅读全网唯一秃头数据集：20万张人像，网罗各类秃头Uber 前无人驾驶工程师告诉你，国内无人驾驶之路还要走多久？如何用NLP辅助投资分析？三大海外机构落地案例详解AI 终极问题：我们的大脑是一台超级计算机吗？借助大数据进行社交媒体营销，企业们得这么玩！力挺比特币的世界第2交易员：仅次于索罗斯，连续25年无亏损你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106595809
31			['平安科技王健宗：所有AI前沿技术，都能在联邦学习中大展身手_AI科技大本营-CSDN博客']			「AI 技术生态论」 人物访谈栏目是 CSDN 发起的百万人学 AI 倡议下的重要组成部分。通过对 AI 生态顶级大咖、创业者、行业 KOL 的访谈，反映其对于行业的思考、未来趋势判断、技术实践，以及成长经历。本文为 「AI 技术生态论」系列访谈第二十七期，CSDN 邀请到平安科技副总工程师、联邦学习技术部总经理王健宗，来详细讲解关于联邦学习，我们必须要了解的事实。百万人学 AI 你也有份！今日起，点击阅读原文报名「2020 AI开发者万人大会」，使用优惠码“AIP211”，即可免费获得价值299元的大会在线直播门票一张。限量100张，先到先得。作者 | 夕颜出品 | AI科技大本营（ID: rgznai100）今天，我们来聊聊联邦学习（Federated Learning）。人工智能和大数据领域的人对于这个新兴词汇一定不陌生，但关于这个连名字都有多种叫法的技术（联邦学习、联合学习、联盟学习......）究竟是如何实现的，很多人只是一知半解。风头正盛的联邦学习究竟是什么？简单来说，联邦学习作为分布式的机器学习范式，最大的特点是可以让多个参与方进行 AI 协同。本质上来说，联邦学习的目标是为了有效解决“数据孤岛”问题，让参与方在不共享数据的基础上联合建模，从技术上打破数据孤岛，实现AI 协作。自从谷歌在 2016 年提出了针对手机终端的联邦学习，这个概念开始火爆起来，并被视为下一代人工智能协同算法和协作网络的基础。平安科技提出“联邦智能”的架构，将安全通信、层级加密、可信计算、可视化等真正实现保护用户隐私数据的完整系统囊括进来，联邦学习只是其中一个技术环节。虽然联邦学习技术更新迭代，也有了不少实践解决方案，但是在实际落地中，在保护数据隐私的前提下进行 AI 协同，无论是底层技术还是整个部署环节，还有大量的挑战需要克服。为了更加深入了解联邦学习，CSDN 邀请到平安科技副总工程师、联邦学习技术部总经理王健宗，从他个人踏上联邦学习技术和应用研究之路的个人经历开始，到在其带领下构建的自动化机器学习平台“奥卡姆”与联邦智能平台“蜂巢”的技术解析与应用实践，一窥这项技术在信息爆炸的新时代下，到底已经走到了哪一步。从云 AI 转向联邦学习，出于对技术的发展趋势预判王健宗就读于华中科技大学计算机学院计算机系统结构专业，是个典型的拿公派奖学金的“别人家孩子”。2009 年，王健宗被国家公派到美国莱斯大学联合培养博士，当时正值云计算兴起，他参与了莱斯大学与亚马逊公司的云计算服务优化的合作项目，并在读博期间提出了“云 AI”的技术方向，完成了关于云服务质量方向的博士论文。联合培养博士完成后，王健宗当时收到了一些美国的公司和学校的 Offer，但是考虑到国内广阔的应用场景、海量的数据，王健宗毅然决然回国，并加盟了网易公司，从零开始参与搭建网易大数据平台。在从事若干年大数据研发后，王健宗开始思考一个问题——这些数据如何与应用场景相结合？他顺其自然想到了若干年前在美国所提出的“云 AI”方向，从技术路径上讲，云计算、大数据之后，必然走向人工智能。带着对 AI 的前景预判，王健宗再次前往美国，在美国佛罗里达大学，师从人工智能国际知名学者李晓林教授，从事人工智能博士后研究工作。在云计算和人工智能领域深耕数年，王健宗把主要的精力用在分布式人工智能领域，联邦学习算是多年来他一直在做和想做的事。从美国完成博士后项目之后，他回国加入平安科技，专注于金融人工智能和联邦智能领域的研发工作，带领团队自研了自动化机器学习平台“奥卡姆”，以及联邦智能平台“蜂巢”。揭秘联邦学习平台“蜂巢”AutoML 是机器学习至关重要且有潜力的技术，尤其是与联邦学习的结合更是有着无限广阔的前景。但是今天，我们将重点在联邦学习上。王健宗说到，联邦学习想要解决的问题十分明确——就是数据孤岛，这也是它目前主要的落地场景。“蜂巢”的技术架构他介绍到，蜂巢平台的技术框架，是支持联邦智能原生的。在数据部落中，“蜂巢”包含几大功能模块，包括数据预处理、数据特征化、数据质量的评估等。该平台支持传统的统计机器学习和深度学习的模型，如逻辑回归、线性回归、树模型、CNN/RNN等。在整个模型训练过程中，对梯度进行非对称加密，整合梯度和参数优化、更新模型。在联邦推理这一过程中，“蜂巢”会把原始的传输的数据进行加密，最终实现推理结果。在技术研发工程中，他们不仅需要研发有效的分布式机器学习算法，更重要的是如何更好地保障用户数据安全，在此基础上需要开发可靠的加密方法和有效的联邦学习模式。因此，根据在实际应用场景中用户的反馈，例如一些联邦学习算法中涉及大量矩阵大数运算，其通过不断尝试和实验优化矩阵大数运算算子，在密态下矩阵大数运算的效率上有了很大的提升。这不禁让人好奇，在平安科技内部，“蜂巢”的背后是怎样一支团队呢？从王健宗的口中 CSDN 得知，这支团队是由平安集团首席科学家肖京博士指导，由他本人带领的业内联邦学习专属团队，主要的目标是推动 AutoML、联邦学习、AI翻译以及深度图领域的生态发展，探索行业应用与前沿 AI 技术进行深度、自动化融合的方式，近期在多项AI比赛榜单名列第一的自动化机器学习平台“奥卡姆”就也出自这支团队之手。“蜂巢”作为平安科技的主要联邦学习平台，在底层技术和设计上有何独特之处？王健宗介绍，如何打造和实现企业级的联邦智能平台是平安科技的目标，因此，“蜂巢”从最初的架构设计上就考虑到了在平安集团内各个专业子公司之间就存在着很多数据壁垒，金融行业对数据隐私的保护和监管要求是非常严格，企业级的联邦智能平台就一定要满足稳定、安全、合规的要求。为说明这一点，王健宗举了一个例子。“国内金融机构中很常用的加密方式是国密算法，很多的公司对于任何信息的传输和加密都要求采用国密算法，这与我们在业界常见差分隐私和同态加密都不相同，而蜂巢平台能充分支持了国密SM2、国密SM4、混淆电路、差分隐私和同态加密等不同的加密方式，以满足实际企业业务场景的不同需求。”另外， 蜂巢平台采用了完全自主研发的梯度处理方法，可以做到真正适用于企业之间不同的应用场景，通过更加高效、更加健全和更加稳定更新机制，从而保障参与各方能够实现最高效的建模流程。在联邦学习技术研发迭代期间，王健宗和团队总结出了构建联邦学习平台的几个要点，在这里分享给大家参考：1）如何根据不同业务场景改造联邦学习算法？改造联邦学习算法的关键技术之一就是对各方本地计算得到的参数进行联邦聚合，针对不同的业务场景需要选择不同的聚合方法：例如在数据样本量较大、对性能要求较高的情况下，平安科技提供了FedAvg 方法，能够在保证性能的前提下极大程度地满足业务基本需求；针对小样本的联邦学习，自研了FedSmart 算子，能够更好地优化参数，提升模型效果。除此之外，还根据其他业务场景定制化研发了一些聚合算子。2）如何灵活地实现加密功能？保障数据安全是联邦学习技术的核心，针对不同的性能要求，平安联邦学习平台提供了不同等级的加密模式：对于加密要求严格的业务方，提供了国密加密的加密模式，除此之外，还支持信道加密模式等，以适应更多的业务场景。3）如何提升联邦建模的效率？联邦学习技术的落地需要考虑耗时效率问题，多方计算、加密传输等方面都会增加整体的耗时。针对该问题，平安联邦智能平台设计了大量矩阵大数运算算子用于实现各计算模块，对加密算子和数据结构也进行了优化，同时使用了团队自主研发的新网络编码技术，使其能够更好地支持大批量数据的运算，在不影响模型效果的前提下尽可能地提高建模效率。“蜂巢”支持哪些算法和训练模型？图源：视觉中国“蜂巢”支持机器学习、深度学习等多种算法，结合平安自研底层硬件加速技术解决方案，对比竞品速度提升50%，具体到算法和 AI 模型训练，以及自研底层硬件技术解决方案，平安是如何做到的？据王健宗介绍，首先，在蜂巢联邦学习平台的底层的算法设计上分为四个不同领域和方向。第一部分是基础的联邦学习算法，包括常见的逻辑回归、各类树模型和Boosting算法，以及CNN、RNN等深度学习网络，支持TensorFlow和PyTorch等各种主流框架等，充分兼容不同的建模场景，这些是蜂巢联邦学习平台的核心基础。第二个部分是算子层的深度支持和设计，比如，从底层设计上支持图计算算子，基于Gather-Apply-Scatter的结构抽象高层次算法支撑库，实现高效的信息收集、运算和全局更新的处理，使得蜂巢平台的联邦图计算算法有非常好的时效性表现。第三个部分是异构计算，目前联邦学习算法的性能受限于加密和通信，效率表现往往不够好，对此蜂巢平台用GPU等异构计算芯片来加速联邦学习的加密和通信过程，再加上结合前面提到的算子层优化，从而达到了提速50%的效果，这也是蜂巢在深入实际应用场景中，解决企业间联邦学习建模的痛点之一。最后一个部分是安全加密的部分，举个例子，在实际的建模和推理过程中，重要的模型参数、每个用户本地的数据等关键信息都是存放在安全容器中的，每一次访问都需要经过安全审计和加密，从而可以达到很好的隐私和安全保护效果。联邦学习能与机器学习算法结合，还有哪些新可能？联邦学习与机器学习两者的结合是近年来的研究热点。对此，王健宗介绍，联邦学习除了可以和经典机器学习算法结合应用在分类、预测等场景，在一些细分领域也有很好的应用场景。比如，在推荐系统中可以与协同过滤技术相结合，多方基于矩阵分解（Matrix Factorization）技术进行联合推荐；在医疗健康领域，多方可以通过深度学习模型例如U-Net、ResNet等进行医学成像模型的联合训练以提高模型准确度；在机器翻译领域，多方在训练语料对不出本地的前提下进行联合建模，最大化翻译模型的准确性；在OCR领域，联邦学习同样可以通过共享模型参数，充分利用他方的训练样本信息来弥补己方在一样识别场景中数据匮乏的不足，提高字符识别准确度。此外，王健宗也提到也可以积极探索联邦学习与 AutoML、GNN 等领域的技术结合与应用。蜂巢联邦学习平台在进行联邦学习建模的过程中支持多种不同的自动化调参方式，可以更加高效地找到最佳的模型参数以达到更好的效果。对于图神经网络技术，联邦学习也同样可以通过结合图结构数据的特征，增加对图卷积等算子的支持和优化，从而实现更加丰富应用场景。正如他所说，联邦智能之于联邦学习，就像是人工智能之于深度学习，所有人工智能的前沿技术，都可以在联邦智能的研究和发展中大展身手。从原始数据的传输上来说，联邦学习减少了原始数据传输至中心服务器的通信开销，但是由于大量的模型训练交互，增加了交互通信成本。加密是必不可少的一环，但加密本身往往会影响联邦学习的效率。在实际的工程中，需要针对不同的应用场景，找到“高效”与“可用”之间的平衡。图源：视觉中国同时，联邦学习也有许多 IoT 应用场景，“现在进入 5G 时代，我们可以积极思考 5G 能够给联邦学习的通信带来什么便利之处，使联邦学习的能力可以赋能普惠 AI。未来，联邦学习与量子通信的结合也是我们很看好的一个方向，相信可以给联邦学习带来质的提升。基于传统的网络编码的思路，我们可以在联邦学习多方通信的过程中通过引入中间节点，分别用于接收和转发经过线性或函数加密的参数信息，通过网络编码通信框架实现在每一个信道上传输的参数都不可读，而在接收端有效解码的效果，”王健宗说道。联邦学习底层技术是否成熟？当前，联邦学习底层技术是否成熟？目前存在哪些短板？相信这些问题是大家关心且有望找出突破口的地方。王健宗认为，目前联邦学习底层技术相对来说日渐成熟，目前的短板则是在于计算算力以及带有加密的通信方面，虽然英特尔 SGX，ARM 的 TrustZone 可以支持部分联邦学习的场景，但目前还没有联邦学习专用芯片，联邦学习也没有统一的业内标准和相关协议。产业落地难点在哪？当前，平安科技的联邦智能平台定位是服务于营销、获客、定价、风控、智慧城市和智慧医疗，“蜂巢”能够提供智慧金融、智慧城市、智慧医疗商用级的一站式解决方案，并研发了具备联邦智能能力的联邦机器人，以机器人为服务终端进行数据采集与联合建模，完成金融领域下的客户识别与定制化服务。我们都知道，本质上来说，联邦学习的目标就是解决“数据孤岛”问题，以及在保障数据隐私和安全的前提下实现人工智能。但不得不提的是，在实际落地中，这仍然是一大挑战，举一个很简单的例子，在联邦学习中，在不共享数据的前提下协同建模，有一个经常被大家利用的方法就是梯度共享，但不幸的是，梯度共享的方法在有限条件下可以被成功攻破。这些试图保护数据隐私的学习模型被攻破，未来还会有效吗？这次事件暴露出的联邦学习的隐私安全性问题该怎么保障让人心生疑虑，也为未来技术提出来更高的要求。对此，王健宗解释到，以平安科技为例，在做联邦学习时，他们对隐私安全性有不同层级的设定。仅依赖于梯度共享只能解决联邦学习科研层面的问题。在实际的工程中，平安科技做了很多工作保护梯度共享机制下的联邦学习建模，比如，在传输和计算运用了同态加密的梯度，不仅要保证底层数据的安全性和隐私性，同时对梯度信息也要进行保护。虽然有应对的方法，但涉及到数据隐私，类似问题的存在仍不能掉以轻心。未来趋势对于联邦学习的未来发展，王健宗还有哪些观点与思考？他认为，联邦学习的关注度很高，这说明市场对联邦学习的需求是非常旺盛的。在金融领域、智慧城市、智能家居、车联网等拓展领域上都能看到一些公司在布局联邦智能。他希望，大家无论是在训练、推理，还是数据部落的构建、使用方面都能有联邦智能的理念和意识。平安科技未来的发展方向是打造完整的联邦智能生态，在联合建模的基础上，完善算法选择的多样化，让用户自己定义联邦学习的语言与参数指标，利用平台配套算子打造建立用户自己的定制化模型。希望未来不仅在联邦学习的应用上做到全面布局，在联邦学习的可扩展性上也可以进一步发展。另外一点，制定统一的联邦学习标准也是推动这项技术向前发展的重要环节，虽然目前还没有统一的标准出来，但相关工作已经在推进之中，包括平安科技在联合多家企业和机构编写和发布《联邦学习白皮书 v2.0》，就是一次不小的进步。“因为这是一个重要的新技术方向，平安会努力一直在这个方向占据领先地位，力争做行业标杆。因为平安天然具备丰富的业务场景，所以我们对标准化工作的贡献不仅是理论层面、工程层面，更是注入了我们长时间以来对业务场景、对 AI 应用落地的理解和经验。我希望大家能够共同打造联邦学习的生态，让各行各业能充分发挥其价值，使更多的垂直行业能够落地。”王健宗对于联邦学习的未来充满信心。【END】更多精彩推荐  ☞认知智能再突破，阿里 18 篇论文入选 AI 顶会 KDD☞年仅 5 岁的 Rust 如何成为最受欢迎的编程语言？☞AI 看脸算命，3 万张自拍揭露：颜值即命？☞避坑！使用 Kubernetes 最易犯的 10 个错误☞必读！53个Python经典面试题详解☞赠书 | 1月以来 Tether 增发47亿 USDT，美元都去哪儿了？你点的每个“在看”，我都认真当成了喜欢			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106393759
32			['Java异常 | Intellij IDEA 2019.2.2配置Maven3.6.2打开Maven项目出现 Unable to import Maven project_有时有味的博客-CSDN博客']			1. 异常现象从 Intellij IDEA 2017.1 版本升级到当前最新版本 Intellij IDEA 2019.2.2 之后，打开原有的项目时出现异常：14:42 Unable to import Maven project   See logs for details   Show Log in ExplorerIntellij IDEA 异常日志：2019-09-19 14:54:44,349 [2556395]  ERROR -      #org.jetbrains.idea.maven - IntelliJ IDEA 2019.2.2  Build #IU-192.6603.28 2019-09-19 14:54:44,349 [2556395]  ERROR -      #org.jetbrains.idea.maven - JDK: 11.0.3; VM: OpenJDK 64-Bit Server VM; Vendor: JetBrains s.r.o 2019-09-19 14:54:44,349 [2556395]  ERROR -      #org.jetbrains.idea.maven - OS: Windows 10 2019-09-19 14:54:44,352 [2556398]  ERROR -      #org.jetbrains.idea.maven - Last Action: Maven.ShowSettings 2019-09-19 14:55:06,491 [2578537]  ERROR -      #org.jetbrains.idea.maven - com.google.inject.CreationException: Unable to create injector, see the following errors:1) No implementation for org.apache.maven.model.path.PathTranslator was bound.  while locating org.apache.maven.model.path.PathTranslator    for field at org.apache.maven.model.interpolation.AbstractStringBasedModelInterpolator.pathTranslator(Unknown Source)  at org.codehaus.plexus.DefaultPlexusContainer$1.configure(DefaultPlexusContainer.java:350)2) No implementation for org.apache.maven.model.path.UrlNormalizer was bound.  while locating org.apache.maven.model.path.UrlNormalizer    for field at org.apache.maven.model.interpolation.AbstractStringBasedModelInterpolator.urlNormalizer(Unknown Source)  at org.codehaus.plexus.DefaultPlexusContainer$1.configure(DefaultPlexusContainer.java:350)2 errors java.lang.RuntimeException: com.google.inject.CreationException: Unable to create injector, see the following errors:1) No implementation for org.apache.maven.model.path.PathTranslator was bound.  while locating org.apache.maven.model.path.PathTranslator    for field at org.apache.maven.model.interpolation.AbstractStringBasedModelInterpolator.pathTranslator(Unknown Source)  at org.codehaus.plexus.DefaultPlexusContainer$1.configure(DefaultPlexusContainer.java:350)2) No implementation for org.apache.maven.model.path.UrlNormalizer was bound.  while locating org.apache.maven.model.path.UrlNormalizer    for field at org.apache.maven.model.interpolation.AbstractStringBasedModelInterpolator.urlNormalizer(Unknown Source)  at org.codehaus.plexus.DefaultPlexusContainer$1.configure(DefaultPlexusContainer.java:350)2 errors	at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:543)	at com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:159)	at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:106)2. 排查分析升级前的 Maven 版本是 Apache Maven 3.5.0，升级后版本是 Apache Maven 3.6.2：1）确认 conf\settings.xml 有无问题。2）Maven 新老版本兼容有问题：当前 Maven 最新版本 Maven 3.6.2 有可能与最新版的 IDEA 有冲突或兼容问题当前 Maven 最新版本 Maven 3.6.2 本身存在兼容问题3. 解决方案下载并使用 Maven 3.6.1 与 Intellij IDEA 2019.2.2 搭配。重新打开原有项目，问题解决。Maven 库地址：https://archive.apache.org/dist/maven/maven-3/			https://blog.csdn.net/itanping/article/details/101024299
33			['2020 年，AI 芯片内存哪家强？_AI科技大本营-CSDN博客']			目前多家公司都在开发网络边缘系统的AI芯片，本文作者详细分析AI边缘芯片遇到的问题和挑战，并给出一些新的内存技术解决方案。作者 | Mark LaPedus译者 | 弯月，责编 | 伍杏玲封图 | CSDN下载自视觉中国出品 | CSDN（ID:CSDNnews）以下为译文：目前多家公司都在开发网络边缘系统的AI芯片，然而供应商在处理结点和内存选择上面临着各种各样的难题，这些难题还会因应用而异。例如网络边缘类的产品涉及汽车、无人机、监控摄像头、智能扬声器乃至企业服务器。所有这些应用都包含运行机器学习算法的低功耗芯片。尽管这些芯片的许多组件与其他数字芯片并无二样，但主要区别在于这些芯片的大部分处理都是在内存中，或是在内存附近进行的。鉴于这一点，AI边缘芯片的制造商都在为下一代设备评估不同类型的内存。每种类型的内存都有自己的难题。在许多情况下芯片使用的都是成熟的工艺，而非最先进的技术，但它们本身也必须采用低功耗架构。AI芯片有时也称为深度学习加速器或处理器，在经过优化后，可以使用机器学习来处理系统中的各种工作负载。机器学习是AI的子集，它利用神经网络处理数据并识别模式，匹配某些模式，并了解其中哪些属性很重要。这些芯片面向整个计算应用领域，但是这些设计直接存在明显的差异。例如，为云开发的芯片通常基于高级流程，并且设计和制造成本很高。同时，边缘设备包括为汽车市场开发的芯片，以及无人机、监控摄像头、智能手机、智能门铃和语音助手。在这一广泛的领域中，每个应用都有不同的要求。例如，智能手机芯片与智能门铃的芯片截然不同。对于许多边缘产品而言，它们的目标是开发具有刚好够用的计算能力的低功耗设备。“这类的产品无法承受300瓦的GPU。对于许多这类应用来说，即使是30瓦的GPU也太大了。”The Linley Group首席分析师Linley Gwennap表示，“但是，设备制造商仍然希望制作出一些复杂的设备。这就需要比微控制器更强大的AI功能。你需要功能强大，但又不会耗尽电池或成本过高的芯片，尤其是在消费类应用程序中。因此，你必须考虑一些颇为激进的新解决方案。”一方面，大多数边缘设备都不需要昂贵的高级节点芯片，因为它们太昂贵了。当然也有例外。此外，许多AI边缘芯片都在内存内或在内存附近处理功能，这样做可以用更少的功耗来加速系统。供应商们正在考虑各种内存方法，并为将来的芯片探索新的方法：使用SRAM等常规存储器。使用NOR存储器，或一种名为模拟内存计算的新技术。使用相变化存储器、MRAM、ReRAM和其他下一代存储器，AI边缘芯片已开始广泛采用这些存储器。AI“爆炸”机器学习问世已经几十年了。然而我们的系统没有足够的能力来运行这些算法。近年来，由于GPU和其他芯片以及机器生成的算法的出现，机器学习开始蓬勃发展。“从1990年代开始，机器学习才开始得到了应用，”D2S首席执行官Aki Fujimura说，“但随着GPU的出现，近年来情况发生了变化。GPU推进了深度学习的应用，因为如今我们的计算能力加强了。”这些设备以及其他设备的目标是处理神经网络中的算法，其本质是计算矩阵乘积并求和。首先将数据矩阵加载到网络中。然后，每个元素乘以预先定好的权重，并将结果传递到网络的下一层，再乘以一组新的权重。重复几次这个步骤后，得出的结果就是有关数据的结论。机器学习已在许多行业中得到应用，其中在半导体行业中，已经出现了几十个机器学习芯片供应商。许多都是为云开发芯片的公司。这些系统的芯片旨在加速Web搜索、语言翻译以及其他应用程序。根据Linley Group的数据，2019年这些设备的市场规模超过了30亿美元。此外，市场上还涌现了几十个AI边缘芯片供应商，例如Ambient、BrainChip、GreenWaves、Flex Logix、Mythic、Syntiant等。预计到2024年，总共将有16亿台边缘设备配备深度学习加速器。AI边缘芯片可使用8位计算来运行机器学习算法。“你可以在同一个地方生成、使用和处理数据。这有很大的优点：我们都会面临电池寿命的问题。如果可以不用开互联网的连接，而是在本地进行AI处理，那就可以节省大量电量。响应性也很重要，还有可靠性，以及最终也要保证隐私。”Syntiant首席执行官Kurt Busch说，“在深度学习中，最大的问题就在于内存访问。电池和性能的瓶颈最终都会落在内存上。其次，并行处理。在深度学习中，我可以并行进行数百万次乘法和累加，并通过并行处理有效地线性缩放。”AI边缘芯片有不同的要求。例如，智能手机集成了领先的应用处理器。但其他边缘产品（比如门铃、监控摄像头和扬声器等）则并非如此。UMC业务开发副总裁Walter Ng表示：“边缘设备的解决方案涉及经济的问题。它必须对成本非常敏感。整体目的是具有竞争力的成本、低功耗以及简化的计算分布。”此外，还有其他因素需要考虑。许多AI边缘芯片供应商都需要在40nm左右的成熟节点上交付产品。目前这一工艺很理想，成本并不昂贵。但展望未来，供应商希望以低功耗获得更高的性能。下一个节点是28nm，这也很成熟而且很便宜。最近，制造厂商已经引入了各种22nm的工艺，这是28nm的扩展。22nm比28nm略快，但是价格高。大多数供应商都不会迁移到16nm/14nm的finFET，因为太贵了。迁移到下一个节点不是一个简单的决定。“如今许多客户及其应用都在40nm上。” Ng说，“当着眼于下一个节点路线图时，他们是否会满意，并在28nm上获得最佳性价比？还是说22nm看起来比28nm更具吸引力，能提供更多好处？这是许多人都在考虑的因素。”使用传统的内存技术在传统系统中，内存层次结构很简单。为此，我们将SRAM集成到可以访问常用程序的高速缓存处理器。用于主内存的DRAM是独立的，位于内存模块中。在大多数系统中，数据会在内存和处理器之间来回移动。但是这种交换会导致等待时间和功耗的增加，有时也称为“内存墙”，而且这个问题会随着数据量的增加而变得越来越严重。因此，在内存内或内存附近进行计算，就非常适合解决这个问题。内存内计算可以将需要处理的任务放到内存中，而内存附近计算可以使用距离处理逻辑最近的内存。并非所有芯片都使用内存计算。但是，AI边缘芯片供应商正在使用这些方法来打破内存墙。他们还从云上转移了一些处理功能。去年，Syntiant推出了第一款产品“神经决策处理器”，该处理器将神经网络体系结构集成到了一个小巧的低功耗芯片中。这个40nm的音频设备还集成了具有112KB RAM的Arm Cortex-M0处理器。Syntiant基于SRAM的存储器，将其体系结构归类为围绕内存的计算。该芯片背后的想法是让语音成为系统中的主要接口。亚马逊的Alexa就是一个很好的在线语音界面的例子。“语音是下一代的界面。”Syntiant的Busch说，“我们专门构建了这些解决方案，为所有电池供电的设备（小到助听器到，大到笔记本电脑或智能扬声器）增加了长期在线的语音接口。”Syntiant正在开发新设备，并在研究不同的存储器类型。“我们正在研究一些新兴的内存技术，例如MRAM和ReRAM，主要是为了提高存储密度。”Syntiant首席科学家Jeremy Holleman说，“首先是读取时的耗电，其次待机时的耗电也是一件大事，因为对于大型模型，最终的内存都会很大。但是，也许你只需要在给定实例上对进行较小一部分的计算。在不使用存储单元时，降低耗电的能力非常关键。”目前不需要高级流程。“在可预见的将来，先进节点的泄漏对于超低功耗应用来说太高了。” Syntiant的Busch说，“边缘设备经常无所事事。与数据中心中的设备相反，一旦开机就需要处理计算，而且你也希望它一直运转。但边缘设备经常在等待事情的发生。因此，你需要非常低的功耗，而高级节点并不擅长于此。”如今，大多数AI芯片都依赖内置的SRAM，速度很快。“但是，无论采用哪种技术，使用SRAM在独立的数字边缘处理器中安装数百万级的芯片都是非常昂贵的。”Cypress IP业务部设计总监Vineet Kumar Agrawal表示，“从DRAM获取数据的代价比从内部SRAM获取数据的代价高500倍。”同时，许多AI边缘芯片供应商正在使用或寻找另一种内存类型：NOR。NOR是一种非易失性闪存，用于独立和嵌入式应用程序中。NOR通常用于代码存储。NOR技术成熟，但需要在每个节点上都增加额外且昂贵的屏蔽步骤。而且很难将NOR的规模扩展到28nm/22nm以上。不过，有些公司正在使用当今的NOR闪存，开发一种称为模拟内存计算的技术。这些设备大多数是从40nm节点开始的。“看看传统的数字AI架构功耗的两个主要来源都是计算：乘法和加法。然后，其次是将数据从内存移至计算单元，然后再移回去。”Linley Group的Gwennap解释说，“人们的尝试都是在解决这两个问题。他们将计算直接放入存储电路中，因此数据就不必移动太远。他们没有使用传统的数字乘法器，而是使用了模拟技术，让电流能够通过可变电阻运行。然后使用欧姆定律来计算电流和电阻的乘积。”在内存内的模拟技术有望降低功耗。但是，并非所有的NOR都是一样的。例如，某些NOR技术基于浮栅体系结构。Microchip使用基于NOR的浮栅方法，开发了一种用于机器学习的内存内模拟计算架构。该技术集成了乘法累加（multiply-accumulate，MAC）处理引擎。“采用这种方法，用户无需将模型参数或权重存储在SRAM或外部DRAM中。”Microchip SST部门嵌入式存储器产品开发总监Vipin Tiwari表示，“将输入数据提供给阵列进行MAC计算。这样做可以消除MAC计算中的存储瓶颈，因为计算是在存储权重的地方完成的。”还有其他NOR的方法。例如，Cypress长期以来一直在提供另一种称为SONOS的嵌入式NOR闪存技术。SONOS基于电荷陷阱闪存，是一种双晶体管技术，可以通过从氮化物层添加或去除电荷来改变阈值电压，它适用于28nm以下的各种节点。SONOS经过优化后可以作为机器学习的嵌入式存储器。“两个SONOS多位嵌入式非易失性存储单元最多可以替代8个SRAM单元，即48个晶体管。这非常有效，而且你还可以将功率效率和吞吐量提高50-100倍。”Cypress的Agrawal说，“SONOS使用高度线性和低功率的隧穿工艺进行编程，该工艺能够通过高度控制来瞄准Vts，从而产生纳安级比特单元电流水平。这与使用热电子的浮栅相反，在浮栅中你无法控制流入电池的电流量。另外，你的电池电流要高得多。”使用新的内存技术由于NOR无法扩展到28nm/22nm以上，因此AI边缘芯片供应商正在研究几种下一代存储器类型，例如相变存储器（PCM）、STT-MRAM、ReRAM等。对于AI而言，这些存储器还运行带有神经网络的机器学习应用程序。这些存储器很有吸引力，因为它们将SRAM的速度和闪存的非易失性结合在一起，具有无限的耐久性。但是，由于新存储器使用复杂的材料和切换方案来存储数据，因此它们的开发时间更长。“半导体制造商从基于电荷的存储器（SRAM、NOR）迁移到电阻性存储器（ReRAM，PCM）时面临新的挑战，”KLA过程控制解决方案亚洲地区总监Masami Aoki说，“这些新兴的存储器由新元素组成，需要精确控制材料性能和新的缺陷控制策略，才能确保性能均匀性和可靠性，特别是对于大规模集成而言。”长期以来，英特尔一直在发售3D XPoint，这是一种PCM。美光公司也出售PCM。非易失性存储器PCM通过更改材料的状态来存储数据，比具有更好耐久性的闪存快。PCM是一项具有挑战性的技术，尽管供应商已解决了这些问题。“使用3D XPoint相变存储器，硫族物对环境条件和过程化学反应异常敏感。”Lam Research执行副总裁兼首席技术官Rick Gottscho表示，“处理所有这些问题的技术策略多种多样。”PCM也是AI的目标。2018年，IBM发表了一篇关于使用PCM处理8位精度内存乘法技术的论文。尽管还没有人批量销售产品，但是IBM和其他公司仍在为AI边缘应用程序开发PCM。STT-MRAM也在发售中，它具有SRAM的速度和闪存的非易失性以及无限的耐用性。它利用电子自旋的磁性在芯片中提供非易失性。STT-MRAM是嵌入式应用的理想选择，旨在取代22nm及更高波长的NOR。“看看新的内存，MRAM是低密度（小于1Gb）的最佳选择。MRAM是最好的嵌入式内存。它比NOR更好，尽管你可以在28nm或更大的芯片上采用NOR。NOR添加了12个以上的蒙版，因此从成本、密度和性能的角度来看，MRAM是嵌入式的首选。”MKW Ventures Consulting负责人Mark Webb说。但是，一些专家认为，MRAM仅支持两个级别，因此不适合内存计算。有些人则有不同的看法。Imec杰出的技术人员Diederik Verkest说：“一个MRAM设备确实只能存储一个位。但是，在内存计算中，重要的是要了解存储设备和计算单元之间的差异。计算单元执行存储的权重和输入激活的乘法。在最佳情况下，计算单元内部的存储设备可以存储多个重量级别。但是，可以使用多个存储设备制作存储权重的计算单元。如果使用3级权重（则权重可以为-1、0、1），则可以使用两个存储设备，并且计算单元将由两个存储设备以及围绕该存储单元的一些模拟电路组成，用以计算乘积重量值和激活。因此，MRAM设备可以在计算单元内部使用，存储多级权重并构建内存计算解决方案。”ReRAM是另一种选择。与闪存相比，该技术具有更低的读取延迟和更快的写入性能。ReRAM将电压施加到材料堆栈上，从而导致电阻变化，并将数据记录在内存中。在最近的IEDM会议上，Leti发表了一篇论文，介绍了有关有关使用模拟和ReRAM技术开发集成脉冲神经网络（Spiking Neural Network，SNN）的芯片技术。130nm测试芯片的每个峰值功耗为3.6pJ，一台使用28nm FD-SOI的研发设备。SNN与传统的神经网络不同。Linley Group的Gwennap表示：“它不会有任何耗电，除非输入发生变化。因此，从理论上讲，如果你的监控摄像头正对着你的前院，那么它就是理想的选择。除非有人走过去，否则一切都不会改变。”Leti的SNN设备是边缘的理想选择。Leti的研究工程师Alexandre Valentian说：“到底边缘是什么意思，还有待观察，但是我可以说ReRAM和SNN是特别针对端点设备而定制的。ReRAM和脉冲编码非常适合，因为这种编码策略简化了内存计算。不需要在输入端使用DAC（如矩阵矢量乘法），它可以简化输出端的ADC（位数更少），或者如果神经元是模拟的，则最终将其完全删除。”然而，ReRAM很难开发。只有少数几个零件可用。“在我们看来，理论上 ReRAM适合于1T1R设计（嵌入式），以及将来使用合适的交叉点选择器的1TnR。难点在于，过去两年中实际产品的开发非常缓慢。我们认为，这是由于存储元素本身的保留问题和干扰（相对于循环）。这些问题需要解决，我们需要具有64Mbit嵌入式和1Gbit交叉点的产品。” MKW的Webb说。总而言之，在下一代存储器中，哪一类更适合于AI边缘应用尚无共识。业界继续探索当前和未来的选择。例如，Imec最近在评估了几种选择后，使用名为AiMC的模拟内存计算架构启用10000TOPS/W矩阵矢量乘法器。Imec评估了三个选择：SOT-MRAM、IGZO DRAM和投影PCM。自旋轨道扭矩MRAM（Spin-orbit torque MRAM，SOT-MRAM）是下一代的MRAM。而氧化铟镓锌（indium gallium zinc oxide，缩写：IGZO）是一种新型的晶体结构。Imec的Verkest表示，“存储DNN的权重的设备有很多种。这些设备使用不同的机制来存储权重值（磁性、电阻、电容），并采用AiMC阵列的不同实现。”总结目前尚不清楚哪种当前或下一代内存技术是赢家。也许所有技术都拥有一席之地。SRAM、NOR和其他常规存储器也有用武之地。但数十家AI芯片供应商的空间不大。目前已有重大动荡的迹象，大型公司开始收购创业公司。与所有新的芯片部门一样，有些公司将取得成功，有些将被收购，而有些将失败。原文链接：https://semiengineering.com/memory-issues-for-ai-edge-chips/本文为CSDN翻译文章，转载请注明出处。推荐阅读GitHub标星2000+，如何用30天啃完TensorFlow2.0？清华周界详解《基于图神经网络的事实验证》 |  百万人学AI百年 IBM 终于 All In 人工智能和混合云！微软为一人收购一公司？破解索尼程序、写黑客小说，看他彪悍的程序人生！机器学习项目模板：ML项目的6个基本步骤BM、微软、苹果、谷歌、三星……这些区块链中的科技巨头原来已经做了这么多事！你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/105423836
34			['如何实时抓取动态网页数据？_网页爬虫与数据采集 · 八爪鱼-CSDN博客']			我们所生活的数字世界正在不断地产生大量的数据。利用动态大数据已经成为企业数据分析的关键。在本文中，我们将回答以下几个问题：1、为什么采集动态数据很重要?2、动态数据是如何有效的促进业务增长?3、最重要的是，我们如何能够轻松地获取动态数据?1、为什么采集动态数据如此重要?一般来说，通过持续监测动态的数据，你可以在最短的时间里做出正确的决策。更具体地说，获取动态数据可以帮助：（1）更快地进行数据驱动的决策采集动态数据可以为您实时的提供关于市场和竞争对手最新趋势的信息。有了所有更新的信息，您可以更快、更轻松获得基于数据的分析结果，做出由数据驱动的决策。正如亚马逊首席执行官杰夫•贝佐斯(Jeff Bezos) 在给股东的一封信中所说： “业务的速度至关重要”。“高速决策”对业务发展具有重要意义。（2）建立更强大的数据库随着数据量的不断增长，与每条数据相关的价值已急剧下降。为了提高数据分析的质量和决策的准确性，企业需要通过不断采集动态数据来构建一个全面的，高容量的数据库。数据是一项对时间敏感的资产。时间越早的数据，收集起来就越困难。随着信息的数量每年在规模和速度上成倍增长，监控不断更新的数据以进行进一步分析变得异常重要。一般来说，短期数据收集可以帮助解决最近的问题并做出较小的决策，而长期数据收集可以帮助企业识别市场趋势和商业模式，从而帮助企业设置长期的业务目标。（3）建立自适应分析系统数据分析的最终目的是建立一个自适应、自主的数据分析系统，从而持续地分析问题。毫无疑问，自适应分析系统是以自动收集动态数据为基础的。在这种情况下，它可以节省每次构建分析模型的时间，并消除了循环收集数据中的人为因素。无人驾驶汽车是自适应分析解决方案的一个很好的例子。2.动态数据如何有效地促进业务增长?我们可以在很多方面应用动态数据分析，并以此来促进业务发展，如：（1）产品监控产品信息，如价格，描述，客户评论，图片等，都可以在线上平台上获取，并且实时更新。例如，通过在亚马逊上搜索产品信息或者从eBay上抓取价格信息，可以轻松地进行产品预发布市场研究。抓取更新数据还可以让您评估产品的竞争地位，并制定有效的定价和库存策略。这是一种监视竞争对手市场行为的可靠和有效的方法。（2）客户体验管理公司比以往更加关注客户体验管理。从Gartner的定义来看，它是“设计和响应客户交互以达到或超过客户期望，从而提高客户满意度，忠诚度和拥护度的做法。”例如，提取亚马逊上某产品的所有评论，通过分析评论的情感正负面，可以帮助企业了解客户对产品的看法。同时这有助于了解客户的需求，以及实时知道客户的满意度。（3）市场营销策略动态数据分析可以让企业知道过去哪种策略效果最好，当前的营销策略效果如何，以及哪些地方可以进行改进。动态数据的采集可以使企业实时评估营销策略的成功程度，并据此进行相应的精确调整。3.我们如何能够轻松地获取动态数据?为了及时、持续地收集动态数据，传统的手工复制粘贴已不再可行。在这种情况下，一个简单易用的网页抓取工具可能是最佳的解决方案，它具有以下优点：（1）无需编程使用网页抓取工具，操作人员无需具备编程知识。任何人和任何企业都可以轻松地从网页上抓取动态数据。（2）适用于各种网站不同的网站具有不同的结构，因此即使是经验丰富的程序员也需要在编写爬虫脚本之前先研究网站的结构。但一个强大的网页抓取工具可以让您轻松快捷地从不同的网站上抓取信息，从而节省了您研究不同网站结构的大量时间。（3）定时抓取这需要网页抓取工具支持在云端采集数据，而不仅仅只是在本地电脑上运行。通过云端采集这种方式，采集器可以根据您设置的时间自动运行采集数据。八爪鱼云采集的功能远不止这些。（4）灵活的定时时间表八爪鱼云采集支持随时随地抓取网页数据，定时时间和频率可根据您的需要进行调整。（5）采集速度更快通过8-12个云服务器同时采集，同一组数据的抓取速度可以比在本地计算机上运行快8-12倍。（6）数据抓取成本更低八爪鱼云采集支持在云端抓取数据，并将采集到的数据存储在云端的数据库中，企业无需担心高昂的硬件维护成本或者采集中断。此外，与市场上同类的竞品相比，八爪鱼采集器的数据采集成本降低了50%。八爪鱼一直致力于提高数据分析的价值，使每个人都能以可承受的价格使用大数据。（7）API，自定义数据对接虽然云采集数据可以自动导出到用户的数据库中，但是通过API，可以大大提高数据导出到您自己系统中的灵活性，轻松实现自己系统和八爪鱼采集器的无缝对接。需要知道的是，八爪鱼采集器的API有两种：数据导出API和增值API。数据导出API仅支持导出数据；增值API，支持导出数据，同时还支持修改任务里面的部分参数，控制任务的启动/停止等。----------------------------------------------------------作者: Surie M. (八爪鱼团队)编辑/翻译：蒋红（八爪鱼团队）			https://blog.csdn.net/BAZHUAYUdata/article/details/101269912
35			['github设置仓库可见性  私人仓库设置他人协作/可见_种花家的奋斗兔的博客-CSDN博客']			设置仓库可见性您可选择能够查看仓库的人员。本文内容关于仓库可见性将仓库设为私有将仓库设为公共将仓库设为内部关于仓库可见性创建仓库时，您可以选择将其设为公共、内部或私有。 公共仓库可供使用GitHub.com 的所有人访问，而私有仓库只有您和您与其共享的人员可访问。 内部仓库适用于 GitHub Enterprise Cloud 并且只有企业帐户的成员可访问。 更多信息请参阅“创建内部仓库”。仓库所有者、具有组织所拥有仓库管理员权限的人员和组织所有者均可更改仓库的可见性。对于组织拥有的仓库，如果组织所有者将更改仓库可见性的功能限制为仅组织所有者，则具有公共仓库管理员权限的人员无法将其设为私有。将仓库设为私有如果您使用 GitHub Free，则个人帐户拥有的私有仓库最多可有三个协作者。 如果您添加了超过三个其他用户作为仓库协作者，您需要在将仓库设为私有之前将协作者数量减少为三个或更少，或升级到 GitHub Pro。 更多信息请参阅“从个人仓库中删除协作者”。如果您使用 GitHub Free 并将仓库的可见性从公共更改为私有，您将失去对如受保护分支和 GitHub 页面 之类功能的访问权限。 任何已发布的 GitHub 页面 站点都将自动取消发布。 如果您将自定义域添加到 GitHub 页面 站点，应在将仓库设为私有之前删除或更新 DNS 记录，以避免域接管的风险。 更多信息请参阅“添加或删除 GitHub Pages 站点的自定义域”。如果您将仓库的可见性从内部更改为私有，则没有新私有仓库访问权限的任何用户所属的复刻都将被删除。私有仓库具有不限数量的协作者以及 GitHub Pro、GitHub Team 和 GitHub Enterprise Cloud 的完整功能。 更多信息请参阅“GitHub 的产品。”警告：公共仓库设为私有时，将分离该公共仓库的公共复刻并放入新网络中。 公共复刻无法设为私有。 更多信息请参阅“删除仓库或更改其可见性时，复刻会发生什么变化？”在 GitHub 上，导航到仓库的主页面。在仓库名称下，单击Settings（设置）。在“Danger Zone（危险区域）”下“Make this repository private（将此仓库设为私有）”旁边，单击Make private（设为私有）。阅读关于将仓库设为私有的警告。输入您要设为私有的仓库的名称，例如accountname/reponame。单击I understand, make this repository private（我已了解，请将此仓库设为私有）。将仓库设为公共警告：将私有仓库设为公共时，将分离其私有复刻。 更多信息请参阅“删除仓库或更改其可见性时，复刻会发生什么变化？”在 GitHub 上，导航到仓库的主页面。在仓库名称下，单击Settings（设置）。在“Danger Zone（危险区域）”下“Make this repository public（将此仓库设为公共）”旁边，单击Make public（设为公共）。阅读关于将仓库设为公共的警告。输入您要设为公共的仓库的名称，例如accountname/reponame。单击I understand, make this repository public（我已了解，请将此仓库设为公共）。用于创建开源项目的资源如果您将私有仓库转换为公共仓库作为转向创建开源项目的组成部分， 请参阅开源指南以获得有用的提示和指导。 您还可以通过GitHub Learning Lab参加有关管理开源项目的免费课程。 您的仓库设为公共后，您还可以查看仓库的社区资料以了解项目是否符合支持贡献者的最佳做法。 更多信息请参阅“查看您的社区资料”。将仓库设为内部注：内部仓库可用于 GitHub Enterprise Cloud。更多信息请参阅“GitHub 的产品”。内部仓库目前处于测试阶段，可能会发生变化。在 GitHub 上，导航到仓库的主页面。在仓库名称下，单击Settings（设置）。在“Danger Zone（危险区域）”下“Make this repository internal（将此仓库设为内部）”旁边，单击Make internal（设为内部）。阅读关于将仓库设为内部的警告。输入您要设为内部的仓库的名称，例如accountname/reponame。单击I understand, make this repository internal（我已了解，请将此仓库设为内部）。			https://blog.csdn.net/IT_flying625/article/details/101288908
36			['IDEA创建三级父子模块Maven项目（Springboot）（一）----项目构建_程序员精进之路-CSDN博客']			1.创建完之后的结构2.创建一级项目Project    dhcc-shop-parent3.创建二级项目Module      dhcc-shop-basics此处需要特别注意，Springboot项目层级默认是2层的，所以最后一层的打包方式Packaging一定是Jar/War，但是我们还要创建下一级的子项目，所有此处需改成pom打包方式，pom方式打包是可以继承的。在pom.xml中增加打包方式的约定<packaging>pom</packaging>4.创建第三级项目Module      dhcc-shop-basics-springcloud-eureka此处需要注意，因为默认是两层项目，第三层的项目文件是默认放到第二层的根目录下，这个是有问题的，文件会覆盖，项目会自动被删除，改变第三层项目的存储路径即可。默认情况下需要改成：5.测试服务运行情况主类上加注解@EnableEurekaServer配置文件###服务端口号server:  port: 8100###eureka 基本信息配置eureka:  instance:    ###注册到eurekaip地址    hostname: 127.0.0.1  client:    serviceUrl:      defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/    ###因为自己是为注册中心，不需要自己注册自己    register-with-eureka: false    ###因为自己是为注册中心，不需要检索服务    fetch-registry: false链接：https://pan.baidu.com/s/1uOSHFjD4yyrE0UBnlBsJZw提取码：how7			https://blog.csdn.net/sunhuansheng/article/details/101307407
37			['手动上传SNAPSHOT文件到Maven私服Nexus的方法_IT技术爱好者-CSDN博客']			公司用Nexus搭建的Maven私服，之前一直用代理方式链接兄弟公司的Maven私服，来使用他们的研发成果。最近他们出于安全考虑禁止了外部访问，改为直接把jar包发送给我们，而我们需要把jar包手动上传到我们的私服上供开发团队使用。问题来了：他们提供的jar是SNAPSHOT版本，Nexus私服的Release仓库不允许上传SNAPSHOT版本，会报错，而SNAPSHOT仓库压根就不提供Web界面上传功能。经过调查，找到的办法是通过Maven命令行直接上传文件。命令行的完整写法如下：mvn deploy:deploy-file -DgroupId=com.youcompany -DartifactId=your-artifactID -Dversion=1.0.0-SNAPSHOT -Dpackaging=jar -Dfile=F:\jar\your-jar-1.0.1-SNAPSHOT.jar -Durl=https://yourcompany.com/nexus/content/repositories/snapshots/ -DrepositoryId=snapshots前面几个参数显而易见不解释，最后两个参数简单说一下：url: 在nexus上的目标SNAPSHOT仓库的URL地址。repositoryId: 在maven本地settings.xml中，与上述URL对应的节点中配置的id。比如，如果settings.xml配置的是：<server><id>releases</id><username>admin</username><password>123</password></server><server><id>snapshots</id><username>admin</username><password>123</password></server>那么要上传SNAPSHOT包的话，repositoryId就应该是snapshots。参数都确定后，就去命令行执行吧，一切顺利的话，上传成功后，在Web界面应该就能搜索到了。			https://blog.csdn.net/zazzh007/article/details/101272511
38			['基于深度学习和传统算法的人体姿态估计，技术细节都讲清楚了_AI科技大本营-CSDN博客']			作者 | 站长 pursueYfuture来源 | AI专栏（ID: pursue-Y-future）计算机视觉的一大研究热点是人体姿态估计，还有很多问题急需解决，比如遮挡，交互等等。在最近的CVPR2020里边也有很多这方面的工作。本文站长主要是想谈谈基于深度学习的实时多人姿态估计。人体姿态估计要干嘛？关于人类活动规律的研究，必定是计算机视觉领域首要关注的内容。其中，人体姿态估计便是计算机视觉领域现有的热点问题，其主要任务是让机器自动地检测场景中的人“在哪里”和理解人在“干什么”。随着信息化时代的迅速发展，人类每时每刻都在通过多种多样的手段和途径获得海量的可视化图像数据，这使得基于自然场景图像的人姿态估计研究在现实生活中拥有很多潜在的应用价值。图1展示了自然场景图像中人体姿态估计的研究应用。Fig.1: 自然场景图像人体姿态估计的研究应用在信息化的时代，视频监控正在银行、超市以及公安机关等关乎人民财产、人身安全的重要场所发挥着举足轻重的角色。面对海量的视频图像序列，为了及时地制止现场事故的进一步发生，工作人员必须长时间且精神高度集中地观察视频监控画面并对异常事件作出处理。除了工作人员很难长时间地保持高度警惕外，长期投入大量的人力来监测小概率发生的事件也不是单位机构提倡的做法。因此，实现视频监控的智能化成为一种互联网时代的必然趋势。但是，实现智能视频监控的前提条件是让机器自动地识别视频图像序列中的人体姿态，从而进一步分析视频图像中人类的行为活动。这就涉及到了我们下面所要说的人体行为分析了。人体行为分析又为那般？人体行为分析理解成为了近几年研究的热点之一。在人体行为分析理解的发展过程中，研究人员攻克了很多技术上的难关，并形成了一些经典算法，但仍有很多尚未解决的问题。从研究的发展趋势来看，人体行为分析的研究正由采用单一特征、单一传感器向采用多特征、多传感器的方向发展。而人体姿态估计作为人体行为识别的一个重要特征，是进行人体行为分析的基础，是人体行为分析领域备受关注的研究方向之一。Fig.2: 人体行为分析的实际应用情形人体姿态估计是指从图像中检测人体各部分的位置并计算其方向和尺度信息。人体行为分析是基于多帧图像的前后关系进行分析理解，而人体姿态识别是针对单帧静态图像进行处理。正确识别出多帧连续的静态图像的姿态信息，为实现正确的行为分析理解提供了可能。因此，人体姿态估计的准确性与实时性直接影响人体行为分析的准确性和实时性，确保实时准确的姿态识别是进行下一步行为分析的基础。现在，我们的人体姿态估计课题的发展已越来越贴近实际，例如在步态分析、人机交互以及视频监控等领域，人体姿态估计均具有广泛的应用前景。所以呢，研究人体姿态估计还是蛮有意思的，好玩 !当前姿态估计算法有哪些？目前主流的人体姿态估计算法可以划分为传统方法和基于深度学习的方法。基于传统方法的人体姿态估计传统方法一般是基于图结构和形变部件模型，设计2D人体部件检测器，使用图模型建立各部件的连通性，并结合人体运动学的相关约束不断优化图结构模型来估计人体姿态。其缺点是什么？First，传统方法虽然拥有较高的时间效率，但是由于其提取的特征主要是人工设定的HOG和SHIFT特征，无法充分利用图像信息，导致算法受制于图像中的不同外观、视角、遮挡和固有的几何模糊性。同时，由于部件模型的结构单一，当人体姿态变化较大时，部件模型不能精确地刻画和表达这种形变，同一数据存在多个可行的解，即姿态估计的结果不唯一，导致传统方法适用范围受到很大限制。Second，另一方面，传统方法很多是基于深度图等数字图像提取姿态特征的算法，但是由于采集深度图像需要使用专业的采集设备，成本较高，所以很难适用于所有的应用场景，而且采集过程需要同步多个视角的深度摄像头以减小遮挡问题带来的影响，导致人体姿态数据的获取过程复杂困难。因此这种传统的基于手工提取特征，并利用部件模型建立特征之间联系的方法大多数是昂贵和低效的。基于深度学习的人体姿态估计算法随着大数据时代的到来，深度学习在计算机视觉领域得到了成功的应用。因此，考虑如何将深度学习用于解决人体姿态估计问题，是人体姿态估计领域的学者们继图结构模型后所要探索的另一个重点。早期利用深度学习估计人体姿态的方法，都是通过深度学习网络直接回归出输入图像中关节点的坐标。Fig.3: DeepPose基于深度学习的人体姿态估计方法主要是利用卷积神经网络(CNN)从图像中提取人体姿态特征，相比于传统方法人工设计特征，CNN 不仅可以得到语义信息更为丰富的特征，而且能够获得不同感受野下多尺度多类型的人体关节点特征向量和每个特征的全部上下文(contextual)，摆脱对部件模型结构设计的依赖，然后对这些特征向量进行坐标回归以反映当前姿态，从而将姿态信息应用于具体实际之中。其好处是什么？First，相较之下日常生活中的单目摄像头更为常见，虽然其采集的彩色图像容易受到光照等环境因素的影响，但是可以利用神经网络提取出比人工特征更为准确和鲁棒的卷积特征，以预测更为复杂的姿态，所以基于深度学习的人体姿态估计方法得到了深入的研究。Second，不同于传统方法显式地设计特征提取器和局部探测器，进行深度学习时构建CNN比较容易实现，同时可以设计处理序列问题的CNN模型，例如循环神经网络RNN，通过分析连续多帧图像获得人体姿态的变化规律，进而为人体姿态中各个关节点之间建立更为准确的拓扑结构。OK，in summary，人体姿态估计算法其实主要可分为基于传统方法的人体姿态估计和基于深度学习的人体姿态估计计算法两大类，而基于传统方法的人体姿态估计一般通过待处理图像到，部位或关节定位的非线性映射来实现。不论是具有理论优势的匹配模型，还是使得人体姿态估计准确性有所提升的深度学习网络，都正促使着人体姿态估计领域快速地发展。但是，如何在表征人体复杂结构的理论数学模型和提升估计结果的精度上同时取得突破，是人体姿态估计领域一直以来探索的终极目标。因此，人体姿态估计领域在未来的工作中具有较大的研究发展空间。Fig.4: 2D估计多人姿态估计本文站长主要是想谈谈基于深度学习的实时多人姿态估计。主要是拜读了文献7，所以本文站长想谈谈自己通过很多文献的全面阅读后，自己的一些想法和理解，有理解不到位的地方请大家斧正，谢谢。目前多人姿态估计主要有两种思路，一种是基于自顶向下的算法，另一种是基于自底向上的算法。Fig.5: 实时多人估计自顶向下自顶向下的算法先从图像中检测出所有人，随后利用单人姿态估计的方法对所有人进行姿态估计。自顶向下算法的缺点是算法运行效率随着人数增加而降低，且部分被遮挡的人无法被检测，精度不高。自底向上自底向上的算法，先检测出所有人的骨点，再将骨点进行连接形成图，最后通过图优化的方法剔除错误的连接，实现多人姿态估计。自底向上算法的优点是运行时间不随人数增加而线性增加，更有利于实时多人姿态估计。站长这次采用的多人姿态估计方法是基于自底向上的方法，是一种六阶段双分支的深度神经网络结构，可同时获得骨点位置以及骨点之间的连接置信度，有效稀疏骨点连接图，提高算法运行效率。对于骨点连接置信度，有骨点之间的亲和区域方法，通过在亲和区域上的线性积分计算骨点连接置信度。网络结构对于一张输入图像，深度神经网络同时预测出每个骨点的热力图S=(S1，S2，…，SJ)和骨点之间的亲和区域L=(L1，L2，…，LC) 。热力图的峰值为骨点的位置，骨点相互连接构成二分图，亲和区域对图的连接进行稀疏，最后对二分图进行最优化实现多人姿态估计。网络结构深度解读如图6所示，整体网络架构为六阶段双分支，上分支负责预测骨点位置，下分支负责预测骨点之间的亲和区域。前一阶段的预测结果融合原有图像特征并作为下一阶段的输入，经过多阶段的操作以提高骨点预测精度。Fig.6: 网络结构图像特征采用VGG－19模型进行提取，并用符号F表示图像特征。在第一阶段，网络以F作为输入，输出关节点的热力图S1 = ρ1 (F) 和骨点之间的亲和区域L1 = φ1(F)，其中 ρ1 和 φ1 为网络的映射函数，其本质是一系列的卷积操作。在第一阶段，对于输入特征采用3×3大小的卷积核连续进行三次卷积，之后用1×1 大小的卷积核连续进行三次卷积。之后的阶段将前一阶段的预测结果和原图像特征F进行融合，作为当前阶段的输入，经过卷积操作分别预测出关节点热力图和关节点的亲缘关系程度（站长自己的理解haha，简单点就是两个关节点的朋友关系的亲密程度呗):其中ρt 和φt 分别表示现阶段t 的卷积操作，先用大小为7×7的卷积核连续进行五次卷积操作，之后用大小为1×1的卷积核连续两次卷积操作，最终输出本阶段的关节点热力图和关节点亲和区域。损失函数由于关节点热力图和关节点的亲缘关系程度本质有所不同，因此在训练的时候需要分别对关节点位置和亲和区域进行监督，损失函数均采用L2损失。为了避免梯度消失现象发生，在每个阶段的输出都添加损失函数，起到中继监督作用。另外在样本标注的时候，会存在行人漏标等情况对损失函数造成影响，因此需要对损失函数在各个位置进行掩膜操作。于是，对于关节点位置和亲和区域的损失函数形式如下:其中S*j和L*c分别为关节点位置和亲和区域的实际测量值，W为掩膜函数，有标注的位置为1，没标注的位置为0，W(p)=0表示在位置p处没有标注。Finally，整个网络的最终损失为每个阶段两个损失之和的累加:站长关于两个概念的深层剖析前面站长谈到了关节点热力图和关节点亲和区域（这个名称是曹大佬自己说的haha）,可能有些站友对这2个概念可能云里雾里，下面站长就根据自己的理解好好的盘他一般。关节点热力图关节点热力图是衡量关节点在图像某位置出现的置信度，由一系列二维的点组成，每个点表示骨点出现在该位置的置信度，骨点最终位置定义为置信度最高的位置。对于图像中只有一个人的情况，则某一类型可见的关节点在热力图中只有一个峰值。对于多人情况，某一类型可见的骨点存在多个峰值，表示不同人的同一个类型骨点。Fig.7: 关节点热力图（引自文献7）强调！Attention！这里站长还是要强调下多人情况下的热力图和单人是有很大区别的。与CPM不同，CPM网络只是针对单个人的Pose，所以它的网络输出的P张置信度图中（假如一个人总共有P个关节点），每一张置信度上只有一个热点，这个热点只是一个人的一个关节点，比如右手腕关节这个关节点。但如果图片上有多个人，它的第一行网络输出的P张置信度图中（假如单个人总共有P个关节点），每一张置信度上就有多个热点了，比如右手腕关节，假设有K个人，则要有K个右手腕关节点，所以此时这张置信度上就要有K个热点了。为了更清楚地解释这件事，站长画了下图以形象化得辅助大家理解（画得太丑了orz大家别见怪haha，道理理解清楚就行了）Fig.8: 站长丑画hahaIn other way，我们在对样本进行标注时，只需确定骨点的位置，则该骨点真值热力图为在该点处放置一个固定方差的高斯核，用高斯函数确定各个位置的置信度。对于第k个人的第j个骨点，以xj，k表示骨点的实际位置，则该骨点周围的像素点的置信值为:其中标准差σ控制了置信值的分布范围。对于一张存在多人的图像，每个人特定骨点的实际热力图为取得高斯核范围内的最大值:Fig.9: Max操作（引自文献7)注意：P是个二维的坐标点(x,y)，表示图片中的每一个点。关节点亲和区域这个概念其实可以简单点理解就是两个关节点的朋友关系的亲密程度。如图10所示，骨点亲和区域由一系列单位向量组成，每一段肢体对应一个亲和区域，位于肢体上的像素点都由一个单位向量进行表示，包含了位置和方向信息，所有在肢体上的单位向量构成亲和区域。Fig.10: 关节点亲和区域二分图优化（重点来了）这是这个网络实现关节点检测的关键所在了，上面经过网络推理，得到骨点热力图以及骨点之间的亲和区域，对热力图采取非极大值抑制得到一系列候选骨点。由于多人或者错误检测，对于每一类型的骨点会存在多个候选骨点。这些候选骨点之间的连接构成二分图，每两个骨点之间的连接置信度通过线积分计算得到。为二分图找到最优的稀疏性是NP－Hard 问题。优化该二分图即在所有边中选择一组边使得最终二分图的总权重最大，所以目标函数可写为:约束条件:其中Ec为二分图优化之后肢体c的权重，我们要取其中总权重之和最大的;Zc为所有骨点连接集合Z的子集;约束条件表示一段肢体最多只存在一条连接边。Fig.11: 算法效果问题分解与简化为扩展到多人所有骨点的最优化问题，即定义Z为K 维匹配问题，这是一个NP－hard问题，为了提高最优化效率，如图所示，本文采用两种方法降低二分图优化算法的复杂度。首先，如图所示，剔除跨骨点之间的连接构成稀疏二分图，代替全连接二分图;然后根据肢体将稀疏后的二分图拆解得到图所示的多个简化二分图。Fig.12: 多人姿态估计求解（引自文献7）因此，整体优化问题转化为对各个简化后的二分图进行最优化。而最优化的目标函数为所有简化二分图的权重之和达到最大:优化之后将各个简化二分图中共同的骨点进行整合得到最终多人人体姿态估计。这样做的优点是将NP－hard问题转化为多个较容易求解的二分图最优化，可以有效逼近全局最优解，同时降低算法复杂度，提高算法的运行效率，达到实时多人姿态估计的目的。站长的笔记整理我对这个算法的整体思路做了个笔记，字太丑了orz，大家别见怪haha，道理讲明白理解清楚就行了。Fig.13: 站长的算法笔记（求各位大佬斧正）站长的实验结果和分析实验所使用的显卡为NVIDIA TITAN XP，CPU为Intel i7－6900K。图像大小为1920× 1080，通过下采样方法额外获得1280 × 720 和720 × 480 两个低分辨率的视频。首先分析运行效率与人数的关系，在相同视频流和相同分辨情况下，计算自顶向下与自底向上运行时间与人数关系，计算结果如图14所示。由图可知，自顶向下随着人数的增加耗时几乎呈线性增加，而自底向上的运行耗时几乎不随人数增加而递增。卷积神经网络预测关节点的耗时也几乎不随人数增加而增加。因此我所使用的自底向上算法的运行效率不受行人数量的影响，对人数不确定的情况依然可以实时进行多人姿态估计。Fig.14: 实验的运行耗时最后，对三种分辨率视频采用两种不同方法进行耗时分析，结果如表所示，随着分辨率的降低，处理速度越来越快。若对视频所有帧都进行关节点检测，在最高分辨率情况下每秒可处理23帧，人眼感觉不到卡顿，基本达到实时。如果采用间隔检测结合跟踪，帧率可提高十几帧，完全达到实时要求。站长测试（使用自己乱糟糟的图片才有说服力哈）以下是我采用深度学习算法（Openpose)最终的实验结果（效果果然杠杠的）：Fig.15: 一次旅游haha总体而言，效果还算很nice的,关节点都检测出来了，Great!(羞涩的我）能阅读到这里，说明你也是个踏踏实实的做研究的人了。此时，我们娱乐时间到了，让我们来段测试视频放松放松下哈：总结六阶段双分支网络结构在关节点预测精度上略高于现有传统的的人体姿态估计算法。本次站长采用的算法利用自底向上的思想，首先预测出所有骨点位置，并将骨点连接形成图结构，通过图优化实现多人体姿态估计。算法运行效率方面，由于网络同时预测出关节点位置和关节点之间的空间关系，为多人姿态估计算法提供更加稀疏的二分图，降低二分图优化复杂度而达到了实时的效果。巨人的肩膀[1] Qian C, Sun X, Wei Y, et al. Realtime and robust hand trackingfrom depth[C]//Proceedings of the IEEE conference on computer vision and patternrecognition. 2014: 1106-1113.[2] Joseph  Tan  D, Cashman  T,  Taylor J,  et  al. Fits  like  a glove:  Rapid  and reliable  hand shape  personalization[A].  IEEE Conference  on  Computer Vision  and  Pattern Recognition[C], 2016: 5610-5619.[3] Tang D, Jin Chang H, Tejani A, et al. Latent regression forest:Structured estimation of 3d articulated hand posture[A]. IEEE conference oncomputer vision and pattern recognition[A], 2014: 3786-3793.[4] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification withdeep convolutional neural networks[A]. Advances in neural informationprocessing systems[C], 2012: 1097-1105.[5] Zhou E, Cao Z, Yin Q. Naive-Deep Face Recognition: Touching theLimit of LFW Benchmark or Not?[J]. Computer Science, 2015.[6]   Sharp  T,  Keskin C,  Robertson  D, et  al.  Accurate, robust,  and  flexible real-time  hand tracking[A].  Proceedings of  the  33rd Annual  ACM  Conference on Human  Factors in Computing Systems. ACM[C], 2015: 3633-3642.[7] Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields.2017[8] Oberweger  M  , Wohlhart  P  , Lepetit  V  . Hands  Deep  in Deep  Learning  for Hand  Pose Estimation[J]. ComputerScience, 2015.推荐阅读全球Python调查报告：Python 2正在消亡，PyCharm比VS Code更受欢迎看他那台笔记本，盲猜是个程序员来了来了！趋势预测算法大PK附代码 | OpenCV实现银行卡号识别，字符识别算法你知多少？没错，你离分布式搜索只差一个Elasticsearch入门重磅！阿里巴巴开源首个边缘计算云原生项目 OpenYurt区块链共识算法总结 | 原力计划你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106435171
39			['Python正则表达式，看这一篇就够了_AI科技大本营-CSDN博客']			作者 | 猪哥来源 | 裸睡的猪（ID: IT--Pig）大多数编程语言的正则表达式设计都师从Perl，所以语法基本相似，不同的是每种语言都有自己的函数去支持正则，今天我们就来学习 Python中关于 正则表达式的函数。re模块主要定义了9个常量、12个函数、1个异常，每个常量和函数猪哥都会通过实际代码案例讲解，让大家能更直观的了解其作用！注：为避免出现代码格式错乱，猪哥尽量使用代码截图演示哦。re模块简介聊到Python正则表达式的支持，首先肯定会想到re库，这是一个Python处理文本的标准库。标准库的意思表示这是一个Python内置模块，不需要额外下载，目前Python内置模块大概有300个。可以在这里查看Python所有的内置模块：https://docs.python.org/3/py-modindex.html#cap-r因为re是内置模块，所以不需要再下载，使用时直接引入即可：import rere模块官方文档：https://docs.python.org/zh-cn/3.8/library/re.htmlre模块库源码：https://github.com/python/cpython/blob/3.8/Lib/re.pyre模块常量常量即表示不可更改的变量，一般用于做标记。re模块中有9个常量，常量的值都是int类型！上图我们可以看到，所有的常量都是在RegexFlag枚举类来实现，这是在Python 3.6做的改版。在Python 3.6以前版本是直接将常量写在re.py中，使用枚举的好处就是方便管理和使用！下面我们来快速学习这些常量的作用及如何使用他们，按常用度排序！1. IGNORECASE语法：re.IGNORECASE 或简写为 re.I作用：进行忽略大小写匹配。代码案例：在默认匹配模式下大写字母B无法匹配小写字母b，而在 忽略大小写 模式下是可以的。2. ASCII语法：re.ASCII 或简写为 re.A作用：顾名思义，ASCII表示ASCII码的意思，让\w,\W,\b,\B,\d,\D,\s和\S只匹配ASCII，而不是Unicode。代码案例：在默认匹配模式下\w+匹配到了所有字符串，而在ASCII模式下，只匹配到了a、b、c（ASCII编码支持的字符）。注意：这只对字符串匹配模式有效，对字节匹配模式无效。3. DOTALL语法：re.DOTALL 或简写为 re.S作用：DOT表示.，ALL表示所有，连起来就是.匹配所有，包括换行符\n。默认模式下.是不能匹配行符\n的。代码案例：在默认匹配模式下.并没有匹配换行符\n，而是将字符串分开匹配；而在re.DOTALL模式下，换行符\n与字符串一起被匹配到。注意：默认匹配模式下.并不会匹配换行符\n。4. MULTILINE语法：re.MULTILINE 或简写为 re.M作用：多行模式，当某字符串中有换行符\n，默认模式下是不支持换行符特性的，比如：行开头 和 行结尾，而多行模式下是支持匹配行开头的。代码案例：正则表达式中^表示匹配行的开头，默认模式下它只能匹配字符串的开头；而在多行模式下，它还可以匹配 换行符\n后面的字符。注意：正则语法中^匹配行开头、\A匹配字符串开头，单行模式下它两效果一致，多行模式下\A不能识别\n。5. VERBOSE语法：re.VERBOSE 或简写为 re.X作用：详细模式，可以在正则表达式中加注解！代码案例：默认模式下并不能识别正则表达式中的注释，而详细模式是可以识别的。当一个正则表达式十分复杂的时候，详细模式或许能为你提供另一种注释方式，但它不应该成为炫技的手段，建议谨慎考虑后使用！6.LOCALE语法：re.LOCALE 或简写为 re.L作用：由当前语言区域决定\w,\W,\b,\B和大小写敏感匹配，这个标记只能对byte样式有效。这个标记官方已经不推荐使用，因为语言区域机制很不可靠，它一次只能处理一个 “习惯”，而且只对8位字节有效。注意：由于这个标记官方已经不推荐使用，而且猪哥也没使用过，所以就不给出实际的案例！7.UNICODE语法：re.UNICODE 或简写为 re.U作用：与 ASCII 模式类似，匹配unicode编码支持的字符，但是 Python 3 默认字符串已经是Unicode，所以有点冗余。8. DEBUG语法：re.DEBUG作用：显示编译时的debug信息。代码案例：虽然debug模式下确实会打印编译信息，但猪哥并不理解这是什么语言 以及表达的含义，希望了解的朋友能不吝赐教。9.TEMPLATE语法：re.TEMPLATE  或简写为 re.T作用：猪哥也没搞懂TEMPLATE的具体用处，源码注释中写着：disable backtracking(禁用回溯)，有了解的同学可以留言告知！10. 常量总结9个常量中，前5个（IGNORECASE、ASCII、DOTALL、MULTILINE、VERBOSE）有用处，两个（LOCALE、UNICODE）官方不建议使用、两个（TEMPLATE、DEBUG）试验性功能，不能依赖。常量在re常用函数中都可以使用，查看源码可得知。常量可叠加使用，因为常量值都是2的幂次方值，所以是可以叠加使用的，叠加时请使用|符号，请勿使用+符号！最后来一张思维导图总结一下re模块中的常量吧，需要高清图或者xmind文件的同学可在文章末尾获取。re模块函数re模块有12个函数，猪哥将以功能分类来讲解；这样更具有比较性，同时也方便记忆。1.查找一个匹配项查找并返回一个匹配项的函数有3个：search、match、fullmatch，他们的区别分别是：search： 查找任意位置的匹配项match： 必须从字符串开头匹配fullmatch： 整个字符串与正则完全匹配我们再来根据实际的代码案例比较：案例1:案例1中search函数是在字符串中任意位置匹配，只要有符合正则表达式的字符串就匹配成功，其实有两个匹配项，但search函数值返回一个。而match函数是要从头开始匹配，而字符串开头多了个字母a，所以无法匹配，fullmatch函数需要完全相同，故也不匹配！案例2:案例2删除了text最开头的字母a，这样match函数就可以匹配啦，而fullmatch函数依然不能完全匹配！案例3:案例3中，我们只留下一段文字，并且与正则表达式一致；这时fullmatch函数终于可以匹配了。完整案例：注意：查找 一个匹配项 返回的都是一个匹配对象（Match）。2.查找多个匹配项讲完查找一项，现在来看看查找多项吧，查找多项函数主要有：findall函数与finditer函数：findall： 从字符串任意位置查找，返回一个列表finditer：从字符串任意位置查找，返回一个迭代器两个方法基本类似，只不过一个是返回列表，一个是返回迭代器。我们知道列表是一次性生成在内存中，而迭代器是需要使用时一点一点生成出来的，内存使用更优。如果可能存在大量的匹配项的话，建议使用finditer函数，一般情况使用findall函数基本没啥影响。3.分割re.split(pattern, string, maxsplit=0, flags=0)函数：用pattern分开 string ，maxsplit表示最多进行分割次数，flags表示模式，就是上面我们讲解的常量！注意：str模块也有一个 split函数 ，那这两个函数该怎么选呢？str.split函数功能简单，不支持正则分割，而re.split支持正则。关于二者的速度如何？ 猪哥实际测试了一下，在相同数据量的情况下使用re.split函数与str.split函数执行次数与执行时间对比图：通过上图对比发现，1000次循环以内str.split函数更快，而循环次数1000次以上后re.split函数明显更快，而且次数越多差距越大！所以结论是：在 不需要正则支持 且 数据量和数次不多 的情况下使用str.split函数更合适，反之则使用re.split函数。注：具体执行时间与测试数据有关！4.替换替换主要有sub函数与subn函数，他们功能类似！先来看看sub函数的用法：re.sub(pattern, repl, string, count=0, flags=0)函数参数讲解：repl替换掉string中被pattern匹配的字符， count表示最大替换次数，flags表示正则表达式的常量。值得注意的是：sub函数中的入参：repl替换内容既可以是字符串，也可以是一个函数哦！如果repl为函数时，只能有一个入参：Match匹配对象。re.subn(pattern, repl, string, count=0, flags=0)函数与re.sub函数功能一致，只不过返回一个元组 (字符串, 替换次数)。5.编译正则对象compile函数 与template函数将正则表达式的样式编译为一个 正则表达式对象 （正则对象Pattern），这个对象与re模块有同样的正则函数（后面我们会讲解Pattern正则对象）。而template函数与compile函数类似，只不过是增加了我们之前说的re.TEMPLATE模式，我们可以看看源码。6.其他re.escape(pattern) 可以转义正则表达式中具有特殊含义的字符，比如：.或者*，举个实际的案例：re.escape(pattern)看似非常好用省去了我们自己加转义，但是使用它很容易出现转义错误的问题，所以并不建议使用它转义，而建议大家自己手动转义！re.purge() 函数作用就是清除 正则表达式缓存，具体有什么缓存呢？我们来看看源码就知道它背地里干了 什么：看方法大概是清除缓存吧，我们再来看看具体的案例：猪哥在两个案例之间使用了re.purge()函数清除缓存，然后分别比较前后案例源码里面的缓存，看看是否有变化！7.总结同样，最后来一张思维导图总结一下re模块中的函数吧，需要高清图或者xmind文件的同学可在末尾获取。re模块异常re模块还包含了一个正则表达式的编译错误，当我们给出的正则表达式是一个无效的表达式（就是表达式本身有问题）时，就会raise一个异常！我们来看看具体的案例吧：上图案例中我们可以看到，在编写正则表达式中我们多写了一个后括号，这导致执行结果报错；而且是在其他所有案例执行之前，所以说明是在正则表达式编译时期就报错了。注意：这个异常一定是 正则表达式 本身是无效的，与要匹配的字符串无关！正则对象Pattern关于re模块的常量、函数、异常我们都讲解完毕，但是完全有必要再讲讲正则对象Pattern。1.  与re模块 函数一致在re模块的函数中有一个重要的函数compile函数，这个函数可以预编译返回一个正则对象，此正则对象拥有与re模块相同的函数，我们来看看Pattern类的源码。既然是一致的，那到底该用re模块还是正则对象Pattern？而且，有些同学可能看过re模块的源码，你会发现其实compile函数与 其他re函数（search、split、sub等等） 内部调用的是同一个函数，最终还是调用正则对象的函数！也就是说下面 两种代码写法底层实现 其实是一致的：# re函数re.search(pattern, text)# 正则对象函数compile = re.compile(pattern)compile.search(text)那还有必要使用compile函数得到正则对象再去调用search函数吗？直接调用re.search 是不是就可以？2. 官方文档怎么说关于到底该用re模块还是正则对象Pattern，官方文档是否有说明呢？官方文档推荐：在多次使用某个正则表达式时推荐使用正则对象Pattern以增加复用性，因为通过re.compile(pattern)编译后的模块级函数会被缓存！3. 实际测试又如何？上面官方文档推荐我们在多次使用某个正则表达式时使用正则对象，那实际情况真的是这样的吗？我们再实测一下吧猪哥编写了两个函数，一个使用re.search函数另一个使用compile.search函数，分别(不同时)循环执行count次(count从1-1万)，比较两者的耗时！得出的结果猪哥绘制成折线图：得出的结论是：100次循环以内两者的速度基本一致，当超出100次后，使用正则对象Pattern的函数 耗时明显更短，所以比re模块要快！通过实际测试得知：Python 官方文档推荐  多次使用某个正则表达式时使用正则对象函数 基本属实！注意事项Python 正则表达式知识基本讲解完毕，最后稍微给大家提一提需要注意的点。1.字节串 与 字符串模式和被搜索的字符串既可以是 Unicode 字符串 (str) ，也可以是8位字节串 (bytes)。但是，Unicode 字符串与8位字节串不能混用！2.r 的作用正则表达式使用反斜杠（’\’）来表示特殊形式，或者把特殊字符转义成普通字符。而反斜杠在普通的 Python 字符串里也有相同的作用，所以就产生了冲突。解决办法是对于正则表达式样式使用 Python 的原始字符串表示法；在带有 ‘r’ 前缀的字符串字面值中，反斜杠不必做任何特殊处理。3.正则查找函数 返回匹配对象查找一个匹配项（search、match、fullmatch）的函数返回值都是一个匹配对象Match，需要通过match.group()获取匹配值，这个很容易忘记。另外还需要注意：match.group() 与match.groups() 函数的差别！4.重复使用某个正则如果要重复使用某个正则表达式，推荐先使用re.compile(pattern)函数返回一个正则对象，然后复用这个正则对象，这样会更快！5.Python 正则面试笔试可能会遇到需要使用Python正则表达式，不过不会太难的，大家只要记住那几个方法的区别，会正确使用，基本问题不大。文章所有内容精华猪哥已经整理成一份思维导图：链接(或阅读原文):https://pan.baidu.com/s/10MMpuf6Rcba-gvBo1oIzlw  密码:y6z3【END】今日福利遇见陆奇同样作为“百万人学 AI”的重要组成部分，2020 AIProCon 开发者万人大会将于 7 月 3 日至 4 日通过线上直播形式，让开发者们一站式学习了解当下 AI 的前沿技术研究、核心技术与应用以及企业案例的实践经验，同时还可以在线参加精彩多样的开发者沙龙与编程项目。参与前瞻系列活动、在线直播互动，不仅可以与上万名开发者们一起交流，还有机会赢取直播专属好礼，与技术大咖连麦。门票限量大放送！今日起点击阅读原文报名「2020 AI开发者万人大会」，使用优惠码“AIP211”，即可免费获得价值299元的大会在线直播门票一张。限量100张，先到先得！快来动动手指，免费获取入会资格吧！点击阅读原文，直达大会官网。你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/105648280
40			['《评人工智能如何走向新阶段》后记（再续24）_AI科技大本营-CSDN博客']			394，基于Loihi计算架构的神经拟态计算2020年4月9日英特尔中国研究院院长宋继强接受记者采访，谈到英特尔对未来计算的研究和布局问题时，他说量子计算和神经拟态计算是非常重要的新兴计算方式。他在谈到神经拟态计算时说，面向神经拟态计算，英特尔发布了Loihi新型计算架构，可以模拟人脑神经元连接构建的连接方式，将计算和存储融合，并考虑到时间序列，采用“异步脉冲”方式进行计算。关于英特尔Loihi神经拟态芯片（整合计算和存储）其主要参数：128个内核、13万个神经元、1.3亿个突触（每个神经拟态计算内核模拟1000个逻辑神经元，片上网络连接支持高效的脉冲消息分发，高度复杂的神经网络拓朴，支持多种学习模式的可扩展的片上学习能力）。在神经拟态芯片提供的底层硬件基础上有两类做法：①第一类模拟设计不变，把现有深度神经网络移植到神经拟态芯片上，需要大量数据，只是运行网络的功耗更低。②第二类涉及模型设计和算法基本原理的改变。模型设计首先需要用非深度学习的方法去做，现在配合神经拟态计算比较常用的就是脉冲神经网络（SNN）。SNN充分考虑了时间序列上的差异，在设计该网络时会模仿一些生物感知和处理节奏。比如英特尔近期发布的基于神经拟态芯片Loihi的嗅觉系统，其网络构造系统借鉴了人类嗅觉系统的结构设计。这是全新的模型设计，它不像深度学习那样需要大量数据、大量参数来达到一个稳定状态。395，依托人工智能，人脸识别需要安全加持近年来人脸识别获得广泛应用。人脸识别相比指纹、密码、语音等生物识别技术，其安全性、便捷性、可靠性都更胜一筹。但在安全防护、防伪攻击等要求更高的领域，人脸识别还需安全加持。396，开源为AI构建一个开放共进的生态环境并且帮助行业加速AI落地近年来像TensorFlow、Keras、PyTorch、Caffe、Theano、Paddle paddle、Angel、XDL等一些AI框架和平台，相继发展成开源项目；最近国内AI开源生态突然热闹起来，清华刚开源了一个强化学习平台，华为和旷视又相继开源了AI计算和深度学习框架。开源已成为人工智能领域的关键词，开源为AI行业发展提供了不可或缺的动力。开源为AI构建一个开放共进的生态环境，并帮助行业加速AI应用落地，在解决行业实际问题时持续更新和迭代，源源不断地给AI领域输送重要的技术养料和创造力。397，深度学习可实时同步扫描心脏血流英国《自然-机器智能》杂志4月13日发表一项医学人工智能研究成果：瑞士科学家研发一种人工智能深度学习模型可以在几秒之内重建并扫描心血管血流。瑞士苏黎世联邦理工学院瓦雷黑-韦诗耐韦斯基教授的研发团队研发一种深度学习模型可以实时快速同步重建并扫描心血管血流，让临床医生在患者接受四维（4D）核磁共振扫描的同时可实时观察患者血流变化，完成对医疗成像的实时评估，从而优化医疗诊断。该团队用11个扫描案例训练了一种神经网络（人工神经网络），为了可以在几秒处理时间内（通常扫描的处理时间20秒）对经过心脏的心血管血流进行四维重建（供同步实时扫描之需）。本案例也再次证明，深度学习网络当今将更多地用于物理、医学、生物、社会等人工智能数据分析领域，尚未到达天花板，再放异彩！398，可编辑神经网络（Editable neural network）如今深度神经网络广泛应用于各种任务中，从图像分类和机器翻译到人脸识别和自动驾驶汽车。在许多应用中，一个单一的模型错误可能在财务、声誉甚至危及生命等方面导致毁灭性的后果。因此当模型错误出现时，迅速纠正它们是至关重要的。在这项工作中，研究神经网络编辑问题，如何在不影响其他样本的模型行为的情况下，在特定样本上有效地修补模型的错误。也就是说，提出可编辑的训练，这是一种与模型无关的训练技术，它鼓励快速编辑所训练的模型。通过实验验证了该方法在大规模图像分类和机器翻译任务中的有效性。可查：MachineLearning（cs.LG）Submitted on1Apr2020ICLR2020https：∥github.com/editable-ICLR2020/editable399，美国多地用大疆无人机对抗新冠病毒流行无人机将实时视频导入人工智能分析软件中新泽西警方：救一条命就值了！美媒《航空周刊》4月10日刊文：中国大疆向全美供应无人机以对抗流行病。大疆正在向美国警察、消防和公共安全部门分发100套小型无人机系统。美国DroneDJ网站报导：大疆无人机依托人工智能技术在美国各地用以对抗新冠疫情：①以航拍自动评估人群密集度。对城市中公共场所人群过密时能发预警，也可监控是否有人违反居家隔离令。②喊话提醒。装有喊话器的无人机可以发出警告，并宣传保持社交距离和居家隔离的规定如在佛罗里达的代托海滩使用大疆Mavic2企业级无人机提醒公众景区已关闭。③监控犯罪。监控在疫情肆虐的荒芜街道和关门企业的犯罪活动。德克萨斯州休斯敦市的纪念村警察局规划无人机巡逻自动飞行。④送医疗用品。用无人机做快递/外卖，运送血样、药品、医疗用品。400，最新自然语言处理算法已在医疗业务中率先应用基于NLP构建医疗知识图谱近日，自然语言处理领域国际顶级会议ACL2020（Association for Computations Linguistics）论文接收结果公布，中科院自动化所3篇论文入选：在医疗对话的自动信息抽取，国际疾病分类（ICD）自动编码，ICD自动编码可解释性。这些最新NLP算法将为后续研究提供极具价值的经验和方向。NLP的医疗业务应用，在电子病历方面：电子病历已成为现代医疗的重要组成部分，但目前书写电子病历费时费力，已成为医生沉重负担。通过面向医患对话文本的信息抽取系统，可从对话中抽取出症状、检查、手术一般信息及其相应状态，这些NLP抽取出的信息将有助于医生在书写病历时减轻负担，或进一步用于病历自动生成。在临床医学决策方面：为缓解人工编码耗时费力容易出错的问题，开始研究利用机器进行自动的ICD编码，中科院自动化所语言与知识计算联合实验室的研究团队通过结合中文的语言特点，提出了一种基于空洞卷积和N一gram语言模型的ICD自动编码方法，利用空洞卷积捕获非严格匹配的语义片段证据，和利用N一gram捕获严格匹配的语义片段证据，进而二者结合使用，提升预测结果的可解释性，而可解释的结果对临床医学决策具有重要意义。在构建医疗知识图谱方面：中科院自动化所语言与知识计算联合实验室基于NLP技术构建的医疗知识图谱已储备约50万医学概念，超过169万医学术语和398万医学关系库（涵盖绝大部分药品、疾病、科室与检查，规模达国际领先水准），并在语音病历、病历生成、病历质控、辅助诊断系统等具体应用。401，如何使用Caps克服空间位置局限性如何使用GAN生成“真实”人脸大多数主流的神经网络，如卷积神经网络（Convolution al neuralnetwork，CNN）、人工神经网络（Art ificialNeural Network，ANN）等通常用于图像识别或人脸识别，胶囊神经网络（Capsule neural network，Caps）是为了克服CNN、ANN等主流神经网络的局限性（难以识别图像中的位置关系，或缺少空间分层和空间推理能力）而提出的一种新的网络架构。生成对抗网络（GenerativeAdversarial Network，GAN）较之其他类型的神经网络采用不同的学习方法，GAN的算法体系结构使用两个神经网络模块，即生成器（Generator）和判别器（Discriminator），它们相互竞争以产生所需的结果。生成器的工作是创建看起来逼真的假图像，而判别器的工作是区分真实图像和假图像。如果两者均能正常工作，则结果是能准确选择或生成真实图像。大多数主流神经网络很容易通过在原始数据中加入少量噪声而被错误地分类。CNN、ANN、Caps、GAN一般都与深度学习算法绑定，因此也统称深度神经网络，而深度学习算法是基于深度神经网络建立的。深度学习模型是从有限的数据中学习的，这是一个缺点，因此它很容易过度拟合；同样，输入和输出之间的映射几乎是线性的，即使特征空间中某个点的微小变化也可能导致数据分类错误。而基于GAN生成对抗网络的深度学习模型可以避免陷于分类错误的境地。402，使用人工智能清除系统软件（或核心软件）漏洞深度学习模型促计算机/软件生态建设开发操作系统或其他核心软件，都将带来大量漏洞（Bug），不管是开源软件还是私有闭源软件，都是如此！如果不能及时清除这些漏洞（检出、打补丁、测试，即BugFix，Patch），用户便无法正常、稳定运行该产品；清除这些漏洞不是一次性的，有些漏洞还会在运行中暴露出来，要求产品提供商隨时清除；当这些系统（或核心）软件改版升级时，需同步清除由开发带来的漏洞。这也是企业运维工作或完善生态系统的重要环节。微软使用深度学习模型清除软件漏洞，并提高安全漏洞的识别和分类水平。微软已经开发了一种系统，用自动化工具优先解决安全漏洞，能够在99%的时间里正确区分安全漏洞与非安全漏洞，并能够在97%的时间里准确识别出关键的、高优先级的安全漏洞。该系统通过AzureDevOps和GitHub知识库，对微软47000名开发人员的1300万个工作项目和Bug数据集进行训练。据估计，开发人员每1000行代码就会产生70个Bug，而修复一个Bug所需时间是编写一行代码所需时间的30倍。微软并不是唯一一家使用人工智能清除软件漏洞的科技巨头。亚马逊的CodeGuru服务，在一定程度上是针对其开发的代码审查，以便发现并解决资源泄漏和CPU周期浪费等问题。至于脸书，它开发了一个工具SapFix，在把生成的代码发送给工程师批准前它会修复好Bug。Linux内核约有2600~2780万行代码，约180个Bug;Win10操作系统约有5000万行代码，约350个Bug；安卓移动操作系统约有1200~1500万行代码，约100个Bug；TensorFlow（人工智能框架）约有200万行代码，约15个Bug。403，阿里集团达摩院AI医疗团队研发一种基于人工智能图卷积神经网络模型（CPR一GCN），催生心血管病医学影像辅助诊断应用场景，攻克医学影像深水区：自动化、高精度心脏冠脉血管识别。404，使用自然语言处理技术提升创新效率近日美国Lux research发布《人工智能和机器学习改善创新前端》的白皮书。Lux research数字产品副总裁凯文-西恩博士指出，有效利用机器学习可快速挖掘数据，减少全面分析时间，使用机器学习来提升创新速度和技术包容性，在定义成熟的人工智能和机器学习策略时需要加权考虑一些关键技术点：是否需要构建新的技术框架，使用哪些数据源，如何定义和解释技术。该书指出：使用自然语言处理（NLP）技术提升创新效率（但目前尚未得到充分开发利用）。目前产业界正在研发人工智能利用数据的高效方法，尤其关注NLP。通过NLP和主题建模，可使技术优化、竞争分析和微弱信号检测等流程得到改善，可加快海量文本数据分析。NLP带来的增速是由主题建模实现的，主题建模从文本中提取重要概念，同时大量消除与之相关的人工假设及数据偏差，关于NLP中的知识建模可使分类法来定义特定主题下关键创新领域的技术发展趋势。更多推荐：《评人工智能如何走向新阶段》后记《评人工智能如何走向新阶段》后记（再续1）《评人工智能如何走向新阶段》后记（再续2）《评人工智能如何走向新阶段》后记（再续3）《评人工智能如何走向新阶段》后记（再续4）《评人工智能如何走向新阶段》后记（再续5）《评人工智能如何走向新阶段》后记（再续6）《评人工智能如何走向新阶段》后记（再续7）《评人工智能如何走向新阶段》后记（再续8）《评人工智能如何走向新阶段》后记（再续9）《评人工智能如何走向新阶段》后记（再续10）《评人工智能如何走向新阶段》后记（再续11）《评人工智能如何走向新阶段》后记（再续12）《评人工智能如何走向新阶段》后记（再续13）《评人工智能如何走向新阶段》后记（再续14）《评人工智能如何走向新阶段》后记（再续15）《评人工智能如何走向新阶段》后记（再续16）《评人工智能如何走向新阶段》后记（再续17）《评人工智能如何走向新阶段》后记（再续18）《评人工智能如何走向新阶段》后记（再续19）《评人工智能如何走向新阶段》后记（再续20）《评人工智能如何走向新阶段》后记（再续21）《评人工智能如何走向新阶段》后记（再续22）《评人工智能如何走向新阶段》后记（再续23）			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/105656185
41			['Gitee上传代码 提示remote Incorrect username or password ( access token )_头名字W的博客-CSDN博客']			这个是第一次在使用Git进行项目代码提交的最后一步操作的时候，出现了一个问题：用户名或密码不正确（访问令牌）出现这种现象是因为之push代码的时候windows弹出一个用户名密码输入框，用户名输出了的缘故。这里的用户名其实就是你的gitee账号。解决办法是：打开电脑的控制面板–>用户账户–>管理Windows凭据找到管理Windows凭证：下面就可以看到你刚才输入的gitee的账号信息了，现在只需要[编辑]然后保存就行了最后就是重新push一下你的的代码就行了。			https://blog.csdn.net/YeShenLiaoSuiFeng/article/details/101240396
42			['清华周界详解《基于图神经网络的事实验证》 |  百万人学AI_AI科技大本营-CSDN博客']			事实验证任务要求相关系统能够从大规模的文本知识库中抽取相关的证据（Evidence）并根据这些证据对给定的声明（Claim）给出事实性的判断。在本次报告中，讲者将介绍如何将图神经网络应用到事实验证任务中。具体来说，讲者将介绍两篇相关工作，它们从句子级别和词级别使用图神经网络更好地建模证据和声明之间的关系，从而提升任务的效果。直播时间：2020年4月10日 19:30-20:30嘉宾简介：周界，清华大学计算机系硕士生，导师刘知远副教授。主要研究方向为图神经网络在自然语言处理中的应用。划重点！扫码即刻报名扫码加入课程交流群群内共享PPT和回放联合支持：中国工程院知领直播、学堂在线、CSDN			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/105400805
43			['关于Github访问速度巨慢_maaici的博客-CSDN博客']			晚上七点多开电脑干活，想克隆下做一半的项目，现在九点半了，折腾了个把小时还没好，什么情况？国内不是已经开放访问了么，为啥这么慢？今天竟然直接没办法用，能点开页面，就是克隆不下来，我琢磨着，不是克隆就不克隆吧，我下载下来还不行么，答案是不行！！！真是的一点心情都没有了！按照网上说的修改hosts我试过了，效果不明显，大家有啥其他的方法么。另外我要对github以及我的电脑，包括vs说一句：你们今天的表现真的是太菜了！！！命令行直接git clone ****.git 效率远高于在vs中拉取，或者在页面上直接下载，如果代码下不来的试试这个，前提要先安装git			https://blog.csdn.net/maaici/article/details/101313126
44			['GitHub标星2000+，如何用30天啃完TensorFlow2.0？_AI科技大本营-CSDN博客']			作者 | 梁云1991来源 | Python与算法之美（ID:Python_Ai_Road）天下苦tensorflow久矣！尽管tensorflow2.0宣称已经为改善用户体验做出了巨大的改进，really easy to use，但大家学得并不轻松。tensorflow2.0官方文档和tensorflow1.0官方文档显然是出自同一批作者之手，他们一如既往地秉承着谷歌make things complicated的风格传统，用哈希表一般混乱的文档结构、无法运行的范例代码、复杂的函数嵌套调用关系、随意插入的不常用第三方库等技巧将读者的懵圈程度逐步推向高潮。但吐槽归吐槽，到了工业界，你是无论如何也绕不开这个框架的。既然非学不可，如何让这个学习过程更加平滑舒畅呢？近日来，一个毕业于北京吃饭大学的吃货开源了一本叫做eat tensorflow2.0 in 30 days的工具书。作者声称这本书倾注了一个吃货对美食的全部向往和追求，如果你非常喜欢美食，并且想要学习TensorFlow2，那么这本书一定值得你品尝品尝。???? gitbook电子书地址：https://lyhue1991.github.io/eat_tensorflow2_in_30_days???? github项目地址：https://github.com/lyhue1991/eat_tensorflow2_in_30_days以下是该书一个较为完整的介绍。Tensorflow2还是Pytorch?先说结论:如果是工程师，应该优先选TensorFlow2.如果是学生或者研究人员，应该优先选择Pytorch.如果时间足够，最好Tensorflow2和Pytorch都要学习掌握。理由如下：1，在工业界最重要的是模型落地，目前国内的大部分互联网企业只支持TensorFlow模型的在线部署，不支持Pytorch。并且工业界更加注重的是模型的高可用性，许多时候使用的都是成熟的模型架构，调试需求并不大。2，研究人员最重要的是快速迭代发表文章，需要尝试一些较新的模型架构。而Pytorch在易用性上相比TensorFlow2有一些优势，更加方便调试。并且在2019年以来在学术界占领了大半壁江山，能够找到的相应最新研究成果更多。3，TensorFlow2和Pytorch实际上整体风格已经非常相似了，学会了其中一个，学习另外一个将比较容易。两种框架都掌握的话，能够参考的开源模型案例更多，并且可以方便地在两种框架之间切换。本书面向读者本书假定读者有一定的机器学习和深度学习基础，使用过Keras或者Tensorflow1.0或者Pytorch搭建训练过模型。对于没有任何机器学习和深度学习基础的同学，建议在学习本书时同步参考学习《Python深度学习》一书。此书是Keras之父Francois Chollet所著，假定读者无任何机器学习知识，以Keras为工具，使用丰富的范例示范深度学习的最佳实践，该书通俗易懂，全书没有一个数学公式，注重培养读者的深度学习直觉。该书电子版下载链接：https://pan.baidu.com/s/1-4q6VjLTb3ZxcefyNCbjSA 提取码：wtzo本书写作风格本书是一本对人类用户极其友善的TensorFlow2.0入门工具书，不刻意恶心读者是本书的底限要求，Don't let me think是本书的最高追求。本书主要是在参考TensorFlow官方文档和函数doc文档基础上整理写成的。但本书在篇章结构和范例选取上做了大量的优化。不同于官方文档混乱的篇章结构，既有教程又有指南，缺少整体的编排逻辑。本书按照内容难易程度、读者检索习惯和TensorFlow自身的层次结构设计内容，循序渐进，层次清晰，方便按照功能查找相应范例。不同于官方文档冗长的范例代码，本书在范例设计上尽可能简约化和结构化，增强范例易读性和通用性，大部分代码片段在实践中可即取即用。如果说通过学习TensorFlow官方文档掌握TensorFlow2.0的难度大概是9的话，那么通过学习本书掌握TensorFlow2.0的难度应该大概是3.谨以下图对比一下TensorFlow官方教程与本教程的差异。本书学习方案1.学习计划本书是作者利用工作之余和疫情放假期间大概2个月写成的，大部分读者应该在30天可以完全学会。预计每天花费的学习时间在30分钟到2个小时之间。当然，本书也非常适合作为TensorFlow的工具手册在工程落地时作为范例库参考。2.学习环境本书全部源码在jupyter中编写测试通过，建议通过git克隆到本地，并在jupyter中交互式运行学习。为了直接能够在jupyter中打开markdown文件，建议安装jupytext，将markdown转换成ipynb。如果看到如下输出，则说明tensorflow已经成功安装并运行，接下来就可以愉快地开始课程学习了。tensorflow version: 2.1.0hello tensorflow2如何获取本书这本书目前有4种形式获取。1，gitbook电子书。以网页链接呈现，同时可以在电脑和手机上用浏览器打开。???? gitbook电子书地址：https://lyhue1991.github.io/eat_tensorflow2_in_30_days2，github项目源码。包含全部数据集和md格式源码，可以在jupyter上安装jupytext后将md源码作为ipynb打开。???? github项目地址：https://github.com/lyhue1991/eat_tensorflow2_in_30_days3，pdf格式电子书。4，ipynb格式项目源码。其中github项目源码和gitbook电子书将持续维护。阅读体验优先推荐使用gitbook电子书，既可以在手机也可以在电脑上查看，具有目录查找和上下页翻页功能，字体大小和背景色可以根据个人喜好进行调整，颜值超高。【End】推荐阅读半小时训练亿级规模知识图谱，亚马逊AI开源知识图谱嵌入表示框架DGL-KE首次揭秘！大麦如何应对超大规模高性能选座抢票？AI 四巨头 Google、DeepMind、Microsoft、Uber 深度学习框架大比拼马化腾、马云并列成为中国首富；百度回应“将上线电商直播”；.NET 5 Preview 2 发布 | 极客头条程序员职场背锅甩锅指南警惕！新骗术出现：这些虚假二维码生成器已成功盗取 4.6 万美元！“出道” 5 年采用率达 78%，Kubernetes 的成功秘诀是什么？你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/105400807
45			['搞机器学习，Python和R哪个更合适？_AI科技大本营-CSDN博客']			【编者按】如果你正想构建一个机器学习项目，但却纠结于如何选择编程语言，这篇文章将是你所需要的。这篇文章不仅帮助你理解Python和R这两种语言的区别，还有助于你了解各个语言多方面的优势。作者 | Manav Jain译者 | Joe，编辑 | 夕颜来源 | CSDN（ID:CSDNnews）R和Python有着相同的编程特性，同时都是数据科学家常用的工具。在机器学习领域，大约有69%的开发者使用Python，另外24%的开发者使用R。这两种语言都是开源的，因此都是免费的。但是，Python是被设计成一种广泛适用的编程语言，但R是被用于统计分析。人工智能和数学分析是开源工具创新的两大热门领域。Python和R都营造了很好的开源生态环境，这有利各个水平的数据科学家更有效地完成科学工作。机器学习和数据分析之间的区别随着时间的推移都是相对变化，但是，其主要的区别是机器学习偏向模型解释，而数据分析侧重解释和事实推测。在不断增长的质疑声中，Python在机器学习领域获得了一席之地。R则作为一门用于事实推断和统计推断的语言在数据分析领域享有盛誉。这并不意味着必须把这两个语言划分到不同的领域 ——python也足以被用来作为数据分析的工具，R也具备充分的适应性可以完成机器学习中的重要工作。这两种语言各自有大量的库试图去完成对方的功能。Python具有可以提升其显著推断能力的库，R也有提升其预测精度的库。接下来的文章将进一步讨论两种语言的细节之处，这将非常有助于你选择最适合你当前项目的编程语言。PythonPython诞生于上世纪80年代，并且在Google内部框架中承担重要角色。Python有着饱含激情的设计者团队，同时现在它又被广泛应用于Youtube、Instagram、Quora、和Dropbox。Python已经在IT领域获得了广泛的应用，同时其在协调内部团队工作出色的表现也获得了认可。因此，如果你需要一个多功能的编程语言，同时有设计者维护的强大生态环境，Python将是你的不二之选。Python的优势：通用编程语言 —— Python是一个更好的选择，如果你的企业不需要统计方面的功能。比如构建一个网站。平稳的学习进度 —— Python是一个上手学习比较容易的编程语言。大量常用库 —— Python号称有着数不尽的库可以用来处理数据。比如Scikit-learn就包含了用于数据挖掘和分析的工具。另外Pandas设计团队提供的无法比拟的结构和信息处理功能可以显著提升开发效率。如果你所在的团队特别要求使用R中某个独特的功能，那么RPy2是个合适的选择。更好的整合 —— 通常，在任何设计场景下，Python是优于R的。无论设计者是否可能错误地使用底层语言比如C，C++，或者java，Python wrapper都可以更好地把各个部分整合在一起。另外，数据研究者使用基于python构建完成后续的工作并不难。促进生产效率 —— Python语法非常容易理解，并且和其它编程语言一样，反正跟R不相伯仲。这保证了开发团队的高效率生产。Python的劣势：通用仓库的缺失，某些R库没有对应的python包。由于动态组合，在某些情况下，它会搜寻某个功能并陷入这个缺陷，伴随着各种各样数据错误的任务。RR是由统计学家为数据分析者所创造的，任何一位工程师看一下它的语法就能明白。如果你需要提升细节的理解和创造性地开发，那么R是一个正确的选择，因为R包含的科学计算与基于统计分析的机器学习相关。如果你的工作需要更深入的理解，这时R是一个非常不错的选择。它可以用于不断提升你对工作的理解，只需要一次调用数据库。比如你想要通过把段落内容拆分成单词或者短语来分析一个语料库，从而理解这些例子，那么R就是你最好的选择。R的优势适用于数据分析 —— 如果数据检验或数据表示对你们企业非常重要，那么R将会是你最好的选择，因为用它可以快速实现原型开发设计，并和数据集一起可以构建人工智能/机器学习模型。大量实用库和工具 ——和python一样，R包含大量库可以帮助使用机器学习的企业。比如，Caret不同寻常的功能使其非常高效，这也提升了R在人工智能方面的功能。还在不断开发的数据分析库给R使用者带来了巨大的优势。这些数据分析库不仅全面，而且专注于模型认证和信息表示。适用于探索性任务 —— 如果你在项目的前期阶段需要在模型验证方面进行一些探索性的工作，那么R会让工作变得简单，因为工程师们只需要写几行代码就可以了。R的劣势学习难度大，并且容易写出错误的代码。弱类型是危险的，函数都有着返回非期望类型对象的恶习。相较于其它编程语言的独特之处：向量的索引是从1开始，而不是0。解决某些问题的语法并不是那么明显。由于R有着大量的库，一些并不常用的库并没有完善的文档说明。结论对于机器学习，Python 和R都有它们各自的优势，因为它们都有大量的库。如果你能够很好地掌握这两种语言，你就能够成为集其大成者，因为其中一个语言可以完成的大多数功能，另一个语言也可以完成。此外，你可以使用Python作为前期阶段的数据处理，之后再把这些信息送入R进行分析处理。R可以提供全面的、更优的数据分析方案。你可以把R当作是Python的一个库，或者把Python当作是R的一个用于预处理的库。掌握了python和R各自的优缺点，现在你可以更好地选择一个最适合于你目前项目的编程语言。原文链接：https://towardsdatascience.com/python-vs-r-which-is-good-for-machine-learning-ecfb87c7f8ca作者简介：Manav Jain，技术顾问与博客专家。推荐阅读量子计算与AI“双拳”出击，他们锁定38种潜在抗疫药物绝悟之后再超神，腾讯30篇论文入选AI顶会ACL微信提出推荐中的深度反馈网络，在“看一看”数据集上达到SOTA平安科技王健宗：所有 AI 前沿技术，都可以在联邦学习中大展身手2 年 6 个月 11 天，外包到阿里的修仙之路 | 原力计划在 520 这天，竟然有人把 Docker讲清楚了？斗地主吗？能学区块链那种！ | 原力计划你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106345465
46			['大促下的智能运维挑战：阿里如何抗住“双11猫晚”？_AI科技大本营-CSDN博客']			作者 | 阿里文娱技术专家子霖出品 | AI科技大本营（ID:rgznai100）2019 双 11 猫晚在全球近 190 个国家和地区播出，海外重保是首要任务，如何提升海外用户观看猫晚的体验？本文将详解双 11 猫晚国际化的技术挑战和技术策略。播前成功率改进1. 海外主站链路优化计算猫晚海外 CDN 带宽用量，明确海外直播 CDN 资源分布及其调度回源链路，结合现有 直播赛事，分析海外各地域国家直播卡顿率，重点分析卡顿率高的地域。对比访问主站各地域国家 TCP 建连时间，调整主站 TCP 建连时间。2. 直播服务单元化1)为什么要做单元化用户体验及资源瓶颈：随着业务体量和服务用户群体的增长，用户需要更优的访问速度，单机房无法支持长期的服务的持续扩容；服务异地容灾:异地容灾已经成为核心服务的标配，有些服务虽然进行了多地多机房部署， 但数据还是只在中心机房。要实现真正意义上的异地多活，就需要对服务进行单元化改造；全球化战略:全球化战略带来的不只是用户增长，同时数据也会快速增长，全球数据都集 中部署在少数几个机房显然不太现实，基于地理区域划分、数据维度驱动的单元化架构是未来 全球化战略的一个技术方案储备。2)单元化收益一是容灾:多地域容灾。任何一个城市异常，核心交易下单行为能够在秒级内全部恢复， 非核心行为在 2 小时内恢复;中心有全量的数据，单元的数据写入后会同步到中心;若单元故 障，单元提供的交易流量会切换到中心;如果是中心故障，中心会切换到中心备份环境；二是扩展性:单元化后，系统扩展不受机房的部署和资源限制，可在不同地域选址横向扩展来满足系统日益增长的需求;在多地部署应用后，按照就近接入原则:用户请求落到离它最近的站点，提升用户体验；三是稳定性:单元快速部署和验证，可在一个空的单元引流来验证功能，降低系统风险; 可以对一个空站点做全链路压测，能快速得到站点容量；四是成本:减少系统对机房部署的强依赖，提供更大的灵活性;能够将单元的规模，所需 的资源信息确定下来，用环境的标准化快速部署来节省成本。3. 部署压测兜底演练在直播单元压测完成，扩容完成后，进入直播链路专项压测阶段。在这个阶段，对存在性 能风险的场景和链路进行放大流量的集群层面的压力探测，对集群层面的指标 CPU(avg/max)、Load(avg/max)、RT(avg/max)等进行监控，并对直播入口首页播放页兜底压测，力求发现系统潜 在风险。2019 年优酷 Java 应用测试团队梳理出多个存在性能风险的链路和场景，进行了专项压 测保证。单链路专项压测的压测方式不尽相同，这一部分是最灵活和最有效的性能与稳定性筛 查。4. 成功率验证多场次直播拉取数据对比验证各场次播前成功率是否符合预期，分析不符合预期的国家进 行相应的回源调度调整。双 11 当天整体表现稳定，接入层和各上云应用 OPS、成功率、RT 均 符合预期。卡顿率改进1. 开启智能档今年的双 11 天猫晚会，优酷 APP 上清晰度列表中有了智能清晰度。智能清晰度是什么， 简单来讲，就是自动调整实际播放的清晰度，而调整的依据就是用户当前的 IP，网络状态等信 息，结合用户所处网络环境为用户选择合适的清晰度，减少卡顿，并提升播放体验。智能档， 也就是码率自适应，优酷在点播场景下已经用了起来且相对较成熟，而今年的双 11，我们将其 应用在直播的场景下，进一步提升用户的播放体验。猫晚海外各端默认全开智能档。目前优酷 的码率自适应实现，基于主流的 HLS 协议，进行分片级别的切档。2. 直播链路优化优化了协议栈，客户端播放使用 HLS 协议，开启 TS 预热。CDN 边缘节点调度优化，边缘节点回源按地域国家统一调度到指定的二级回源节点。3. L2 节点优化L2 节点回源站，优先走高速通道。高速通道故障后，切到备用链路。其中东南亚国家 L2回源链路由公网回源切到了香港 L2 高速通道回源。保障了直播回源链路的稳定。4. 卡顿率验证通过开启智能档，调整边缘节点的调度以及 L2 回直播源站的链路。通过多场次直播验证， 整个播放卡顿率比预期得到极大的改善。资源成本效能通过建站平台完成海外直播单元一键建站，单机压测平台输出单机能力，从而评估出应用 扩容所需资源。将所有扩容应用输入平台，一键完成应用扩容。切流平台保障单元间容灾时平 滑切换。这么复杂的直播链路，通过智能业务链路，对上千个接口进行自动监测、梳理以及容 量的自动评估。其节点部署及交付是小时级别，从成本、用户体验上领跑行业，让海外播放页 面及互动游戏体验和国内一致。有了上述对关键点的梳理，那么一套服务于大促资源全生命周期保障的平台系统 就应运而 生。确定目标如下:1)资源需求:需求收集-单机能力评估-历史活动数据对比-工单生成-批量扩容执行-压测调 整-结束资源回收，全链路 100%平台化流程化;2)单机压测:单机压测能力覆盖所有大促应用，实时提供最精准的能力数据;3)业务巡检:资源健康度巡检，低水位低利用率低 OPS 应用数据输出，自动资源回收，提升资源利用率。1. 一键扩缩容非核心应用资源动态支援能力，10 分钟 1000 台回收交付能力; 整体能力分为两个大类:资源需求线和资源保障线。2. 总体流程明确需求收集范围->改进需求收集方法->实现单机能力自动获取->实现历史容量数据自动 获取->应用上下游依赖链路自动获取-> [业务目标->技术目标转换]->完成资源需求评估。实现需 求上报渠道能力，单机实时压测能力，目标转化能力，上下游链路及流量平衡自动评估能力。3. 资源保障总体流程建设整体资源容量盘点能力->建设应用级别线上水位巡检能力->优化资源快速交付/调整能 力->建设非核心应用应急容量挪用能力 ->快速回收资源 ->完成闭环的生命周期，实现高效交 付，强化资源盘点及 buffer 保障的能力。重保预案及措施1. 主站重报预案及措施接入层水位高:水位达到预定阀值执行自动扩容。不需要人工干涉; 用户超预期:降低码率，执行限流; 单机房出口故障:平台自动执行切流，切到正常的机房; 单元公网出口故障:切流平台执行切流，切到正常单元。2. CDN 重保预案及措施预案执行演练、兜底策略执行演练、大盘实时监控、快速故障处理应急小组。项目总结每次参加双 11 战役，都收获颇丰。精准的计算资源用量、技术方案评审到落地，都要经过多次反复验证，通过每年的双 11 技术沉淀到平台，大大减轻了人力成本。单机压测平台提供单机的能力从而计算出集群的水位，建站平台提供主站及单元部署的一键建站；扩容平台提供大 促一键扩容及一键缩容能力，切流平台提供机房容灾切换能力，运维机器人提供各监控项的快速查看；资源成本平台提供大促成本能力。后面平台优化的地方还很多，从主站到 CDN 实现全链路无缝对接，借助双 11 充分发挥平台优势。【END】今日福利遇见陆奇同样作为“百万人学 AI”的重要组成部分，2020 AIProCon 开发者万人大会将于 7 月 3 日至 4 日通过线上直播形式，让开发者们一站式学习了解当下 AI 的前沿技术研究、核心技术与应用以及企业案例的实践经验，同时还可以在线参加精彩多样的开发者沙龙与编程项目。参与前瞻系列活动、在线直播互动，不仅可以与上万名开发者们一起交流，还有机会赢取直播专属好礼，与技术大咖连麦。门票限量大放送！今日起点击阅读原文报名「2020 AI开发者万人大会」，使用优惠码“AIP211”，即可免费获得价值299元的大会在线直播门票一张。限量100张，先到先得！快来动动手指，免费获取入会资格吧！点击阅读原文，直达大会官网。你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/105648271
47			['What?! Python一行代码，能玩这么多童年的游戏？_AI科技大本营-CSDN博客']			来源 | 早起 Python责编 | Carol封图 |  CSDN 下载自视觉中国儿童节就要来了，虽然秃头程序员没有头发，但是童心还是一直都在的，今天就分享一个私藏的GitHub项目——free-python-games，一行代码就能进入使用Python开发的小游戏快乐玩耍！安装与使用安装当然也很简单一行代码就可以pip install freegames由于该项目中的所有游戏均是基于Python内置模块Turtle制作，所以没有太多依赖，安装不会有困难。安装完之后我们可以使用python -m freegames list来查看所有的游戏列表贪吃蛇现在我们可以使用一行代码启动相关游戏，比如贪吃蛇snakepython -m freegames.snake贪吃蛇的玩法想必不用过多解释了，使用键盘⬆️⬇️⬅️➡️即可操控吃豆人吃豆人没玩过也应该听过，使用下面的代码可以启动一个类似吃豆人的游戏python -m freegames.pacmanFlappyFlappy这个游戏和之前非常火的Flappy bird十分类似只需要更换游戏名即可启动python -m freegames.flappy这次需要不断点击鼠标来控制绿色小圆点飞行，实测比flappy bird难度要更高一点MemoryMemory翻译过来是记忆，该游戏给出一些宫格，点击每一个小宫格会显示背后藏的数字，但会快速消失，当你成功选中两个相同的数字之后就会显示为拼图，可玩性还是很高的python -m freegames.memory迷宫这个游戏应该就不用介绍了，找出走出迷宫对应的路径即可python -m freegames.mazeTic Tac Toe这个游戏我打赌你一定玩过，单击屏幕即可放置一个X或O，当三个同样的图案在一条直线上就赢了python3 -m freegames.tictactoe查看源码还有更多的游戏这里就不再一一介绍，Turtle我们并不陌生，所以当我们玩游戏时应该思考开发者是怎样实现的。使用下面的代码即可在当前目录下生成对应游戏的源码python3 -m freegames copy snake执行之后桌面就多了一个snake.py，打开就能查看游戏对应的逻辑现在我们就可以学习源码并进行修改来实现更多的功能与玩法，抓紧试试吧！6月2日20:00，CSDN 创始人&董事长、极客帮创投创始合伙人蒋涛携手全球顶级开源基金会主席、董事，聚焦中国开源现状，直面开发者在开源技术、商业上的难题，你绝不可错过的开源巅峰对谈！立即免费围观：推荐阅读全球Python调查报告：Python 2正在消亡，PyCharm比VS Code更受欢迎看他那台笔记本，盲猜是个程序员来了来了！趋势预测算法大PK附代码 | OpenCV实现银行卡号识别，字符识别算法你知多少？没错，你离分布式搜索只差一个Elasticsearch入门重磅！阿里巴巴开源首个边缘计算云原生项目 OpenYurt区块链共识算法总结 | 原力计划你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106449562
48			['Maven 初学+http://mvnrepository.com/_凯凯的博客-CSDN博客']			了解maven是一款服务于java平台的自动化构建工具(项目管理工具)构建:全方位、多角度、深层次地建立项目构建是一个项目从：源代码、编译、测试、打包、部署、运行的过程用来解决团队开发遇到的问题如:统一 jar包统一配置文件,单元测试 的代码位置即目录结构的统一传统的项目构建过程1）在eclipse中创建一个java web工程2）在工程中编写源代码及配置文件等3）对源代码进行编译，java文件编译成class文件4）执行Junit单元测试5）将工程打成war包部署至tomcat运行maven项目 构建过程maven将项目构建的过程进行标准化，每个阶段使用一个命令完成优点:一个命令完成构建、运行，方便快捷。maven对每个构建阶段进行规范，非常有利于大型团队协作开发。1.3 什么是依赖管理什么是依赖？一个java项目可能要使用一些第三方的jar包才可以运行，那么我们说这个java项目依赖了这些第三方的jar包。什么是依赖管理？就是对项目所有依赖的jar包进行规范化管理。总结使用maven的好处一步构建 命令执行,标准化依赖管理 安全又方便跨平台 电脑 手机都可用团队开发 有规程 效率高,成本低 大家都喜欢用它核心文件pom.xml<project xmlns="http://maven.apache.org/POM/4.0.0"xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"><modelVersion>4.0.0</modelVersion><groupId>com.xrq.withmaven</groupId><artifactId>withmaven</artifactId><version>0.0.1-SNAPSHOT</version><build/></project>1、modelVersion指定了当前Maven模型的版本号，对于Maven2和Maven3来说，它只能是4.0.02、groupId顾名思义，这个应该是公司名或是组织名。一般来说groupId是由三个部分组成，每个部分之间以".“分隔，第一部分是项目用途，比如用于商业的就是"com”，用于非营利性组织的就　　是"org"；第二部分是公司名， 如"tengxun"、“baidu”、“alibaba”；第三部分是你的项目名3、artifactId可以认为是Maven构建的项目名，比如你的项目中有子项目，就可以使用"项目名-子项目名"的命名方式4、version版本号，SNAPSHOT意为快照，说明该项目还在开发中，是不稳定的版本。在Maven中很重要的一点是，groupId、artifactId、version三个元素生成了一个Maven项目的基本坐标，这非常重要，我在使用和研究Maven的时候多次感受到了这点。在上面的这些元素之外，还有一些元素，同样罗列一下：1、packing项目打包的类型，可以使jar、war、rar、ear、pom，默认是jar2、dependencies和dependency前者包含后者。前面说了，Maven的一个重要作用就是统一管理jar包，为了一个项目可以build或运行，项目中不可避免的，会依赖很多其他的jar包，在Maven中，这些依赖就被称为dependency。本地仓库和远程仓库的概念 如下* 本地仓库   * 远程仓库     * 中央仓库     * 私服  //自己的架构包 放在上面     * 其他公共库  //别人的本地仓库 执行maven命令时才创建maven 本地储存的位置,有很多架构包官方下载的本地仓库的配置在"%MAVEN_HOME%\conf\settings.xml"里面，找一下"localRepository"就可以了；setting.xml:改两个地方确定 本地仓库位置<localRepository>E:\maven\repository</localRepository>为了提高下载速度而 配置的阿里云镜像<mirrors><mirror><id>alimaven</id><name>aliyun maven</name><url>http://maven.aliyun.com/nexus/content/groups/public/</url><mirrorOf>central</mirrorOf></mirror></mirrors>MyEclipse默认的本地仓库的地址在**"{user.home}/.m2/repository"路径下**，同样找一下"localRepository"就可以找到MyEclipse默认的本地仓库了。安装 配置环境首先去Maven官网，下载Maven的包，地址为http://maven.apache.org/download.cgi，找到下面的部分，点击就可以下载将maven解压到一个不含有中文和空格的目录中。bin目录 mvn.bat （以run方式运行项目）、 mvnDebug.bat（以debug方式运行项目 ）boot目录 maven运行需要类加载器conf目录 settings.xml 整个maven工具核心配置文件lib目录 maven运行依赖jar包电脑上需安装java环境，安装JDK1.7 + 版本 （将JAVA_HOME/bin 配置环境变量path ）配置 MAVEN_HOME%MAVEN_HOME%/bin 加入环境变量 pathmvn -v命令检查 maven是否安装成功maven仓库的作用本地仓库和远程仓库是这样的，Maven工程首先会从本地仓库中获取jar包，当无法获取指定jar包时，本地仓库会从远程仓库（中央仓库）中下载jar包，并放入本地仓库以备将来使用。优先从本地仓库查找默认本地仓库位置在u                         s                         e                         r                         .                         d                         i                         r                              /                      .                      m                      2                      /                      r                      e                      p                      o                      s                      i                      t                      o                      r                      y                      ，                          {user.dir}/.m2/repository，user.dir/.m2/repository，{user.dir}表示windows用户目录。远程仓库如果本地需要插件或者jar包，本地仓库没有，默认去远程仓库下载。远程仓库可以在互联网内也可以在局域网内。中央仓库在maven软件中内置一个远程仓库地址http://repo1.maven.org/maven2 ，它是中央仓库，服务于整个互联网，它是由Maven团队自己维护，里面存储了非常全的jar包，它包含了世界上大部分流行的开源项目构件。定义maven坐标每个maven工程都需要定义本工程的坐标，坐标是maven对jar包的身份定义，比如：入门程序的坐标定义如下：<!--项目名称，定义为组织名+项目名，类似包名--><groupId>cn.atcast.maven</groupId><!--模块名称--><artifactId>maven-first</artifactId><!--当前项目版本号，snapshot为快照版本即非正式版本，release为正式发布版本--><version>0.0.1-SNAPSHOT</version><packaging>：打包类型	jar：执行package会打成jar包	war：执行package会打成war包	pom ：用于maven工程的继承，通常父工程设置为pom第一个maven需求创建一个web工程，实现入门程序的功能。1）添加index.jsp，输出hello world2）添加一个servlet转发到jsp页面。① 创建约定的目录结构（maven工程必须按照约定的目录结构创建）根目录：工程名|—src：源码|—|---main:存放主程序|—|---|—java：java源码文件|—|---|—resource：存放框架的配置文件|—|---test：存放测试程序|—pop.xml：maven的核心配置文件我们按照上面的文件夹目录结构手动创建一下，不用任何IDE环境（手动的其实最有助于我们理解maven）出错:项目的结构 也不对 检查配置 和setting.xml 是否出错pom.xml报错缺少 web.xml----aa\src\main\webapp\WEB-INFweb.xml<?xml version="1.0"encoding="UTF-8"?><web-app xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"xmlns="http://java.sun.com/xml/ns/javaee"xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd"version="2.5"><welcome-file-list><welcome-file>index.html</welcome-file></welcome-file-list><servlet><description></description><display-name>HelloServlet</display-name><servlet-name>HelloServlet</servlet-name><servlet-class>cn.atcast.web.HelloServlet</servlet-class></servlet><servlet-mapping><servlet-name>HelloServlet</servlet-name><url-pattern>/HelloServlet</url-pattern></servlet-mapping></web-app>NewFile.xml报错 (新建一个xml 显示123123)解决:1.2.3. pom.xml 包配置+ 添加servelt/jsp的包<dependencies><dependency><groupId>javax.servlet</groupId><artifactId>servlet-api</artifactId><version>2.5</version><scope>provided</scope></dependency><dependency><groupId>javax.servlet.jsp</groupId><artifactId>jsp-api</artifactId><version>2.0</version><scope>provided</scope></dependency><dependency><groupId>junit</groupId><artifactId>junit</artifactId><version>4.9</version><scope>test</scope></dependency></dependencies>目录问题解决 后 让他跑起来吧–>Run As案例二:①、配置选择菜单windows–>preferences（参数）–>maven选择Installations（安装），添加你自己下载并解压好的maven目录。并打上对勾 √，点击Apply（应用）再选择User Settings目录，在User Settings中选择Browse（浏览），选择你自己maven里的conf下的settings.xml文件。插一句：settings.xml这个配置文件，主要是配置你本地仓库的路径的。不想使用默认路径，就打开文件，加上自己的路径配置。**<localRepository>C:Program FilesJavarepository</localRepository>到此，maven整个的设置就OK了。**项目就创建完成后，但是jdk的版本还有sevlet-api等jar包还没有选择创建好的工程单击右键，选择properties 并找到 Java Build Path，把jdk的版本选择你电脑上的正确的jdk版本。选择创建好的工程单击右键，选择properties 并找到 Project Facets，版本选择3.1，下面的java版本选择1.8，点击Apply选择创建好的工程单击右键，找到build path找到Libaries，添加Tomcat8.5的依赖库，点击OK错误解决1.手动安装Eclipse maven-archetype-quickstart下载C:\Users\DELL然后从CMD里手动安装。mvn install:install-file -DgroupId=org.apache.maven.archetypes -DartifactId=maven-archetype-quickstart -Dversion=X.X -Dpackaging=jar -Dfile=maven-archetype-quickstart-1.1.jar2解决一:解决二:案例三(创建一个service)我们在之前(案例一)的基础上 在web.xml文件 配置一下<servlet><description></description><display-name>HelloServlet</display-name><servlet-name>HelloServlet</servlet-name><servlet-class>cn.atcast.web.HelloServlet</servlet-class></servlet><servlet-mapping><servlet-name>HelloServlet</servlet-name><url-pattern>/HelloServlet</url-pattern></servlet-mapping>Run As---->Maven buildPOM文件内容：(重点)定义:pom: 项目对象模型 ,是一个 XML 文件 包含了对象是 使用maven来构建 ,每个项目只有一个pom.xml文件概念:父POM 类似Object类<?xml version="1.0" ?><project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">    <modelVersion>4.0.0</modelVersion>    <groupId>com.hzg.maven</groupId>    <artifactId>Hello</artifactId>    <version>0.0.1-SNAPSHOT</version> 工程版本号    <name>Hello</name>   <url>http://maven.apache.org</url>project:工程的根标签modelVersion:pom模板版本groupId:组织标识 (一般唯一)artifactId:工程名称packaging:打包方式:JAR ,WAR ,EAR 三种//依赖配置信息 <dependencies>            <dependency>                         <groupId>junit</groupId>                         <artifactId>junit</artifactId>                         <version>4.0</version>                         <scope>test</scope>            </dependency>        </dependencies>    </project><build>    <finalName>WebMavenDem</finalName></build></project>parnt 父项目的信息modules 模块properties 定义pom变量 类似 int a;dependencyManageMent 有多模块时,统一子项目使用依赖项的同一版本 ,同时有了父模块后 ,子模块 就不会优先引用对应依赖 .同时 修改时 也只要改父类就可以了.常用maven命令执行maven命令必须进入到pom.xml的目录中进行执行进入到项目的pom.xml目录之后，就可以执行啦。1、运行 mvn compileOK，运行完毕，你在pom.xml配置的依赖的包已经导入到仓库了，问题来了，仓库默认的位置在哪？仓库的默认位置：c:Usrs[登录当前系统的用户名].m2repository刚才执行完compile之后，之前的文件夹发生了变化我们发现Hello项目里里多了一个target文件夹。文件夹的内容为：发现target里主要存放的就是编译后的字节码文件2、运行mvn test-compile，target文件夹下面除了classes之外多了test-classes文件夹3、运行mvn package，target文件夹下面又多了一个打好的jar包在这里插入图片描述4、运行mvn clean，发现整个target文件夹都没了。又回到了编译之前我们手动创建的文件夹五、仓库和坐标①pom.xml：Project Object Model 项目对象模型。它是maven的核心配置文件，所有的构建的配置都在这里设置。② 坐标：使用下面的三个向量在仓库中唯一的定位一个maven工程< packaging > ：打包类型jar：执行package会打成jar包war：执行package会打成war包pom ：用于maven工程的继承，通常父工程设置为pom③ maven工程的坐标与仓库中路径的关系：maven坐标和仓库对应的映射关系：[ groupId ] [ artifactId ][version][artifactId]-[version].jar去本地仓库看一下此目录：orgspringframeworkspring-core4.3.4.RELEASEspring-core-4.3.4.RELEASE.jar果然是完全对应的<project>：文件的根节点.<modelversion>： pom.xml使用的对象模型版本<groupId>：项目名称，一般写项目的域名<artifactId>：模块名称，子项目名或模块名称<version>：产品的版本号.<packaging>：打包类型，一般有jar、war、pom 等<name>：项目的显示名，常用于 Maven 生成的文档。<description>：项目描述，常用于 Maven 生成的文档<dependencies>：项目依赖构件配置，配置项目依赖构件的坐标<build>：项目构建配置，配置编译、运行插件等。六、依赖① maven解析依赖信息时会到本地仓库中取查找被依赖的jar包对于本地仓库中没有的会去中央仓库去查找maven坐标来获取jar包，获取到jar之后会下载到本地仓库对于中央仓库也找不到依赖的jar包的时候，就会编译失败了② 如果依赖的是自己或者团队开发的maven工程，需要先使用install命令把被依赖的maven工程的jar包导入到本地仓库中举例：现在我再创建第二个maven工程HelloFriend，其中用到了第一个Hello工程里类的sayHello(String name)方法。我们在给HelloFriend项目使用 mvn compile命令进行编译的时候，会提示缺少依赖Hello的jar包。怎么办呢？到第一个maven工程中执行 mvn install后，你再去看一下本地仓库，你会发现有了Hello项目的jar包。一旦本地仓库有了依赖的maven工程的jar包后，你再到HelloFriend项目中使用 mvn compile命令的时候，可以成功编译③ 依赖范围scope就是依赖的范围七、生命周期Maven有三套相互独立的生命周期，请注意这里说的是“三套”，而且“相互独立”，初学者容易将Maven的生命周期看成一个整体，其实不然。这三套生命周期分别是：①CleanLifecycle 在进行真正的构建之前进行一些清理工作。Clean生命周期一共包含了三个阶段：pre-clean执行一些需要在clean之前完成的工作clean移除所有上一次构建生成的文件post-clean执行一些需要在clean之后立刻完成的工作②DefaultLifecycle 构建的核心部分，编译，测试，打包，部署等等。validate验证工程是否正确(编译)generate-sourcesprocess-resources复制并处理资源文件，至目标目录，准备打包compile 编译项目的源代码process-classesgenerate-test-sourcesprocess-test-sourcesgenerate-test-resourcesprocess-test-resources复制并处理资源文件，至目标测试目录test-compile编译测试源代码process-test-classestest使用合适的单元测试框架运行测试。这些测试代码不会被打包或部署prepare-packagepackage接受编译好的代码，打包成可发布的格式，pre-integration-testintegration-testpost-integration-testverify运行所有检查,验证包是否有效install将包安装至本地仓库，以让其它项目依赖。deploy将最终的包复制到远程的仓库，以让其它开发人员与项目共享那我们在Hello的项目中执行 mvn install 命令，通过日志看看中间经历了什么？通过日志我们发现，其实执行mvn install，其中已经执行了compile 和 test 。总结：不论你要执行生命周期的哪一个阶段，maven都是从这个生命周期的开始执行插件：每个阶段都有插件（plugin），看上面标红的。插件的职责就是执行它对应的命令。③SiteLifecycle 生成项目报告，站点，发布站点。pre-site执行一些需要在生成站点文档之前完成的工作site生成项目的站点文档post-site执行一些需要在生成站点文档之后完成的工作，并且为部署做准备site-deploy将生成的站点文档部署到特定的服务器上九、maven工程的依赖高级特性① 依赖的传递性WebMavenDemo项目依赖JavaMavenService1 JavaMavenService1项目依赖JavaMavenService2pom.xml文件配置好依赖关系后，必须首先mvn install后，依赖的jar包才能使用。WebMavenDemo的pom.xml文件想能编译通过，JavaMavenService1必须mvn installJavaMavenService的pom.xml文件想能编译通过，JavaMavenService2必须mvn install传递性：在Eclipse中，为JavaMavenService2中增加了一个spring-core.jar包后，会惊喜的发现依赖的两个项目都自动的增加了这个jar包，这就是依赖的传递性。注意：非compile范围的依赖是不能传递的。② 依赖版本的原则：1、路径最短者优先原则Service2的log4j的版本是1.2.7版本，Service1排除了此包的依赖，自己加了一个Log4j的1.2.9的版本，那么WebMavenDemo项目遵守路径最短优先原则，Log4j的版本和Sercive1的版本一致。2、路径相同先声明优先原则这种场景依赖关系发生了变化，WebMavenDemo项目依赖Sercive1和Service2，它俩是同一个路径，那么谁在WebMavenDemo的pom.xml中先声明的依赖就用谁的版本。③ 统一管理依赖的版本：为了统一管理版本号，可以使用properties标签，里面可以自定义版本的标签名。在使用的地方使用${自定义标签名}build配置<build>　　<!-- 项目的名字 -->　　<finalName>WebMavenDemo</finalName>　　<!-- 描述项目中资源的位置 -->　　<resources>　　　　<!-- 自定义资源1 -->　　　　<resource>　　　　　　<!-- 资源目录 -->　　　　　　<directory>src/main/java</directory>　　　　　　<!-- 包括哪些文件参与打包 -->　　　　　　<includes>　　　　　　　　<include>**/*.xml</include>　　　　　　</includes>　　　　　　<!-- 排除哪些文件不参与打包 -->　　　　　　<excludes>　　　　　　　　<exclude>**/*.txt</exclude>　　　　　　　　　　<exclude>**/*.doc</exclude>　　　　　　</excludes>　　　　</resource>　　</resources>　　<!-- 设置构建时候的插件 -->　　<plugins>　　　　<plugin>　　　　　　<groupId>org.apache.maven.plugins</groupId>　　　　　　<artifactId>maven-compiler-plugin</artifactId>　　　　　　<version>2.1</version>　　　　　　<configuration>　　　　　　　　<!-- 源代码编译版本 -->　　　　　　　　<source>1.8</source>　　　　　　　　<!-- 目标平台编译版本 -->　　　　　　　　<target>1.8</target>　　　　　　</configuration>　　　　</plugin>　　　　<!-- 资源插件（资源的插件） -->  　　　　<plugin>  　　　　　　<groupId>org.apache.maven.plugins</groupId>  　　　　　　<artifactId>maven-resources-plugin</artifactId>  　　　　　　<version>2.1</version>  　　　　　　<executions>  　　　　　　　　<execution>  　　　　　　　　　　<phase>compile</phase>  　　　　　　　　</execution>  　　　　　　</executions>  　　　　　　<configuration>  　　　　　　　　<encoding>UTF-8</encoding>  　　　　　　</configuration> 　　　　</plugin>　　　　<!-- war插件(将项目打成war包) -->  　　　　<plugin>  　　　　　　<groupId>org.apache.maven.plugins</groupId>  　　　　　　<artifactId>maven-war-plugin</artifactId>  　　　　　　<version>2.1</version>  　　　　　　<configuration>　　　　　　　　<!-- war包名字 -->  　　　　　　　　<warName>WebMavenDemo1</warName>　　　　　　</configuration>  　　　　</plugin>  　　</plugins></build>配置好build后，执行mvn package之后，在maven工程指定的target目录里war包和文件都按照配置的生成了加入第三方架包Maven私服问题：项目组编写了一个通用的工具类，其它项目组将类拷贝过去使用，当工具类修改bug后通过邮件发送给各各项目组，这种分发机制不规范可能导致工具类版本不统一。解决方案：项目组将写的工具类通过maven构建，打成jar，将jar包发布到公司的maven仓库中（私服），公司其它项目通过maven依赖管理从仓库自动下载jar包。私服(用于jar的发布 和下载)公司在自己的局域网内搭建自己的远程仓库服务器，称为私服私服服务器即是公司内部的maven远程仓库，每个员工的电脑上安装maven软件并且连接私服服务器，员工将自己开发的项目打成jar并发布到私服服务器，其它项目组从私服服务器下载所依赖的构件（jar）。私服还充当一个代理服务器，当私服上没有jar包会从互联网中央仓库自动下载依赖版本冲突依赖版本冲突：当一个项目依赖的构件比较多时，它们相互之前存在依赖，当你需要对依赖版本统一管理时如果让maven自动来处理可能并不能如你所愿，如下例子：（传递依赖：当A 依赖B、B依赖C，在A中导入B后会自动导入C，C是A的传递依赖，如果C依赖D则D也可能是A的传递依赖。）<!--struts2-spring-plugin依赖spirng-beans-3.0.5--><dependency><groupId>org.apache.struts</groupId><artifactId>struts2-spring-plugin</artifactId><version>2.3.24</version></dependency><!--spring-context依赖spring-beans-4.2.4--><dependency><groupId>org.springframework</groupId><artifactId>spring-context</artifactId><version>4.2.4.RELEASE</version></dependency>org.apache.struts依赖spirng-beans-3.0.5，spring-context依赖spring-beans-4.2.4，但是发现spirng-beans-3.0.5加入到工程中，而我们希望spring-beans-4.2.4加入工程。依赖调解原则maven自动按照下边的原则调解：声明者优先原则在pom文件定义依赖，先声明的依赖为准。测试：如果将上边struts-spring-plugins和spring-context顺序颠倒，系统将导入spring-beans-4.2.4。分析：由于spring-context在前边，以spring-context依赖的spring-beans-4.2.4为准，所以最终spring-beans-4.2.4添加到了工程中。排除依赖上边的问题也可以通过排除依赖方法辅助依赖调解，如下：比如在依赖struts2-spring-plugin的设置中添加排除依赖，排除spring-beans，下边的配置表示：依赖struts2-spring-plugin，但排除struts2-spring-plugin所依赖的spring-beans。<!--struts2-spring-plugin依赖spirng-beans-3.0.5--><dependency><groupId>org.apache.struts</groupId><artifactId>struts2-spring-plugin</artifactId><version>2.3.24</version><!--排除 spring-beans--><exclusions><exclusion><groupId>org.springframework</groupId><artifactId>spring-beans</artifactId></exclusion><exclusion><groupId>org.springframework</groupId><artifactId>spring-context</artifactId></exclusion></exclusions></dependency>锁定版本面对众多的依赖，有一种方法不用考虑依赖路径、声明优化等因素可以采用直接锁定版本的方法确定依赖构件的版本，版本锁定后则不考虑依赖的声明顺序或依赖的路径，以锁定的版本的为准添加到工程中，此方法在企业开发中常用。首先父工程中pom.xml文件添加如下的配置是锁定了spring-beans和spring-context的版本：最后推荐个最新最全的maven依赖项版本查询网站：http://mvnrepository.com/创建 maven maven-archetype-quickstart 项目抱错问题解决方法Archetype，骨架的意思。文章出处：http://m.blog.csdn.net/blog/FireOfStar/42526027Archetype是什么？简单的说，Archetype是Maven工程的模板工具包。一个Archetype定义了要做的相同类型事情的初始样式或模型。这个名称给我们提供来了一个一致的生成Maven工程的方式。Archetype会帮助作者给用户创建Maven工程模板，并给用户提供生成相关工程模板版本的参数化方法。使用Archetype提供的好的方法，是开发者能够使用最佳实践来快速的构建和组织一致化的工程。在Maven工程中，我们努力使用Archetype来尽可能快的给用户提供示例工程，同时也会把Maven的最佳实践介绍给新的用户。一个新的用户可以使用工作中的Maven工作作为跳板来研究更过的Maven中功能。我们也可以使用Archetype的添加机制，这样就意味着允许我们抓取Archetype中项目片段，并把它们添加到既存的工程中。Maven网站的Archetype就是很好的例子。例如，你可以使用“quick start archetype”来生成一个工程，然后就可以通过其中既存的“site archetype”来快速的创建一个网址工程。你能够使用Archetype来做很多这样的事情。在你的团队中可能想要标准化的J2EE开发，这需要你提供EJBs、或者是WARs、或者是Web services的原型。一旦在你团队资源库中创建和部署这些原型，它们就可以在你团队内共享使用。如何使用Archetype要基于Archetype来创建一个新的工程，需要像下面示例这样来调用：mvn archetype:generate已有的ArchetypesArchetype ID说明maven-archetype-archetype一个样例原型maven-archetype-j2ee-simple简单的J2EE应用程序样例maven-archetype-mojoMaven插件样本的示例maven-archetype-pluginMaven插件样本maven-archetype-plugin-siteMave插件网站的样例maven-archetype-portletJSR-268门户样例maven-archetype-quickstartMaven工程样例maven-archetype-simple一个简单的Maven工程maven-archetype-siteMaven网站的样例，它演示了对诸如APT、XDoc和FML等文档类型的支持，并演示了如果把网站国际化（i18n）maven-archetype-site-simpleMaven网站样例maven-archetype-webappMaven的Webapp工程样例常用Archetype1，maven-archetype-quickstart默认的Archetype,基本内容包括：一个包含junit依赖声明的pom.xmlsrc/main/java主代码目录及一个名为App的类src/test/java测试代码目录及一个名为AppTest的测试用例2，maven-archetype-webapp一个最简单的Maven war项目模板，当需要快速创建一个Web应用的时候可以使用它。生成的项目内容包括：一个packaging为war且带有junit依赖声明的pom.xmlsrc/main/webapp/目录src/main/webapp/index.jsp文件src/main/webapp/WEB-INF/web.xml文件pom.xml 配置文件 如果第一次的话 可以复制下来 帮助你下载完整插件 最后检查 对不对标志(目录结构是否正确)<?xml version="1.0"encoding="UTF-8"?><project xmlns="http://maven.apache.org/POM/4.0.0"xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"><modelVersion>4.0.0</modelVersion><groupId>com.alipay.test</groupId><artifactId>test</artifactId><version>1.0-SNAPSHOT</version><name>test</name><!--FIXMEchange it to the project's website--><url>http://www.example.com</url><properties><project.build.sourceEncoding>UTF-8</project.build.sourceEncoding><maven.compiler.source>1.7</maven.compiler.source><maven.compiler.target>1.7</maven.compiler.target></properties><dependencies><dependency><groupId>junit</groupId><artifactId>junit</artifactId><version>4.11</version><scope>test</scope></dependency><dependency><groupId>commons-logging</groupId><artifactId>commons-logging</artifactId><version>1.2</version></dependency><dependency><groupId>com.alipay.sdk</groupId><artifactId>alipay-sdk-java</artifactId><version>3.0.0</version><exclusions><exclusion><artifactId>commons-logging</artifactId><groupId>commons-logging</groupId></exclusion></exclusions></dependency><dependency><groupId>commons-lang</groupId><artifactId>commons-lang</artifactId><version>2.6</version></dependency><dependency><groupId>commons-configuration</groupId><artifactId>commons-configuration</artifactId><version>1.10</version><exclusions><exclusion><artifactId>commons-logging</artifactId><groupId>commons-logging</groupId></exclusion></exclusions></dependency><dependency><groupId>commons-codec</groupId><artifactId>commons-codec</artifactId><version>1.11</version></dependency><dependency><groupId>com.google.zxing</groupId><artifactId>core</artifactId><version>3.2.1</version></dependency><dependency><groupId>org.hamcrest</groupId><artifactId>hamcrest-core</artifactId><version>1.3</version><scope>test</scope></dependency><dependency><groupId>com.google.code.gson</groupId><artifactId>gson</artifactId><version>2.8.5</version></dependency></dependencies><build><pluginManagement><!--lock down plugins versions to avoid using Mavendefaults(may be moved to parent pom)--><plugins><!--clean lifecycle,see https://maven.apache.org/ref/current/maven-core/lifecycles.html#clean_Lifecycle--><plugin><artifactId>maven-clean-plugin</artifactId><version>3.1.0</version></plugin><!--defaultlifecycle,jar packaging:see https://maven.apache.org/ref/current/maven-core/default-bindings.html#Plugin_bindings_for_jar_packaging--><plugin><artifactId>maven-resources-plugin</artifactId><version>3.0.2</version></plugin><plugin><artifactId>maven-compiler-plugin</artifactId><version>3.8.0</version></plugin><plugin><artifactId>maven-surefire-plugin</artifactId><version>2.22.1</version></plugin><plugin><artifactId>maven-jar-plugin</artifactId><version>3.0.2</version></plugin><plugin><artifactId>maven-install-plugin</artifactId><version>2.5.2</version></plugin><plugin><artifactId>maven-deploy-plugin</artifactId><version>2.8.2</version></plugin><!--site lifecycle,see https://maven.apache.org/ref/current/maven-core/lifecycles.html#site_Lifecycle--><plugin><artifactId>maven-site-plugin</artifactId><version>3.7.1</version></plugin><plugin><artifactId>maven-project-info-reports-plugin</artifactId><version>3.0.0</version></plugin></plugins></pluginManagement></build></project>			https://blog.csdn.net/qq_39088066/article/details/101294451
49			['万字长文综述目标检测领域，你要的都在这里_AI科技大本营-CSDN博客']			来源 | AI专栏（ID: pursue-Y-future）目标检测是计算机视觉中的一个重要问题，近年来传统检测方法已难以满足人们对目标检测效果的要求，随着深度学习在图像分类任务上取得巨大进展，基于深度学习的目标检测算法逐渐成为主流。总体上站长我都做了summary，先上图为敬：目标检测要干什么?目标检测是机器视觉中最常见的问题。是一种基于目标几何和统计特征的图像分割，它将目标的分割和识别合二为一，其准确性和实时性是整个系统的一项重要能力，近年来，目标检测在人工智能，人脸识别，无人驾驶等领域都得到了广泛的应用。然而，在目标检测的过程中会受到各种各样干扰，比如角度、遮挡、光线强度等因素，这些因素会导致目标发生畸变，为目标检测增加了新的挑战。相比于图像分类，目标检测更具难度。目标检测，就是将目标定位和目标分类结合起来，利用图像处理技术、机器学习等多方向的知识，从图像（视频）中定位感兴趣的对象。目标分类负责判断输入的图像中是否包含所需物体，目标定位则负责表示目标物体的位置，并用外接矩形框定位。这需要计算机在准确判断目标类别的同时，还要给出每个目标相对精确的位置。目标检测算法有哪些？现有的目标检测算法有传统检测算法和基于深度学习的检测算法。传统的目标检测算法，多是基于滑动窗口的框架或是根据特征点进行匹配。自2012年AlexNet在当年度ImageNet大规模视觉识别挑战赛中一举夺冠，且效果远超传统算法，将大众的视野重新带回到深度神经网络。传统的检测算法如下图所示，传统检测方法一般分三个步骤：首先在给定图像上采用不同大小的滑动窗口对整幅图像进行遍历选择候选区域，使用不同大小的滑动窗口框住待测图像中的某一部分作为候选区域，然后提取该候选区域相关的视觉特征；特征提取，如人检测和普通目标检测常用的HOG和SIFT 特征等，然后对这些区域提取特征；分类器分类，即使用训练完成的分类器进行分类，如常用的支持向量机，最后使用分类器进行分类。虽然这种方法取得了不错的结果，但是采用滑动窗口进行区域选择时没有针对性导致时间复杂度高且窗口冗余，另外手工设计的特征没有很好的鲁棒性。基于深度学习的检测算法2014年R-CNN的提出，使得基于CNN的目标检测算法逐渐成为主流。深度学习的应用，使检测精度和检测速度都获得了改善。自从AlexNet 在比赛中使用卷积神经网络进而大幅度提高了图像分类的准确率，便有学者尝试将深度学习应用到目标类别检测中。卷积神经网络不仅能够提取更高层、表达能力更好的特征，还能在同一个模型中完成对于特征的提取、选择和分类。在这方面，主要有两种主流的算法：一类是结合regionproposal、CNN网络的，基于分类的R-CNN 系列目标检测框架（two stage）；另一类则是将目标检测转换为回归问题的算法（single stage）。传统的目标检测算法传统的目标检测算法大致可以分为目标实例检测与传统目标类别检测两类：目标实例检测问题通常利用模板和图像稳定的特征点，获得模板与场景中对象的对应关系，检测出目标实例。目标实例检测关注的只是具体目标本身，图像中的其余对象都是无关量。传统目标类别检测则通过使用AdaBoost算法，HOG特征和支持向量机等方法，根据选定的特征和分类器，检测出有限的几种类别。基于SIFT系列算法SIFT算法Lowe提出的SIFT算法，通过查找不易受光照、噪声、仿射变换影响的特征点来匹配目标，是目前应用极为广泛的关键点检测和描述算法。该算法通过使用高斯模糊实现尺度空间，高斯差分函数进行极值检测，再通过对边缘主曲率的判定，筛除边缘响应的不稳定点，得到匹配稳定、抗噪能力强的关键点。最后利用方向直方图统计关键点邻域梯度和方向，获得描述符。SIFT 算法通过一系列方法，保证提取的特征具有平移、缩放及旋转不变等特性，对于光线、噪声、少量视角改变也具有一定的鲁棒性，针对部分遮挡也有不错的识别率。但是，SIFT 算法存在复杂度高，检测速度慢，对模糊图像和光滑边缘很难提取有效特征点等问题。PCA-SIFT算法该算法在SIFT的基础上，对其最后一步做出了改进。引入主成分分析（PCA）方法，使用PCA替代直方图，来对描述子向量进行降维，以提高匹配效率。相较SIFT，PCA-SIFT维数更少且灵活可变，检测速度约为SIFT的3倍。但降维损失部分信息，导致只对具有代表性的图像有较好效果，具有局限性。SURF算法SURF算法也是一种基于SIFT的改进算法，Hessian矩阵是该算法的核心。该算法利用高斯滤波保证尺度无关性，并用盒式（box）滤波器替代高斯滤波器，简化计算。通过构建Hessian矩阵，获取关键点定位。另外，在尺度空间中，不同于SIFT 构建不同尺度的图像，SURF 保持图像大小不变，只改变滤波器的大小，从减少了计算量。简单来说，SURF 算法利用近似的Hessian 矩阵减少降采样过程，快速构建尺度金字塔，实现了目标检测速度的提高。比较对于SIFT、PCA-SIFT 以及SURF这三种算法，站长作出如下总结：PCA-SIFT与SURF算法分别对SIFT的匹配过程做出简化，因此在特征点匹配精度上必然有所下降。其中SIFT算法提取的特征点最为丰富，在尺度、旋转等情况下都具有最好的性能，但高复杂度导致检测速度最慢，且对于模糊、光滑边缘的提取效果并不理想；PCA-SIFT使用PCA方法进行降维，减少计算的同时，产生信息丢失，因此整体性能在三种算法中比较一般；SURF 合理利用积分图减少运算，小波变换、Hessian 矩阵等方法基本不会降低精度，因此在获得好检测速度的同时，也保证了整体性能优于PCA-SIFT。基于AdaBoost 系列算法AdaBoost 算法AdaBoost是一种是基于Boosting的机器学习算法。初始时，设训练集中n个样本具有相同的权重。在每次训练后调整训练集中数据权重，增加错误样本的权重，使得下一个分类器能够对错误样本进行重点训练。经过N轮训练后，将N个弱分类器整合，根据各分类器的性能分配相应的权值，组成一个高准确率、低错误率的强分类器。Viola-Jones 算法Viola-Jones算法是第一种能实时处理且效果较好的人脸检测算法，此算法的提出标志着人脸检测进入实际应用阶段。Viola-Jones 检测算法（简称VJ 算法）使用Haar 特征来描述窗口，反映局部区域的明暗变化，并利用积分图的思路解决Haar特征提取时计算量大、重复的缺点。同时，引入级联的思想。如下图所示，VJ根据分类器的复杂程度和计算代价排列，分类代价越高的分类器需要分类的图像越少，减少分类工作量。概括地说，VJ 算法利用Haar-like 特征描述目标共有属性，利用积分图实现特征快速计算，使用级联分类器减少AdaBoost 的计算量，快速检测出目标。Summary如下表所示，站长针对传统算法做出对比总结。总的来说，这些算法的目的都是在保证提取丰富、准确特征的前提下，快速地进行特征计算及预测。但传统算法提取的特征基本都是低层次、人工选定的特征，这些特征相对更直观、易理解，针对特定对象更有针对性，但不能很好地表达大量、多类目标。LeNet-5LeNet-5 于1998 年提出，主要用于手写数字识别。该网络共有7层网络，包括2 个卷积层、2 个池化层和3 个全连接层，其准确率很高，可由于当时计算机的计算能力及数据量的不足，该模型未能受到重视，从而没有很好地发展起来。AlexNet直到2012年ILSVRC比赛中，AlexNet在算法准确率方面以超过使用传统方法的第二名11%的情况获得冠军后，才使卷积神经网络重回大众视野并得到重视。该网络是对LeNet-5的扩展，它将卷积神经网络的思想应用到了更深更宽的网络上，共有8 层网络，包括5 个卷积层和3 个全连接层。VGGNetVGGNet 则充分代表了从2012 年到2014 年在卷积神经网络结构上的一个进展。相比AlexNet 来说该网络会更深一些，共有19 层网络：16 个卷积层和3 个全连接层。该网络在2014年ILSVRC比赛中获得了第二名，在目标检测上获得了第一名。GoogleNetGoogleNet 则是2014年ILSVRC比赛中获得第一名的网络。该网络通过增加网络的深度和广度来获取更好的结果，然而当网络加深加宽到一定程度时，继续加深加宽反倒不能再提高效果。因此，GoogleNet通过设计稀疏连接的Inception结构来解决这个问题。ResNet2015 年ILSVRC 比赛中ResNet 网络获得冠军。同样该网络也是用于解决网络加深到一定程度就不能提高效果的问题，从而使得网络能够继续加深到更深的层次。基于深度学习的检测算法基于分类的检测算法Region proposal（候选区域）是通过Selective Search等算法，根据图像中纹理、边缘、颜色等信息，检测较少区域的同时保证了较高的召回率。OverFeat 算法OverFeat是最先将深度学习应用到目标检测中的算法之一。严格来说，OverFeat 并没有使用region proposal，但其思路被后面的R-CNN系列沿用并改进。该算法通过多尺度的滑动窗口结合AlexNet 提取图像特征，完成检测。在ILSVRC 2013 数据集上的平均准确率为24.3%，检测效果较传统算法有显著改进，但依旧存在较高错误率。R-CNN算法在Overfeeat 提出后不久，Ross Girshick 等人提出了R-CNN模型，使目标检测取得巨大突破。如下图所示，R-CNN利用Selective Search获得候选区域（约2000个）。随即对候选区域大小进行归一化，用作CNN网络的标准输入。再使用AlexNet获得候选区域中的特征，最后利用多个SVM进行分类以及线性回归微调定位框。R-CNN将检测效果从OverFeat的24.3%大幅提升至31.4%（ILSVRC2013数据集），并在VOC2007数据集上获得58.5%的准确率。但是，R-CNN对近2000个候选区域分别做特征提取，而候选区域之间存在许多重复区域，导致大量且重复的运算，运行缓慢，平均每幅图片的处理时间为34s。同时，对每一步的数据进行存储，极为损耗存储空间。另外，对候选区域进行归一化操作，会对最终结果产生影响。SPP-Net如下图所示，针对R-CNN对所有候选区域分别提取特征的缺点，SPP-Net一次性对整张图片作卷积操作提取特征。使得特征提取从R-CNN 的近2 000 次变为提取1次整张图片特征，大大减少了工作量。另外，SPP-Net 在最后一个卷积层后、全连接层前添加空间金字塔池化层（SPP层），提取固定尺寸的特征向量，避免对候选区域大小进行归一化的复杂操作。以上两点改进使得SPP-Net 的检测速度比R-CNN快38~102倍，并解决了候选区域归一化问题。SPP-Net虽然更换了卷积网络，但准确率相差无几。同时，SPPNet依然没有解决R-CNN存储空间消耗的问题，确定候选区域、特征提取、对象分类、定位修正这些步骤依然是分离的。Fast-RCNNFast R-CNN算法在SPP-Net的基础上，将SPP层简化为ROI Pooling 层，并将全连接层的输出作SVD分解，得到两个输出向量：softmax的分类得分以及Boundingbox外接矩形框的窗口回归。这种改进将分类问题和边框回归问题进行了合并。用softmax代替SVM，将所有的特征都存储在显存中，减少了磁盘空间的占用。SVD分解则在几乎不影响精度的情况了，极大加快检测速度。Fast R-CNN使用VGG16代替AlexNet，平均准确率达到70.0%，且训练速度较R-CNN提升9倍，检测速度达到每幅图片0.3 s（除去region proposal 阶段）。Fast R-CNN 依然使用Selective Search 方法选取候选区域，这一步骤包含大量计算。在CPU上运行时，获取每张图片的候选区域平均需要2s。由此可见，改进Selective Search是Fast R-CNN速度提升的关键。Faster-RCNNSPP-Net 和Fast R-CNN从特征提取的角度，减少了工作量，但依然没有解决Selective Search 选择候选区域速度慢的问题。Faster R-CNN使用RPN网络替代Selective Search 算法，使目标识别实现真正端到端的计算。如下图所示，RPN 网络通过在特征图上做划窗操作，使用预设尺度的锚点框映射到原图，得到候选区域。RPN网络输入的特征图和全连接层中的特征图共享计算。RPN的使用，使Faster R-CNN 能够在一个网络框架之内完成候选区域、特征提取、分类、定位修正等操作。RPN使得Faster R-CNN在region proposal阶段只需10 ms，检测速度达到5 f/s（包括所有步骤），并且检测精度也得到提升，达到73.2%。但是，FasterR-CNN仍然使用ROI Pooling，导致之后的网络特征失去平移不变性，影响最终定位准确性，ROIPooling后每个区域经过多个全连接层，存在较多重复算，Faster R-CNN在特征图上使用锚点框对应原图，而锚点框经过多次下采样操作，对应原图一块较大的区域，导致Faster R-CNN检测小目标的效果并不是很好。R-FCN目标检测要包括两个问题：分类问题和检测定位问题。前者具有平移不变性，后者具有平移敏感性。R-FCN使用全卷积网络ResNet代替VGG，提升特征提取与分类的效果；针对全卷积网络不适应平移敏感性的缺陷，该算法使用特定的卷积层生成包含目标空间位置信息的位置敏感分布图，ROI Pooling 层后不再连接全连接层，避免重复计算。R-FCN的准确率达到83.6%，测试每张图片平均花费170 ms，比Faster-RCNN快了2.5~20倍。但是R-FCN在得到Score map 需要生成一个随类别数线性增长的channel 数，这一过程虽然提升了目标检测精度，但减慢了检测速度，导致其难以满足实时性要求。Mask R-CNNMask R-CNN是一种在Faster R-CNN 基础上加以改进的算法，增加了对实例分割的关注。该算法在分类和定位回归以外，加入了关于实例分割的并行分支，并将三者的损失联合训练。实例分割要求实例定位的精准度达到像素级，而Faster-R-CNN因为ROI Pooling层的等比例缩放过程中引入了误差，导致空间量化较为粗糙，无法准确定位。Mask R-CNN 提出双线性差值RoIAlign获得更准确的像素信息，使得掩码准确率提升10%到50%；Mask R-CNN 还使用ResNet 基础网络，在COCO数据集上的检测速度为5f/s，检测准确性从FastR-CNN的19.7%提升至39.8%。Mask R-CNN在检测精度、实例分割方面都达到目前最高的层次。其后一些算法在性能上有所提升，但基本维持在同一水平。但是该算法的检测速度依旧难以满足实时要求，并且实例分割目前也还面临着标注代价过于昂贵的问题。Summary如下图 所示，从R-CNN开始，研究者将目标检测的问题关注点集中到分类上，采用“regionproposal+CNN feature+SVM”的思路，利用了CNN网络，大大提高了检测的精度；后面的SPP-Net、Fast-RCNN、Faster-RCNN等基本沿用了这一思路，在检测效率上进行改进；但Faster-RCNN只能达到5f/s，就实时性而言略有不足。随后的R-FCN 虽然有所提升，但效果依然无法令人满意。对此，研究者提出了另一种新思路，直接将目标检测转化到回归上，用一张图片得到bounding box 以及类别。基于回归的检测算法YOLO从R-CNN到Faster-RCNN，目标检测始终遵循“region proposal+分类”的思路，训练两个模型必然导致参数、训练量的增加，影响训练和检测的速度。由此，YOLO提出了一种“single-stage”的思路。如下图所示，YOLO将图片划分为S×S的网格（cell），各网格只负责检测中心落在该网格的目标，每个网格需要预测两个尺度的bounding box和类别信息，一次性预测所有区域所含目标的bounding box、目标置信度以及类别概率完成检测。YOLO采用以cell 为中心的多尺度区域取代region proposal，舍弃了一些精确度以换取检测速度的大幅提升，检测速度可以达到45f/s，足以满足实时要求；检测精度为63.4%，较Faster R-CNN的73.2%，差距较大。YOLO在极大提高检测速度的情况下，也存在以下问题：因为每个网格值预测两个boundingbox，且类别相同，因此对于中心同时落在一个网格总的物体以及小物体的检测效果差，多物体环境下漏检较多；由于YOLO关于定位框的确定略显粗糙，因此其目标位置定位准确度不如Fast-RCNN；对于外型非常规的物体检测效果不佳。SSDFaster-RCNN检测检测精度高但检测速度慢，YOLO检测精度不高但检测速度快，SSD则结合两者的优点，在YOLO的基础上借鉴了RPN的思路，在保证高精度检测的同时，兼顾检测速度。如下图所示，因为不同层的特征图具有对应大小的感受野，特定层的特征图只需要训练对应尺度的对象检测。因此，SSD结合高层和底层的特征图，使用多尺度区域特征进行回归。SSD300的mAP能达到73.2%，基本与Faster R-CNN持平，而检测速度达到59f/s，比Faster R-CNN快6.6 倍。但是SSD具有以下问题：小目标对应到特征图中很小的区域，无法得到充分训练，因此SSD对于小目标的检测效果依然不理想；无候选区域时，区域回归难度较大，容易出现难以收敛等问题；SSD不同层的特征图都作为分类网络的独立输入，导致同一个物体被不同大小的框同时检测，重复运算。YOLOv2 以及YOLO9000YOLOv2通过在每一个卷积层后添加batch normalization、多尺度训练，加入K-mean 维度聚类等方式，使得检测速度和精度的再次提升。该算法能够在76.8%正确率的同时达到67 f/s 的检测速度，78.6%的正确率时达到40 f/s。该算法性能基本代表目前业界的最先进水平。同文还提出了YOLO9000，该算法采用wordTree 层次分类，混合检测数据、识别数据集，在分类和检测数据集上同时训练，实现9418类的检测。无论是YOLO系列还是SSD算法，都沿用R-CNN系列算法先在大数据集上进行分类预训练，再在小数据集上fine-tune 的方法。但fine-tune 预训练模型有以下问题：预训练模型，往往无法迁移到如医疗图像等特定数据上；预训练模型结构基本固定，难以修改；预训练样本和最终检测目标有所区别，得到的模型未必是检测目标的最佳模型。Summary针对预训练模型的问题，DSOD算法提出一种从零训练网络的方法，达到媲美fine-tune模型的效果。DSOD基于SSD算法，在特征融合部分引入DenseNet思想，减少了参数量；mAP 为77.7%，与SSD300 相当；检测速度为17.4f/s，较SSD300 的46 f/s 尚有较大差距。R-SSD算法则在SSD的基础上，增加不同层之间特征图的关联，避免同一物体重复框的问题；同时增加特征金字塔中特征图的数量，改善小物体的检测效果。该算法mAP为80.8%，略高于SSD。但是特征图的增加，导致计算量增加，检测速度降低，仅为16.6 f/s。目标检测可参考创新点基于深度学习的目标检测在检测精度以及检测速度上，较传统方法获得了极大的提高，但依然面临这一些问题：对于小数据量对于小数据量，目前的框架可能无法得到好的结果。目前的算法，大多使用了迁移学习，也就是现在现有的大数据集中进行训练，再将训练好的“半成品”做fine-tune操作。若目标数据不在ImageNet 等数据集中，训练效果要视目标与大数据集相关程度而定。DSOD算法虽然设计了一种从零开始训练的网络，也取得了不错的效果，但是其检测速度尚有待提升。深度学习解释性差特别是在更深的层次上，很多时候只能依靠测试和经验来猜测其有效或无效的原因，对于中间的过程缺少明确的解释，更像是一个黑盒。计算强度大GPU的使用，提升了计算机的运算能力，但是很多操作依然过于庞大。如何简化、复用计算的同时，尽可能保证准确率，可能会是一个可以创新的点。信息的损失对于场景信息、语义信息等视频中原有信息的利用不充分，造成一些有效信息的损失。小目标检测问题无论是R-CNN系列还是SSD等算法，始终无法在小目标检测问题上获得令人满意的效果。就目前算法而言，为保证检测速度，通常减少特征金字塔的图像，以减少计算量，但这必然导致小目标在特征图上得不到充分训练；如R-SSD增加特征图数量，损失了检测速度。总结目前已有众多基于深度学习的目标检测算法，且其相关研究仍在不断进行，几乎每年都有最新成果出现。即便如此，当前这些方法的检测效果仍处于一个较低的水平，从而导致不能广泛应用于实际通用的检测任务当中。此外，尽管当前基于深度学习的检测算法成为发展的主流，但传统的检测方法仍在使用，并未完全被抛弃，或许将传统方法有效应用于深度学习方法中会对检测效果的提高有所帮助。从最初的人为寻找特征到最近的基于深度学习的目标检测算法，可以看出对于目标检测的要求始终是快速、精准以及适用范围广。就目前来说，传统的目标检测方法仍在使用，且在一段时间内仍会有一定市场。传统的目标检测技术对数据量要求少，在针对数据来源不够丰富的项目时，可能会取得比深度学习更好的效果。但是将深度学习应用到目标检测中是可以预见的主流趋势。特别是随着硬件设备性能的提升，一定范围内的运算量处理将不会再成为实时检测的掣肘。如何利用上下文关联信息、场景信息和语义信息，将会是接下来目标检测的一个重要研究方向。假使平行视觉的思路切实可行，那么数据集标注困难、数据量不足的问题，将获得较好的解决。另外，如何更好解决与训练集关联性不大的小数据集检测问题，也是一个比较重要的研究方向。Hinton 的capsule 能否获得比传统CNN更好的效果，也需要进行进一步的研究。巨人的肩膀[1] Krizhevsky A，Sutskever I，Hinton G E.ImageNet classification with deep cnn[J].Communications of the ACM，2012，60（2）[2] Huang G，Liu Z，Maaten L V D，et al.Densely connected cnn[C]//Proceedings of CVPR 2016.[3] Jeong J，Park H，Kwak N.Enhancement of SSD by concatenating feature maps for object detection[C]//Proceedings of CVPR 2017.[4]万圣贤，兰艳艳，郭嘉丰，等. 基于弱监督预训练深度模型的微博情感分析[J]. 中文信息学报，2017，31（3）：191-197.[5]周俊宇，赵艳明. 卷积神经网络在图像分类和目标检测应用综述[J]. 计算机工程与应用，2017，53（13）：34-41.[6]张慧，王坤峰，王飞跃. 深度学习在目标视觉检测中的应用进展与展望[J]. 自动化学报，2017，43（8）：1289-1305.[7]王坤峰，苟超，王飞跃. 平行视觉：基于ACP的智能视觉计算方法[J]. 自动化学报，2016，42（10）：1490-1500.[8] 高红红，曹建荣，李振宇，等. 基于背景分类的运动目标检测算法[J].  计算机工程与应用，2017，53（21）：179-184.推荐阅读Uber 前无人驾驶工程师告诉你，国内无人驾驶之路还要走多久？如何用NLP辅助投资分析？三大海外机构落地案例详解Gary Marcus：因果熵理论的荒诞和认知科学带给AI的11个启示 | 文末赠书AI 终极问题：我们的大脑是一台超级计算机吗？借助大数据进行社交媒体营销，企业们得这么玩！力挺比特币的世界第2交易员：仅次于索罗斯，连续25年无亏损你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106580602
50			['全网唯一秃头数据集：20万张人像，网罗各类秃头_AI科技大本营-CSDN博客']			作者 | 神经星星题图 | 视觉中国来源 | Hyper超神经见过对植物图片数据集的，也见过对名人人脸数据集，但你见过专门针对「秃头党」进行分类和识别的吗？一位印度学生 Ashish Jangra ，最近在 Kaggle 上发布了一个名为「Bald Classification Dataset」的数据集。没错，就是这个「秃头」据介绍，Bald Classification Dataset（秃头数据集）由 Ashish Jangra 于今年 5 月发布。数据集中包含20 万张光头人像的图像，分为测试集、训练集、验证集三个文件夹，每个文件夹也包括 Bald 和 NotBald 两种图像。数据集样例截图其中秃头人像数据集的来源，主要是欧美公众人物，包含政商界、娱乐圈、体育界人士。Bald Classification Dataset发布人员：Ashish Jangra包含数量：20 万张秃头人像数据格式：JPG数据大小：1.3 G发布时间：2020 年 5 月下载地址：https://hyper.ai/datasets/12385数据集作者：发量惊人的印度小哥发布者 Ashish Jangra 也是个停不下来的人。他是一位 95 后，2016 年开始就读于 Lovely Professional University（印度拉夫里科技大学)的计算机专业。发量惊人的数据集作者，哼！通过 Udacity、Coursera 和很多开源的课程，学习了 Python 和机器学习的知识。还在 YouTube 上上传了自己的教学课程，包括 70 分钟的计算机视觉入门课程，和口罩人脸识别的最佳实践课程。作者个人主页：https://ashishjangra.com/秃头数据集，怎么用？数据集也下载好了，问题来了，怎么用？我们为大家简单设计了几个该数据集的使用场景：计算目标区域的「含秃率」通过训练秃头数据集，可以快速算出合影、视频、或区域街景中，目标区域中的「含秃率」。可以得出一个区域里某类人群的作息规律、审美习惯、职业背景等等玄学相关因素，公司或学校也可以通过「含秃率」的统计，进行对年度贡献团队/实验室的评定参考。病理性脱发的病程判定在皮肤科、内分泌科、脱发专科门诊，协助医生进行病理性脱发的病程判定。虽然医生也能一眼就看出来，但是有一个基于人工智能的判断结果，也可以让病人心服口服，不再倔强。植发医院的销售线索工具我们经常会在地铁、电梯内看到雍x植发、科X源的广告，相信这款工具也能有效地帮助植发机构，更快找到销售线索。无论是在密集人群中发现目标群体，还是对比特定区域之间的目标群体密度，都可以进行尝试。如何下载该数据集？除了可以访问原作者的 Kaggle 项目主页，我们还将整个数据集搬运回来了。访问https://hyper.ai主页，搜索关键词秃头，或者直接访问https://hyper.ai/datasets/12385都可以找到该数据集，支持通过直链、磁力链接、 BT 种子、REST API等下载方式进行下载。这么好的数据集，还不快快下载来看看？推荐阅读Uber 前无人驾驶工程师告诉你，国内无人驾驶之路还要走多久？如何用NLP辅助投资分析？三大海外机构落地案例详解Gary Marcus：因果熵理论的荒诞和认知科学带给AI的11个启示 | 文末赠书AI 终极问题：我们的大脑是一台超级计算机吗？借助大数据进行社交媒体营销，企业们得这么玩！力挺比特币的世界第2交易员：仅次于索罗斯，连续25年无亏损你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106580596
51			['lerna管理前端packages的最佳实践_qq_37653449的博客-CSDN博客']			背景对于维护过多个package的同学来说，都会遇到一个选择：这些package是放在一个仓库里维护还是放在多个仓库里单独维护，数量较少的时候，多个仓库维护不会有太大问题，但是当package数量逐渐增多时，一些问题逐渐暴露出来：package之间相互依赖，开发人员需要在本地手动执行npm link，维护版本号的更替；issue难以统一追踪，管理，因为其分散在独立的repo里；每一个package都包含独立的node_modules，而且大部分都包含babel,webpack等开发时依赖，安装耗时冗余并且占用过多空间。什么是lernalerna到底是什么呢？lerna官网上是这样描述的用于管理具有多个包的JavaScript项目的工具。这个介绍可以说很清晰了，引入lerna后，上面提到的问题不仅迎刃而解，更为开发人员提供了一种管理多packages javascript项目的方式。一、自动解决packages之间的依赖关系。二、通过git 检测文件改动，自动发布。三、根据git 提交记录，自动生成CHANGELOG常用命令全局安装lernalerna 我们需要全局安装lerna工具。$npmi -g lerna# 或$ yarn global add lerna为所有项目安装依赖，类似于npm/yarn i$ lerna bootstrap提交对项目的更新运行该命令会执行如下的步骤运行lerna updated来决定哪一个包需要被publish如果有必要，将会更新lerna.json中的version将所有更新过的的包中的package.json的version字段更新将所有更新过的包中的依赖更新为新版本创建一个git commit或tag将包publish到npm上$ lerna publish# 用于发布更新$ lerna publish --skip-git# 不会创建git commit或tag$ lerna publish --skip-npm# 不会把包publish到npm上使用lerna 初始化项目$ lerna init# 固定模式(Fixed mode)默认为固定模式，packages下的所有包共用一个版本号(version)$ lerna init --independent# 独立模式(Independent mode)，每一个包有一个独立的版本号为packages文件夹下的package安装依赖$ lerna add<package>[@version][--dev]# 命令签名# 例如$ lerna add module-1 --scope=module-2# 将 module-1 安装到 module-2$ lerna add module-1 --scope=module-2 --dev# 将 module-1 安装到 module-2 的 devDependencies 下$ lerna add module-1# 将 module-1 安装到除 module-1 以外的所有模块$ lerna add babel-core# 将 babel-core 安装到所有模块卸载依赖$ lerna exec -- <command> [..args] # 在所有包中运行该命令# 例如$ lerna exec --scope=npm-list  yarn remove listr # 将 npm-list 包下的 listr 卸载$ lerna exec -- yarn remove listr # 将所有包下的 listr 卸载检查对包是否发生过变更（前提是git代码已经提交）$ lerna updated# 或$ lernadiff显示packages下的各个package的name$ lernals清理node_modules$ lerna cleanlerna run运行npm script，可以指定具体的package。$ lerna run<script>--[..args]# 在所有包下运行指定# 例如$ lerna runtest# 运行所有包的 test 命令$ lerna run build# 运行所有包的 build 命令$ lerna run --parallelwatch# 观看所有包并在更改时发报，流式处理前缀输出$ lerna run --scope my-componenttest# 运行 my-component 模块下的 testlerna.json解析{"version":"1.1.3","npmClient":"npm","command":{"publish":{"ignoreChanges":["ignored-file","*.md"]},"bootstrap":{"ignore":"component-*","npmClientArgs":["--no-package-lock"]}},"packages":["packages/*"]}version：当前库的版本npmClient： 允许指定命令使用的client， 默认是 npm， 可以设置成 yarncommand.publish.ignoreChanges：可以指定那些目录或者文件的变更不会被publishcommand.bootstrap.ignore：指定不受 bootstrap 命令影响的包command.bootstrap.npmClientArgs：指定默认传给 lerna bootstrap 命令的参数command.bootstrap.scope：指定那些包会受 lerna bootstrap 命令影响packages：指定包所在的目录环境配置Git 在一个lerna工程里，是通过git来进行代码管理的。所以你首先要确保本地有正确的git环境。 如果需要多人协作开发，请先创建正确的git中心仓库的链接。 因此需要你了解基本的git操作，在此不再赘述。npm仓库 无论你管理的package是要发布到官网还是公司的私有服务器上，都需要正确的仓库地址和用户名。 你可运行下方的命令来检查，本地的npm registry地址是否正确。$npmconfiglslerna 我们需要全局安装lerna工具$npmi -g lerna# 或$ yarn global add lerna初始化一个lerna工程在这个例子中，我将在我本地d:/jobs 根目录下初始化一个lerna工程。1、在d:/jobs下创建一个空的文件夹，命名为lerna-demo$mkdirlerna-demo初始化 通过cmd进入相关目录，进行初始化$cdd:/jobs/lerna-demo$ lerna init执行成功后，目录下将会生成这样的目录结构。- packages(目录)- lerna.json(配置文件)- package.json(工程描述文件)3、添加一个测试package默认情况下，package是放在packages目录下的。# 进入packages目录cdd:/jobs/lerna-demo/packages# 创建一个packge目录mkdirmodule-1# 进入module-1 package目录cdmodule-1# 初始化一个packagenpminit -y执行完毕，工程下的目录结构如下:--packages	--module-1		package.json--lerna.json--package.json4、安装各packages依赖 这一步操作，官网上是这样描述的在当前的Lerna仓库中引导包。安装所有依赖项并链接任何交叉依赖项。$ cd d:/lerna-demo$ lerna bootstrap在现在的测试package中，module-1是没有任何依赖的，因此为了更加接近真实情况。你可已在module-1的package.json文件中添加一些第三方库的依赖。 这样的话，当你执行完该条命令后，你会发现module-1的依赖已经安装上了。5、发布 在发布的时候，就需要git工具的配合了。 所以在发布之前，请确认此时该lerna工程是否已经连接到git的远程仓库。你可以执行下面的命令进行查看git remote -v// print logorigin  git@github.com:meitianyitan/docm.git (fetch)origin  git@github.com:meitianyitan/docm.git (push)本篇文章的代码托管在Github上。因此会显示此远程链接信息。 如果你还没有与远程仓库链接，请首先在github创建一个空的仓库，然后根据相关提示信息，进行链接。$ lerna publish执行这条命令，你就可以根据cmd中的提示，一步步的发布packges了。实际上在执行该条命令的时候，lerna会做很多的工作。-  Run the equivalent of  `lerna updated`  to determine which packages need to be published.-  If necessary, increment the  `version`  key in  `lerna.json`.-  Update the  `package.json`  of all updated packages to their new versions.-  Update all dependencies of the updated packages with the new versions, specified with a  [caret (^)](https://docs.npmjs.com/files/package.json#dependencies).-  Create a new git commit and tag for the new version.-  Publish updated packages to npm.到这里为止，就是一个最简单的lerna的工作流了。但是lerna还有更多的功能等待你去发掘。lerna有两种工作模式,Independent mode和Fixed/Locked mode，在这里介绍可能会对初学者造成困扰，但因为实在太重要了，还是有必要提一下的。lerna的默认模式是Fixed/Locked mode，在这种模式下，实际上lerna是把工程当作一个整体来对待。每次发布packges，都是全量发布，无论是否修改。但是在Independent mode下，lerna会配合Git，检查文件变动，只发布有改动的packge。lerna最佳实践为了能够使lerna发挥最大的作用，根据这段时间使用lerna 的经验，总结出一个最佳实践。下面是一些特性。采用Independent模式根据Git提交信息，自动生成changelogeslint规则检查prettier自动格式化代码提交代码，代码检查hook遵循semver版本规范大家应该也可以看出来，在开发这种工程的过程的，最为重要的一点就是规范。因为应用场景各种各样，你必须保证发布的packge是规范的，代码是规范的，一切都是有迹可循的。这点我认为是非常重要的。buglerna 3.16.4中的一个bugEnter a custom version? 输入项如果直接enter的话会报错查看更多			https://blog.csdn.net/qq_37653449/article/details/101241968
52			['给Python代码加上酷炫进度条的几种姿势_AI科技大本营-CSDN博客']			作者 | 刘早起来源 | 早起Python（ID: zaoqi-python）大家好，在下载某些文件的时候你一定会不时盯着进度条，在写代码的时候使用进度条可以便捷的观察任务处理情况，除了使用print来打印之外，今天本文就介绍几种给你的Python代码加上酷炫的进度条的方式。自定义ProgressBar最原始的办法就是不借助任何第三方工具，自己写一个进度条函数，使用time模块配合sys模块即可import sysimport timedef progressbar(it, prefix="", size=60, file=sys.stdout):    count = len(it)    def show(j):        x = int(size*j/count)        file.write("%s[%s%s] %i/%i\r" % (prefix, "#"*x, "."*(size-x), j, count))        file.flush()            show(0)    for i, item in enumerate(it):        yield item        show(i+1)    file.write("\n")    file.flush()    for i in progressbar(range(15), "Computing: ", 40):    do_something()    time.sleep(0.1)自己定义的好处就是可以将进度条定义成我们想要的形式比如上面就是使用#与·来输出，为什么不用print？因为sys.stdout就是print的一种默认输出格式，而sys.stdout.write()可以不换行打印，sys.stdout.flush()可以立即刷新输出的内容。当然也可以封装成类来更好的使用[1]，但效果是类似的。from __future__ import print_functionimport sysimport reclass ProgressBar(object):    DEFAULT = 'Progress: %(bar)s %(percent)3d%%'    FULL = '%(bar)s %(current)d/%(total)d (%(percent)3d%%) %(remaining)d to go'    def __init__(self, total, width=40, fmt=DEFAULT, symbol='=',                 output=sys.stderr):        assert len(symbol) == 1        self.total = total        self.width = width        self.symbol = symbol        self.output = output        self.fmt = re.sub(r'(?P<name>%\(.+?\))d',            r'\g<name>%dd' % len(str(total)), fmt)        self.current = 0    def __call__(self):        percent = self.current / float(self.total)        size = int(self.width * percent)        remaining = self.total - self.current        bar = '[' + self.symbol * size + ' ' * (self.width - size) + ']'        args = {            'total': self.total,            'bar': bar,            'current': self.current,            'percent': percent * 100,            'remaining': remaining        }        print('\r' + self.fmt % args, file=self.output, end='')    def done(self):        self.current = self.total        self()        print('', file=self.output)        from time import sleepprogress = ProgressBar(80, fmt=ProgressBar.FULL)for x in range(progress.total):    progress.current += 1    progress()    sleep(0.1)progress.done()tqdm之前我们说了，自定义的好处就是可以自己修改，那么使用第三方库的好处就是可以偷懒，不用自己写，拿来就能用。比如提到Python进度条那肯定会想到常用的tqdm，安装很简单pip install tqdm即可，使用也很简单，几行代码即可实现上面的进度条from tqdm import trangeimport timefor i in trange(10):     time.sleep(1)当然tqdm作为老牌的Python进度条工具，循环处理、多进程、多线程、递归处理等都是支持的，你可以在官方GitHub上学习[2]、解锁更多的玩法。Rich上面两种实现Python进度条的方法都学会了吗，虽然简单但是看上去并不漂亮，颜色也比较单调。所以最后压轴出场的就是一款比较小众的第三方库Rich[3]。Rich主要是用于在终端中打印丰富多彩的文本(最高支持1670万色)所以当然可以使用Rich打印进度条，显示完成百分比，剩余时间，数据传输速度等都可以。并且样式更加酷炫，并且它是高度可配置的，因此我们可以对其进行自定义以显示所需的任何信息。使用也很简单，比如我们使用Rich来实现一个最简单的进度条。from rich.progress import trackimport  timefor step in track(range(30)):    print('早起Python')    time.sleep(0.5)同时Rich支持多个进度条，这在多任务情况下监控的进度很有用(使用方法见官方文档)参考资料[1]stackoverflow: https://stackoverflow.com/questions/3160699/python-progress-bar[2]Tqdm: https://github.com/tqdm/tqdm[3]Rich: https://github.com/willmcgugan/rich推荐阅读懂语言者得天下：NLP凭什么被称为人工智能的掌上明珠？如何用NLP辅助投资分析？三大海外机构落地案例详解张红忠：智慧化时代，如何通吃5G模组、AIoT？屏保壁纸引发血案，三星手机瞬间变砖借助大数据进行社交媒体营销，企业们得这么玩！力挺比特币的世界第2交易员：仅次于索罗斯，连续25年无亏损你点的每个“在看”，我都认真当成了喜欢			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106535960
53			['idea2019导入maven项目中的某些问题_qq_30385099的博客-CSDN博客']			idea2019导入maven项目中的某些问题idea2019导入maven项目,会出现很多莫名其妙的问题，需要注意的是如果是idea2019的版本是3月以前的，那会maven需要下载3.6.1以前版本，3.6.2版本不兼容需要3.6.1版本的童鞋可在官网下载，或者用下面链接获取：链接：https://pan.baidu.com/s/1cO7atc-pk4_GKAy3GMTeng提取码：ktl2配置maven的setting.xml文件如果导入依赖出现问题时，可从以下几方面查看：注：这部分是我后来加上去的，有可能是根本原因哦。首先查看maven的conf目录下的setting查看错误，我建议去idea中打开maven的setting,有错误会提示报红。选中maven项目，右键找到选项maven，点击Open ‘setting.xml’，setting.xml 的配置主要是两个，一个是maven仓库的位置，一个是阿里的国内镜像。maven仓库说白了就是一个文件夹，，里面存放的是以后从镜像中下载的依赖jar包。按道理maven仓库可以随意创建在任何位置，但是这个文件使用的多了可能会很大，建议不要放在C盘；阿里的国内镜像，这个也可以不配置，但是从外网下载jar包的速度一定是非常的慢，所以需要国内的镜像路径，这样可提高下载速度。首先是查看maven仓库的位置，路径是否正确，如下图，这是我的maven仓库的路径。切记改为你自己的仓库位置，还有路径之间分隔符是 \第二个就是国内镜像了，镜像配置如下：<mirror><id>alimaven</id><name>aliyun maven</name><url>http://maven.aliyun.com/nexus/content/groups/public/</url><mirrorOf>central</mirrorOf></mirror>这个问题就很难缠了，我是在idea下才看到的问题，也不知道为什么。setting.xml 目录下会有镜像的根容器， ，如果说你为了不改变原有的状态，直接把镜像文件复制到原有的根容器下，也许会报错，目前我还不知道这是什么原因，如下图：解决办法：把原有的删掉，中间的内容也删掉，或者注释掉，自己写一个，把镜像配置到里面去，就不会报错了，如下图：如果问题还不能解决，还是会报错，就在下面的方法找解决办法吧。。。查看maven配置选中File -> Settings ->Build, Execution, Deployment ->Build Tools -> Maven,查看框中的目录是否是安装maven目录查看import的jre是否选中本地的jdk和Runner的jre是否一致改好之后点击Apply，然后ok，等待进度运行结束查看jdk配置jdk选择本地路径，然后Apply，ok等待如果还不能解决问题，pom.xml文件依然显示为红色，报错Unable to import maven project: See logs for details，可试一下点击Maven的下面个两个图标，或者选中项目，右键，找到Maven选项，点击Reimport如果还是不行，依然显示红色，可尝试Maven的clean，等待下载完成，再点击install，等待下载完成。如果还是不能解决问题，但是pom.xml部分报红，部分不报，可尝试先点击m，然后在2部分的框中输入mvn -v idea:idea,来补全下载maven插件。然后等待。如果依然无法加入依赖，说是缺少什么包类似的话语。请多执行几次clean，validate,comlile,test,package,verify以及install，不按照顺讯执行也行，最重要的是clean和install，多试几次之后等到install执行成功，说明依赖导入，我也没有完全明白这是为什么，也许是因为按照错误说的那样上述的哪个环节缺少父依赖，但是也不知道是什么模块缺少如果依赖文件添加成功了，pom文件中不报错，但是在maven的Dependencies中报红，这时把pom文件中的所有依赖删掉或者注释掉，Reimport（重新导入），等待不报红之后，再把pom文件中的所有依赖撤销回来，这是一般就不报红了以上这两种方法在 Idea2017,2018,2019中我是都试过，百试百灵			https://blog.csdn.net/qq_30385099/article/details/101313398
54			['Uber 前无人驾驶工程师告诉你，国内无人驾驶之路还要走多久？_AI科技大本营-CSDN博客']			受访者 | Graviti 创始人&CEO 崔运凯记者 | Aholiab，编辑 | Carol出品 | AI科技大本营（ID:rgznai100）经过数年的发展，现在的人们谈到“AI”已经不再像过去一般感到遥不可及。但 AI 在国内发挥的作用仍然只是冰山一角，许多应用依旧没有落地，产业链等待完善，国内 AI生态的发展还需要一定的时间。“2018 年我刚回国创业，想要做机器学习的模型，但找遍整个市场，都找不到一家满意的供应商。”AI 数据SaaS提供商 Graviti 创始人崔运凯告诉记者。崔运凯毕业于上海交通大学与美国宾西法尼亚大学，曾是 Uber 无人驾驶部门的Tech Lead Manager ，属于该部门最早一批员工。2018 年，崔运凯从 Uber 离开，回国后却发现，国内不仅无人驾驶行业进度缓慢， AI 产业链也非常不完善，这给许多AI 创业公司带来非常大的挑战。发现了行业在这部分的需求和空白后，2019 年，崔运凯正式创办了Graviti，致力于帮助人工智能开发者更好地管理和使用数据，通过涵盖数据全生命周期的一站式AI数据服务SaaS平台，加速数据在供需方之间的流通，为人工智能行业赋能。团队也吸引到了哈佛，UT Austin，密歇根大学以及国内上海交大、复旦、同济等一流高校的人才。从 Uber 无人驾驶部门的Tech Lead Manager，到国内人工智能行业数据服务商，崔运凯见证了无人驾驶行业的崛起和发展，也对 AI 行业有着独到的见解。近日，CSDN记者采访了崔运凯，跟随着他的技术成长生涯，一起来听听他对无人驾驶与国内 AI 行业现状的理解。Graviti创始人&CEO 崔运凯因一个”秘密项目“，正式踏入无人驾驶的大门CSDN：请简单介绍下你的个人经历？崔运凯：2012 年，我到了美国宾西法尼亚大学学习。2014 年毕业后留下来当了一年助理研究员，但发现这不是自己的兴趣所在，在一次机缘巧合的机会下，我后来在Uber的老板给我打来了一个电话，说他们在匹斯堡做一个秘密项目，我飞到了匹斯堡一看，原来是在做无人驾驶。当时的老板是 CMU（卡耐基梅隆大学） 的一位教授，他离开 CMU 后加入的 Uber 。看完项目，临走前我问他为什么会加入 Uber ？他对我说了一句话让我至今为止都非常印象深刻：“ It's a once in a lifetime opportunity“。他认为这种改变世界的机会可能一生也遇不到一次，所以就加入了。我也深受他的感染和影响，所以在 2015 年就加入了 Uber 做无人驾驶的事情。加入Uber 时，Uber 还算很早期的时候。50 多号人在一个废弃的工厂楼里办公，像一个创业公司一样，后来至少搬过 4 个办公点，见证了它的成长。2018 年 5 月我离开时，公司已经将近 1500 多人。CSDN：在 Uber 时你主要负责的内容是哪些？崔运凯：我是在 2017 年 2 月份时被提拔上了 Tech Lead的，带领一个小的工程师团队去做高精度地图的规模化生产，尤其是用 AI 算法来辅助人工实现规模化的生产。因为当时高精度地图是无人驾驶中比较重要的一部分。如何能快速升级和生产地图，是当时无人驾驶的厂商比较重要的需要攻克的难题，当时也有幸参与了整个过程2018 年离开了美国，2019 年 4 月时创立了 Graviti。Graviti比较幸运的是，在 2019 年资本市场不是很热闹的情况下，获得了包括红杉资本、云启资本、真格基金、风和投资的投资，还是特别感激大家对我们的支持。无人驾驶技术的发展CSDN：很多人在刚开始职业生涯的时候会选择偏向互联网应用的大公司耕耘，你为何会选择自动驾驶领域来攻坚？有什么吸引你的地方？崔运凯：关于谷歌和百度做无人驾驶的问题，我们可以回顾 2015 年初，那时候还没有Waymo这家公司，当时谷歌的无人驾驶叫 Google X，整个 X 部门都是做无人驾驶的研究员，人数大概不到100 人，所以只是谷歌内部的一个部门。而Waymo 成立到组建其实已经是 2016年、2017 年的事情，当时 Google 并没有严肃地将它作为一个主要的商业业务去思考。同理，百度也有很多战略业务，无人驾驶只是他们战略业务之一，百度有非常优秀的人才，但是更多的是把无人驾驶作为前沿的研究而非可以商业化的应用。出行的基本原则满足的是人从 A 地点到 B 地点的诉求，实际上通过 Uber，打车、无人驾驶都可以满足，无人驾驶只是选项之一。而 Waymo这种纯粹的无人驾驶公司会面临一个问题：用户最初可能会因为新鲜感而选择尝试他们的服务，但长期看来，用户只是需要满足从 A 到 B 的需求，到后期追求的更多是便捷，所以还是会选择打车。但 Uber 的目标是：承认我的网络里包含了无人驾驶、有人驾驶这种混合的形态，永远以满足乘客从A到B的需求为优先。这样的商业模式在我看来才更可行。我之所以加入无人驾驶领域，是因为我发现无人驾驶实际上会涉及很多先进的技术，大大帮助知识层面的提升，需要学很多东西才能把无人驾驶做好，至少这部分是让我个人比较兴奋的，所以选择了无人驾驶这个方向。CSDN：你认为无人驾驶的终极目标是「量产商用」，还是「应用普及」？中美技术的发展差异如何？崔运凯：无人驾驶的形态有可能是长期的混合式的形态。因为在 2016 年 9 月时，Uber 为所有的匹斯堡的用户提供无人驾驶服务，每个人都有可能随机匹配到一辆无人驾驶车。我其实认为这已经是很好的无人驾驶商业化落地的方式了。所以我认为，无人驾驶会以这种慢慢进入人们生活的方式，潜移默化地不断提高、迭代自己，通过与用户的交互体验来不断变得完美。是一个循序渐进的过程，不是一夜之间就被所有人接受。从技术层面来说，我认为整体的无人驾驶技术还有挺长的一段路要走。美国的无人驾驶技术确实要比国内的领先很多，在市场应用、算法方面是差不多的，但实际上我们欠缺的是整个产业链上的提升，包括大量的人才积累、操作系统、硬件和芯片能力，也包括国家的一些政策等等，国内都有相对大的空白，有很大的空间可以去打开和探索。让AI触手可及CSDN：Graviti 诞生的经过是怎样的？崔运凯：我在Uber从事无人驾驶研发的几年时间中，需要处理大量图像、点云等非结构化数据，并用这些数据训练算法。当时，Uber内部为了方便算法团队加快算法迭代效率，动用了很多资源开发了Michelangelo机器学习平台。很多有名的开源框架都诞生在这一过程中，包括Horovod。我碰巧作为最早的用户，看到了这个平台的成长，踩过的坑和积累的经验。而这样的投入对于一个初创的人工智能公司是不可想象的。而在获取真值方面，Uber不仅在印度有上千人的数据生产团队，还将部分数据的需求外包给位于西雅图的一家初创公司，除了要承受昂贵的价格，冗长的等待时间，还要面对海量数据的对接、跨境分发、检索、整理及增值数据的保存和使用等一系列难题。2018年离开Uber回国后，我加入了一家高精度地图初创公司担任合伙人，因高精度地图研发需要收集海量数据并训练大量模型，为了管理和使用这些数据，我们一直在寻找类似于Michelangelo的平台。我们找了各种各样的供应商，甚至包括多家国内头部云服务商，可是没有一家可以满足我们的需求，在那个瞬间感觉特别无助。当时我意识到无论是国内还是国外，人工智能研发的整个工具链都处于非常早期阶段而且不完善。如果我们再做一家人工智能公司，还会遇到同样的问题，还是要花很大代价把这些问题再解决一遍。后来我去找了很多在人工智能领域创业的朋友，发现他们也有同样的痛点。与其这样，不如专门做一家帮助开发者解决以图像、文字、视频为代表的非结构化数据管理和使用痛点的公司，让开发者从繁杂的数据管理中脱身，更好地将时间和精力集中在解决业务问题上。我们希望所有的开发者都可以用到和大公司一样好用的工具，让人工智能不再遥远，我们创立Graviti就是肩负着这样的使命的。CSDN：我们注意到Graviti的Slogan 是“让AI触手可及”，Graviti所面临的挑战是怎样的？难点在哪里？崔运凯：这里我们先来区分一下结构化数据和非机构化数据。日常数据可以分为由程序生成的结构化数据与以视频、图像、文字为代表的非结构化数据。平日里大家接触到的数据处理大部分是针对结构化数据的处理，比如Excel表格，数据库等。而人工智能面对的更多是图像、文字、视频等非结构化数据。这些数据所占用的存储资源及处理难度是结构化数据的几百万倍甚至是几亿倍。举个例子，Excel表格中100条数据只有几KB大小，但是一个图片可能需要几MB的空间，而一个视频则需要几百MB的空间。这很直观的展现了结构化数据和非结构化数据在数量级上的对比。处理一个Excel表格和几张图片都是相对简单的任务，基本在个人电脑上都可以处理。但是AI要处理的任务可能是上百万个几百MB的视频，这里程序所面临的挑战和所需要耗费的存储和算力成本可想而知。全世界每天产生亿万量级的非结构化数据，是否能有效地处理好利用好这些数据，决定了人工智能的发展进程。Graviti希望通过高效调度大量的算力及存储，利用大规模分布式并行化技术，打造一站式的创新解决方案，帮助解决非结构化数据从获取、管理、加工到使用的完整数据旅程中所面临的问题，这也是我们的价值所在。CSDN：Graviti提供怎样的产品？又是如何解决开发者的痛点的？崔运凯：大家猜猜支持一个10人算法团队高效运转需要多少资源？我们通过深度调查发现，至少要三个软件工程师提供工具开发和运维服务，需要配置百万元左右的深度学习训练机器，及百人左右的标注团队和上百TB到PB级的共享存储空间。这些加起来往往需要花掉企业千万级前置成本和百万级的维护费用。即使这些都具备，算法工程师还要将大量的时间花在找数据，清理数据，管理权限和可视化上，真正用于算法开发的时间屈指可数。针对这个情况，Graviti提供了面向开发者的SaaS工具，集数据集管理，沙箱训练和模型评估于一体。打通数据在人工智能应用开发的各个环节中的流转。让企业0前置成本启动人工智能应用开发项目，后期费用跟随团队的扩张而增加、收缩而减少，让开发者真正专注在重要的事情上。比如我们与淞泓智能的合作，就使用了我们的数据管理系统和模型评估系统，甚至包括模型管理系统，并为它搭建了一套在线测试平台，让它更好地去测试未来无人驾驶车辆的系统安全性；在伯克利（UC Berkeley）的合作中，也是通过我们的数据管理平台、评估系统，为其搭建了一个平台用来支持他们面向世界级的顶级学术人员和开发者发布的挑战赛，这样大家可以更好地促进相关技术的发展。针对开发者对于真值数据的需求，Graviti提供了基于SaaS的标注服务。和其他标注公司不一样的是，我们的数据标注服务是一键式的。国内的标注公司通常是先联系BPO（商务流程外包Business Process Outsourcing），在线下的微信群里对接需求，发数据文档，他再去联系标注员，而很多标注员可能是第一次接触这样的任务，也没有特别适用的工具。但是在我们的体系里，所有对接都是在软件中交互完成，分发程序有一套算法，会自动找到最合适的标注员，通过发现任务、自主登录、接受培训考试去完成这样的工作。整个过程都是自动化的，效率和准确率都有很大的提升。我们会针对客户的业务情况做咨询，很多客户是做不到明确架构需求的，这时候就需要有人来从专业角度做梳理和设计，这也是我们服务的独特性所在。我们也将在未来的一定时间点考虑开源我的软件或是去做开放平台，和更多合作伙伴做生态上的整合，协同解决更多AI问题，打造像Github一样被开发者喜爱的平台。AI 只是个工具，不要太迷信 AICSDN：除了汽车领域之外，未来还会为其他行业或领域提供服务吗？崔运凯：会的。汽车是目前的重点领域，毕竟在这个领域很多年了。而AI技术本身其实是通用的，我们做这套技术也是通用的，所以我们也在思考全球疫情蔓延的情况下，能否为医疗赋能，通过人工智能为人类健康做出自己力所能及的贡献。同时也希望能为互联网视频、新零售、智能制造、在线教育等领域智能化升级赋能。CSDN：对于AI技术，你有什么想说的？崔运凯：我认为，最重要的其实是「知其然也要知其所以然」。对于技术，开发者一定要不断向自己提问，不要满足于自己现有的知识，更多要问“为什么、可不可以做到更好”，发现一些新的方式加以创新，才能将 AI 这个技术用好。另外，也希望所有的 AI 开发者不要太过于迷信 AI ，因为 AI 归根结底只是个工具，一个有价值的问题可能可以由多种不同的工具解决，AI 只是其中一种。所以用最好的工具解决它最应该解决的问题才是大家应该追求的。推荐阅读追忆童年，教你用Python画出儿时卡通人物如何用NLP辅助投资分析？三大海外机构落地案例详解Gary Marcus：因果熵理论的荒诞和认知科学带给AI的11个启示 | 文末赠书AI 终极问题：我们的大脑是一台超级计算机吗？借助大数据进行社交媒体营销，企业们得这么玩！力挺比特币的世界第2交易员：仅次于索罗斯，连续25年无亏损你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106561113
55			['还在苦恼机器学习和线性回归？这篇总结拿走不谢 | 原力计划_AI科技大本营-CSDN博客']			作者 | 听星的朗瑞责编 | 王晓曼出品 | CSDN博客题图 | 东方IC什么是机器学习？机器学习是一种实现人工智能的方法，从数据中寻找规律、建立关系，根据建立的关系去解决问题，从数据中进行经验学习，实现自我优化与升级。维基百科给出的定义：机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能机器学习是对能通过经验自动改进的计算机算法的研究机器学习是用数据或以往的经验，以此优化计算机程序的性能标准一种经常引用的英文定义是：A computer program is said to learn from experience E with respectto some class of tasks T and performance measure P, if its performance at tasksin T, as measured by P, improves with experience E.机器学习的应用场景数据挖掘计算机视觉自然语言处理证券分析股票涨跌预测电影票房预测医学诊断机器人DNA测序……与人工智能比较，我们可以看到人工智能的主要应用场景也都是机器学习的应用场景，这就印证了之前所说，机器学习是实现人工智能的主流方法。实现机器学习的基本框架将训练数据输入到计算机，计算机自动求解数据关系，在新的数据上做出预测或给出建议。机器学习的类别监督式学习（Supervised Learning）——训练数据包括正确的结果（标签-label）对于监督式学习，我们在一开始给出的数据中就已经告诉计算机正确的结果标签：红色的圆对应类别1，蓝色的叉对应类别2，绿色的星对应类别3。根据该结果，监督式学习就会自动地找出数据的边界（图中虚线部分），以后计算机再得到新的数据，不知道其是圆，叉或者星时，就会根据其所在的位置，自动将其划分为对应的类别。监督式学习包含线性回归逻辑回归决策树神经网络、卷积神经网络、循环神经网络……无监督式学习（Unsupervised Learning）——训练数据不包括正确的结果对于无监督式学习，我们在一开始给出的数据中没有告诉计算机正确的结果标签，只是要求计算机将数据分成3类，这样处理数据时就找不出数据的边界，但是却能根据要求，将比较接近的数据划分为一类，最终将所有数据分为3类。当得到新的数据后，将根据其与3类数据的接近程度自动划分为其中一种。无监督式学习包含聚类算法半监督式学习（Semi-supervised Learning）——训练数据包括少量正确的结果对于半监督学习，给出的标签数据相对较少一些，但也能根据这些标签数据找到数据的边界，将新数据划分为其中一种。强化学习（Reinforcement Learning）——根据每次结果收获的奖惩进行学习，实现优化举个例子，假设有个行走的机器人，它的前面有一面墙，直走的话会撞上去，机器人尝试不同的走法，如上图的行走策略，第一种通过了这面墙为GOOD，第二种撞上了这面墙为BAD。编程时设立奖惩规则，通过+3分，失败-3分，规定机器人行走优化条件是分数越高越好，让程序自动寻找获得高分的方法。学习方式的应用什么是回归分析 (Regression Analysis) ？回归分析是根据数据，确定两种或两种以上变量间相互依赖的定量关系。函数表达式：回归分析的种类：下边将具体讲解线性回归技术。线性回归介绍回归分析中，变量与因变量存在线性关系。函数表达式：y = ax + b线性回归问题求解建立模型的步骤：1、确定P、A间的定量关系2、根据关系预测合理价格3、做出判断将表中数据用散点图表示出来具有线性关系，建立线性模型：y = ax + b现在我们只需要找到合适的a和b，就能解决问题。途径：假设x为变量，y为对应结果，y’为模型输出结果，目标变为：y’尽可能接近y，如下图（m为样本数）因为后边要求导，为了约掉求导后得到的2m，这里除以2m，变为：即为该模型的损失函数J，其值越小越好。可以看出J的值是与a、b有关的，那如何找到这个极小值呢？梯度下降法求解线性回归可以用梯度下降法进行求解，梯度下降法是寻找极小值的一种方法，通过向函数上当前点对应梯度（或者是近似梯度）的规定步长距离点进行迭代搜索，直到在极小点收敛。应用此方法求损失函数J的极小值时，首先创建临时变量temp_a和temp_b，然后重复计算直到收敛，此时a与b的值就是要寻求的值：由此得到了线性模型的表达式：y = ax + b将单因子变量x的值代入就能得到对应的因变量预测值，最后对预测结果做出判断。原文链接：https://blog.csdn.net/weixin_45092215/article/details/106033448推荐阅读我佛了！用KNN实现验证码识别，又 Get 到一招！深度学习基础总结，无一句废话（附完整思维导图）程序员在家办公没显示屏，我被领导鄙视了华为 5G、阿里检测病毒算法、腾讯 AI 一分钟诊断，国内抗疫科技大阅兵！对不起，我把APP也给爬了超级账本Hyperledger Fabric中的Protobuf到底是什么？你点的每个“在看”，我都认真当成了AI			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106247375
56			['追忆童年，教你用Python画出儿时卡通人物_AI科技大本营-CSDN博客']			作者 | 张同学来源 | 凹凸数据大家好，我是张同学。又到一年一度的国际儿童节，作为逢节必过的程序猿，怎么可以放过这个学习技能的机会呢？于是，今天我们来学习 Python 的 Turtle 库绘制童年的卡通人物，一起做回年轻的那个少年。Turtle图形库简介Turtle 库，又称海龟库，是 Python 语言中一个很流行的绘制图像的函数库。大家可以想象一个小乌龟，在一个横轴为x、纵轴为y的坐标系原点，(0,0)位置开始，它根据一组函数指令的控制，在这个平面坐标系中移动，从而在它爬行的路径上绘制了图形。Turtle 库一般 Python 环境会自带，如果没有这个库查询一下安装方法。常用函数1、画笔控制函数penup():抬起画笔；pendown():落下画笔；pensize(width):画笔宽度；pencolor(color):画笔颜色；color为颜色字符串或者rgb值2、运动控制函数forward(d)/fd(d):直行d个像素；circle(r, extent = None):绘制半径为r，角度为extent的弧形，圆心默认在海龟左侧距离r的位置；3、方向控制函数setheading(angle)/seth(angle):改变前进方向；left(angle):海龟左转；right(angle):海龟右转；代码演示下面用海龟库完成小黄人的绘制，下面是部分源码，完整源码以及其余的卡通图案绘制源码见文末。import turtle as tt.pensize(4)t.speed(10)# =======头======def head():    t.penup()    t.fillcolor("#FFEE26")    t.goto(-130, 10)    t.pendown()    t.begin_fill()    t.seth(81)    t.fd(90)    t.seth(100)    t.circle(-500, 3)    t.circle(-100, 10)    t.circle(-200, 25)    t.circle(-110, 20)    t.circle(-140, 30)    t.circle(-180, 30)    t.circle(-200, 20)    t.circle(-140, 10)    t.circle(-160, 50)    t.seth(85)    t.fd(-148)    t.seth(-112)    t.circle(-250, 14)    t.fd(200)    t.right(80)    t.fd(190)    t.seth(110)    t.circle(-200, 7)    t.circle(-130, 30)    t.end_fill()绘制过程：6个卡通图案绘制源码（网页打开直接下载）：https://alltodata.cowtransfer.com/s/2b943c8a803e45注：文中卡通图案绘制参考了部分教程。【END】6月2日20:00，CSDN 创始人&董事长、极客帮创投创始合伙人蒋涛携手全球顶级开源基金会主席、董事，聚焦中国开源现状，直面开发者在开源技术、商业上的难题，你绝不可错过的开源巅峰对谈！立即免费围观：更多精彩推荐☞我只是追个直播，结果被拉进大咖们的群面对面群聊……☞微信公众号关闭iOS端虚拟支付业务；苹果「Apple 登录」存安全漏洞；谷歌推迟发布Android 11 Beta| 极客头条☞可怕！CPU 竟成了黑客的帮凶！☞如何用NLP辅助投资分析？三大海外机构落地案例详解☞这 10 个云计算错误，会让你的业务一蹶不振！☞好扑科技结合区块链行业发展趋势，重磅推出“好扑区块链合伙人”计划你点的每个“在看”，我都认真当成了喜欢			https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/106484802
