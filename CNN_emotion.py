# -*- coding: utf-8 -*-
"""CNN_emotion

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11ChnSjnm3-2xseEw7WFD2R06_qQu2pbk
"""

from google.colab import drive
drive.mount('/content/drive')
print('Files')
!ls/content/drive/'My Drive'

import os
from google.colab import drive
drive.mount('/content/drive')

path = "/content/drive/My Drive"

os.chdir(path)
os.listdir(path)

import random
import re
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import csv
def fix_seed(seed=234):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)

def accuracy(output, target):
    output = torch.round(torch.sigmoid(output))
    correct = (output == target).float()
    acc = correct.mean()
    return acc


def get_word2idx(tokenized_corpus):
    vocabulary = []
    for sentence in tokenized_corpus:
        for token in sentence:
            if token not in vocabulary:
                vocabulary.append(token)

    word2idx = {w: idx + 1 for (idx, w) in enumerate(vocabulary)}
    word2idx['<pad>'] = 0

    return word2idx


def get_model_inputs(tokenized_corpus, word2idx, labels):
    vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]
    sent_lengths = [len(sent) for sent in vectorized_sents]
    max_len = max(sent_lengths)
    sent_tensor = torch.zeros((len(vectorized_sents), max_len)).long()
    for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):
        sent_tensor[idx, :sentlen] = torch.LongTensor(sent)
    label_tensor = torch.FloatTensor(labels)
    return sent_tensor, label_tensor

class CNN(nn.Module):
  def __init__(self, vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout):
    super(CNN, self).__init__()
    # Create the embedding layer as usual
    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
    # in_channels -- 1 text channel
    # out_channels -- the number of output channels
    # kernel_size is (window size x embedding dim)
    self.conv = nn.Conv2d(
      in_channels=1, out_channels=out_channels,
      kernel_size=(window_size, embedding_dim))
    # the dropout layer
    self.dropout = nn.Dropout(dropout)
    # the output layer
    self.fc = nn.Linear(out_channels, output_dim)
        
  def forward(self, x):
    # x -> (batch size, max_sent_length) 
    embedded = self.embedding(x)
    # embedded -> (batch size, max_sent_length, embedding_dim)    
    # images have 3 RGB channels 
    # for the text we add 1 channel
    embedded = embedded.unsqueeze(1)
    # embedded -> (batch size, 1, max_sent_length, embedding dim)
    # Compute the feature maps      
    feature_maps = self.conv(embedded)#(batch_size,n_filters,max_sent_length-window_size+1,1)
    feature_maps = feature_maps.squeeze(3)
    # Apply ReLU
    feature_maps = F.relu(feature_maps)
    # Apply the max pooling layer
    pooled = F.max_pool1d(feature_maps, feature_maps.shape[2])
    pooled = pooled.squeeze(2)
    dropped = self.dropout(pooled)
    preds = self.fc(dropped)
    return preds

import jieba
from sklearn.model_selection import train_test_split
data=list()
data_labels=list()
train=list()
train_labels=list()
test=list()
test_labels=list()
csvFile = open('./情感分析/Comment_dataset.csv','r',encoding='GBK')
reader = csv.reader(csvFile)
for item in reader:
    sentence=item[0]
    data.append(sentence)
    data_labels.append(float(item[1]))
train,test,train_labels, test_labels = train_test_split(data,data_labels,random_state=0,test_size=0.2)

def get_tokenized_corpus(corpus):
    tokenized_corpus = []  # Let us put the tokenized corpus in a list
    for sentence in corpus:
      seg_list=jieba.cut(sentence, cut_all=True)
      new_sentence=[]
      for word in seg_list:
        new_sentence.append(word)
      tokenized_corpus.append(new_sentence)
    return tokenized_corpus

###train
tokenized_corpus = get_tokenized_corpus(train)
word2idx = get_word2idx(tokenized_corpus)
train_sent_tensor, train_label_tensor = get_model_inputs(tokenized_corpus, word2idx, train_labels)

print(tokenized_corpus)

valid=list()
valid_labels=list()
csvFile = open('./Yang-Sihui-03/data/dev.csv','r',encoding="ISO-8859-1")
reader = csv.reader(csvFile)
for item in reader:
    if reader.line_num==1:#ignore the first row
      continue
    sentence=item[1]
    match=re.match(r'(.*)[<](.*)[/][>](.*)',sentence)
    edited=match.group(1)+item[2]+match.group(3)
    valid.append(edited)
    #sent2idx[edited]=item[0]
    valid_labels.append(float(item[4]))

tokenized_valid_corpus = get_tokenized_corpus(test)
#word2idx = get_word2idx(tokenized_valid_corpus)
valid_sent_tensor, valid_label_tensor = get_model_inputs(tokenized_valid_corpus, word2idx, test_labels)
print(valid_sent_tensor)
print(test_labels)

###development
fix_seed()
EPOCHS=20
LRATE=0.001
EMBEDDING_DIM=50
N_OUT_CHANNELS=100
WINDOW_SIZE=1
OUTPUT_DIM=1
DROPOUT=0.0001
model = CNN(len(word2idx), EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)
optimizer = optim.SGD(model.parameters(), lr=LRATE)
loss_fn = nn.BCEWithLogitsLoss()
feature_train = train_sent_tensor
target_train = train_label_tensor
feature_valid = valid_sent_tensor
target_valid = valid_label_tensor
print(f'Will train for {EPOCHS} epochs')
for epoch in range(1, EPOCHS + 1):
  model.train()
  optimizer.zero_grad()
  predictions = model(feature_train).squeeze(1)
  loss = loss_fn(predictions, target_train)
  train_loss = loss.item()
  train_acc = accuracy(predictions, target_train)
  loss.backward()
  optimizer.step()
  model.eval()
  with torch.no_grad():
    predictions_valid = model(feature_valid).squeeze(1)
    print(predictions_valid)
    valid_loss = loss_fn(predictions_valid, target_valid).item()
    valid_acc = accuracy(predictions_valid, target_valid)
  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:6.2f}% |')