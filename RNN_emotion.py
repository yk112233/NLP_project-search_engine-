# -*- coding: utf-8 -*-
"""RNN_emotion

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yerdLGu9xq9RGPGRnkuRDPS0oAPSiUu0
"""

from google.colab import drive
drive.mount('/content/drive')
print('Files')
!ls/content/drive/'My Drive'

import os
from google.colab import drive
drive.mount('/content/drive')

path = "/content/drive/My Drive"

os.chdir(path)
os.listdir(path)

import random
import re
import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import csv
def fix_seed(seed=1234):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

def get_word2idx(tokenized_corpus):
    vocabulary = []
    for sentence in tokenized_corpus:
        for token in sentence:
            if token not in vocabulary:
                vocabulary.append(token)

    word2idx = {w: idx + 1 for (idx, w) in enumerate(vocabulary)}
    word2idx['<pad>'] = 0
    return word2idx

def get_model_inputs(tokenized_corpus, word2idx, labels):
    vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]
    sent_lengths = [len(sent) for sent in vectorized_sents]
    max_len = max(sent_lengths)
    sent_tensor = torch.zeros((len(vectorized_sents), max_len)).long()
    for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):
        sent_tensor[idx, :sentlen] = torch.LongTensor(sent)
    label_tensor = torch.FloatTensor(labels)
    return sent_tensor, label_tensor

def get_tokenized_corpus(corpus):
    tokenized_corpus = []  # Let us put the tokenized corpus in a list
    for sentence in corpus:
      seg_list=jieba.cut(sentence, cut_all=True)
      new_sentence=[]
      for word in seg_list:
        new_sentence.append(word)
      tokenized_corpus.append(new_sentence)
    return tokenized_corpus

import jieba
from sklearn.model_selection import train_test_split
data=list()
data_labels=list()
train=list()
train_labels=list()
test=list()
test_labels=list()
csvFile = open('./情感分析/Comment_dataset.csv','r',encoding='GBK')
reader = csv.reader(csvFile)
for item in reader:
    sentence=item[0]
    data.append(sentence)
    data_labels.append(float(item[1]))
train,test,train_labels, test_labels = train_test_split(data,data_labels,random_state=0,test_size=0.2)

###train
tokenized_corpus = get_tokenized_corpus(train)
word2idx = get_word2idx(tokenized_corpus)
train_sent_tensor, train_label_tensor = get_model_inputs(tokenized_corpus, word2idx, train_labels)

class RNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,
                 bidirectional, dropout, pad_idx):

        super().__init__()

        self.bidirectional = bidirectional
        self.hidden_dim = hidden_dim
        self.embedding = nn.Embedding(vocab_size,embedding_dim,padding_idx=pad_idx)
        self.rnn = nn.RNN(embedding_dim,
                  hidden_dim,
                  batch_first=True,
                  bidirectional=bidirectional,
                  num_layers=1)
        if self.bidirectional:
            linear_hidden_in = hidden_dim * 2
        else:
            linear_hidden_in = hidden_dim
        self.fc = nn.Linear(linear_hidden_in, output_dim)
        self.dropout = nn.Dropout(dropout)
    def forward(self, text):
        embedded = self.dropout(self.embedding(text))
        all_hidden, last_hidden = self.rnn(embedded)
        if self.bidirectional:
            # Concat the final forward (hidden[0,:,:]) and backward (hidden[1,:,:]) hidden layers
            last_hidden = torch.cat((last_hidden[0, :, :], last_hidden[1, :, :]), dim=-1)
            # shape(last_hidden) = [B, D*2]
        else:
            last_hidden = last_hidden.squeeze(0)
            # shape(last_hidden) = [B, D]

        # Our predictions.
        preds = self.fc(self.dropout(last_hidden))
        # shape(logits) = [B, O]
        
        return preds

tokenized_valid_corpus = get_tokenized_corpus(test)
#word2idx = get_word2idx(tokenized_valid_corpus)
valid_sent_tensor, valid_label_tensor = get_model_inputs(tokenized_valid_corpus, word2idx, test_labels)
print(valid_sent_tensor)
print(test_labels)

import time
def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

def evaluate(model,criterion):
  epoch_loss = 0
  epoch_acc = 0
  model.eval()
  with torch.no_grad():
    Start=1
    End=65
    while(end<len(target_train)):
      predictions_valid = model(feature_valid[Start:End]).squeeze(1)
      valid_loss = criterion(predictions_valid, target_valid[Start:End]).item()
      valid_acc = accuracy(predictions_valid, target_valid[Start:End])
      
      epoch_loss += valid_loss
      epoch_acc += valid_acc.item()
    Start+=64
    End+=64
  return epoch_loss , epoch_acc / 17900

def accuracy(preds, y):
    """
    returns accuracy per batch
    """

    class_preds = nn.Softmax(dim=-1)(preds)
    class_preds = class_preds.max(-1)[1]
    correct = (class_preds == y).float() # convert into float for division 
    acc = correct.sum() / len(correct)
    return acc

from tqdm import tqdm
Vocabulary = []
for sentence in tqdm(tokenized_corpus):
    for token in sentence:
        Vocabulary.append(token)
vocabulary = set(Vocabulary)
vocabulary=list(vocabulary)
vocabulary_size = len(vocabulary)
print(vocabulary_size)

###development
fix_seed()
EPOCHS=20
INPUT_DIM = vocabulary_size
EMBEDDING_DIM = 50
HIDDEN_DIM = 128
OUTPUT_DIM = 3
BIDIRECTIONAL = False
DROPOUT = 0.3
PAD_IDX=1
model = RNN(INPUT_DIM,EMBEDDING_DIM,HIDDEN_DIM,OUTPUT_DIM, 
      BIDIRECTIONAL, 
      DROPOUT, 
      PAD_IDX)
# optimizer = optim.SGD(model.parameters(), lr=LRATE)
optimizer = optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss()
#loss_fn = nn.BCEWithLogitsLoss()
feature_train = train_sent_tensor
target_train = train_label_tensor

feature_valid = valid_sent_tensor
target_valid = valid_label_tensor
target_train=target_train.long()

print(f'Will train for {EPOCHS} epochs')
for epoch in range(1, EPOCHS + 1):
  model.train()
  start_time=time.time()
  epoch_loss = 0
  epoch_acc = 0
  # print(feature_train[1:65])
  start=1
  end=65
  while(end<len(target_train)):
    optimizer.zero_grad()
    predictions = model(feature_train[start:end]).squeeze(1)
    # print(predictions)
    # print(target_train)
    loss = criterion(predictions, target_train[start:end])
    train_loss = loss.item()
    epoch_loss+=train_loss
    train_acc = accuracy(predictions, target_train[start:end]).item()
    # train_acc=((predictions-target_train)**2).mean()
    epoch_acc+=train_acc
    loss.backward()
    optimizer.step()
    # print(epoch_loss)
    end+=64
    start+=64
  average_epoch_loss = epoch_loss 
  average_epoch_acc = epoch_acc 
  end_time = time.time()
  epoch_mins, epoch_secs = epoch_time(start_time, end_time)
  average_epoch_valid_loss, average_epoch_valid_acc = evaluate(model, criterion)

  print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')
  print(f'\tTrain Loss: {average_epoch_loss:.3f} | Train Acc: {average_epoch_acc*100:.2f}%')
  print(f'\t Val. Loss: {average_epoch_valid_loss:.3f} |  Val. Acc: {average_epoch_valid_acc*100:.2f}%')